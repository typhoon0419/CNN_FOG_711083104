{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "import os\n",
    "import datetime, time\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D\n",
    "from tensorflow.keras.layers import LSTM, TimeDistributed, ConvLSTM2D\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "from sklearn.metrics import confusion_matrix,accuracy_score\n",
    "from keras import backend as k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_path = os.getcwd() + \"/dataset/total_train.csv\"\n",
    "test_data_path = os.getcwd() + \"/dataset/total_test.csv\"\n",
    "new_data_path = os.getcwd() + \"/dataset/merge.csv\"\n",
    "\n",
    "fromPath = os.getcwd() +\"/dataset/windows\"\n",
    "savePath = os.getcwd() +\"/dataset\"\n",
    "\n",
    "testSubs = [\"S01\", \"S02\", \"S03\", \"S05\", \"S06\", \"S07\", \"S08\", \"S09\"]\n",
    "winLen = int(1*64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_window(window_length,dataframe):\n",
    "\n",
    "  indices = list(dataframe.index)\n",
    "  time = []\n",
    "  time_count = 0\n",
    "  for j in indices:\n",
    "    time.append(dataframe.loc[j, 'time'])\n",
    "\n",
    "\n",
    "  # indices記錄所有Action==act的index\n",
    "  groups = [] # 用來暫存一組(同action)資料的, 型態是[][]\n",
    "  temp = [] # 用來暫存一行資料的\n",
    "  group_count = 0\n",
    "  for i in range(len(indices)):\n",
    "    if i == len(indices)-1:\n",
    "      temp.append(indices[i])\n",
    "      groups.append(temp)\n",
    "      temp = []\n",
    "      break # 如果i已經來到最後的話就break\n",
    "    temp.append(indices[i])\n",
    "    #time_count = time_count + 1\n",
    "    if time[i+1]-16 > time[i]: #如果下個index不是連續的話, 就將前面這些存成第一組\n",
    "      group_count+=1\n",
    "      #time_count = time_count + 1\n",
    "      groups.append(temp)\n",
    "      temp = []\n",
    "\n",
    "  #print(groups)\n",
    "\n",
    "  fs = 64\n",
    "  # window_length = 1\n",
    "  # window_length = int(window_length*fs)\n",
    "\n",
    "  final_dataframe = pd.DataFrame()\n",
    "  sumOfAct=0\n",
    "\n",
    "  for i in groups: # group[][]的每一行i\n",
    "    required = math.floor(len(i)/int(window_length/2))\n",
    "\n",
    "    \n",
    "    sumOfAct= sumOfAct+required\n",
    "\n",
    "    req_index = i[0:(required*int(window_length/2))]\n",
    "\n",
    "    #print(req_index)\n",
    "    # concat([要結合的data集合], axis=0是方向為直的)\n",
    "    final_dataframe = pd.concat([final_dataframe,dataframe.iloc[req_index,:]],axis = 0)\n",
    "  \n",
    "  \n",
    "\n",
    "  return final_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveDFtocsv(fromPath: str, toPath: str, compare:str, useOriginal=True, saveName = \"\"):\n",
    "    \"\"\"_summary_\n",
    "\n",
    "    Args:\n",
    "        fromPath (str): _description_\n",
    "        toPath (str): _description_\n",
    "        compare (str): _description_\n",
    "    \"\"\"\n",
    "    \n",
    "    for file in os.listdir(fromPath):\n",
    "        if compare not in file:\n",
    "            continue\n",
    "\n",
    "        filePath = fromPath + '/' + file\n",
    "        dataset = create_window(winLen, pd.read_csv(filePath))\n",
    "\n",
    "        if useOriginal:\n",
    "            savePath = toPath + \"/\" + file\n",
    "        else:\n",
    "            savePath = toPath + \"/\" + saveName + \".csv\"\n",
    "        dataset.to_csv(savePath, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeDataset(fromPath:str, savePath:str, testSub:str):\n",
    "    train_patients_dataset = pd.DataFrame()\n",
    "    test_patients_dataset = pd.DataFrame()\n",
    "\n",
    "    for file in os.listdir(fromPath):\n",
    "        if \"win_\" not in file:\n",
    "            continue\n",
    "        \n",
    "        filePath = fromPath + '/' + file\n",
    "        dataset = pd.read_csv(filePath)\n",
    "\n",
    "        if testSub in file:\n",
    "            test_patients_dataset = test_patients_dataset.append(dataset)\n",
    "        else:    \n",
    "            train_patients_dataset = train_patients_dataset.append(dataset)\n",
    "\n",
    "    to_path = savePath + \"/not_windowed_train.csv\"\n",
    "    train_patients_dataset.to_csv(to_path, index=False)\n",
    "\n",
    "    to_path = savePath + \"/not_windowed_test.csv\"\n",
    "    test_patients_dataset.to_csv(to_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTimeAndDF(path:str):\n",
    "    \"\"\"get dataset and dataset's time list\n",
    "       some dataset's time is not continued, so get the time is for split windows\n",
    "\n",
    "    Args:\n",
    "        path (str): path of the dataset(for .csv)\n",
    "\n",
    "    Returns:\n",
    "        time (list): the list of dataset time\n",
    "        df (np.array): dataset, columns include [\"A_F\", \"A_V\", \"A_L\", \"Action\"]\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(path)\n",
    "    time = list(df['time'])\n",
    "    action = list(df['Action'])\n",
    "    df = df[[\"A_F\", \"A_V\", \"A_L\"]].values\n",
    "\n",
    "    return time, df, action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTotalWindows(indices:list, windowSize:int , gap: float):\n",
    "    \"\"\"because\n",
    "    check every part of time\n",
    "\n",
    "    Args:\n",
    "        indices (list): _description_\n",
    "        windowSize (int): _description_\n",
    "        gap (float): _description_\n",
    "\n",
    "    Returns:\n",
    "        _type_: _description_\n",
    "    \"\"\"\n",
    "    groups = []\n",
    "    group_count = 0\n",
    "    temp = []\n",
    "    lenOfGroup = []\n",
    "    length_count = 0\n",
    "    for i in range(len(indices)):\n",
    "        if i == (len(indices) - 1):\n",
    "            temp.append(indices[i])\n",
    "            length_count = length_count + 1\n",
    "\n",
    "            groups.append(temp)\n",
    "            lenOfGroup.append(length_count)\n",
    "            length_count = 0\n",
    "            temp = []\n",
    "            break\n",
    "        temp.append(indices[i])\n",
    "        length_count = length_count + 1\n",
    "        if (indices[i+1] - gap > indices[i]):\n",
    "            group_count = group_count + 1\n",
    "\n",
    "            lenOfGroup.append(length_count)\n",
    "            length_count = 0\n",
    "\n",
    "            groups.append(temp)\n",
    "            temp = []\n",
    "\n",
    "    countOfUndivisible = 0\n",
    "    totalWindows = 0\n",
    "\n",
    "    stop_Indexs = []\n",
    "    stop_Index = -windowSize\n",
    "\n",
    "\n",
    "    for lengths in lenOfGroup:\n",
    "        stop_Index = stop_Index + lengths\n",
    "        stop_Indexs.append(stop_Index)\n",
    "        totalWindows = totalWindows + int(float(lengths/windowSize)*2 -1)\n",
    "        \n",
    "        if lengths % (windowSize/2) != 0:\n",
    "            countOfUndivisible = countOfUndivisible + 1\n",
    "            print(lengths)\n",
    "\n",
    "    return totalWindows, stop_Indexs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def XySplit(dataset:np.array, windows:int, length:int, stop:list, action:list):\n",
    "    \"\"\"split dataset into X and y, \n",
    "    X is 2D array, size of X is [windows, 64*3]\n",
    "    y is 1D array, size of y is [windows, 1]\n",
    "\n",
    "    Args:\n",
    "        dataset (np.array): dataset\n",
    "        windows (int): total windows that get from getTotalWindows()\n",
    "        length (int): length of a piece of data, here is 3\n",
    "        stop (list): stopList that get from getTotalWindows()\n",
    "        action (list): action list\n",
    "\n",
    "    Returns:\n",
    "        X(np.array): X is 2D array, size of X is [windows, 64*3]\n",
    "        y(np.array): y is 1D array, size of y is [windows, 1]\n",
    "    \"\"\"\n",
    "    X = np.empty((windows, winLen*(length)))\n",
    "    y = np.empty((windows, 1))\n",
    "\n",
    "    stopIndex = 0\n",
    "    windowCount = 0\n",
    "    for win in range(windows):\n",
    "        for i in range(winLen):\n",
    "            if i == 0:\n",
    "                y[win] = action[int(windowCount*winLen + winLen/2)]\n",
    "\n",
    "            if int(windowCount*winLen)<len(dataset)-winLen-1:\n",
    "                for data in range(length):\n",
    "                    X[win, i*(length)+data] = dataset[int(windowCount*winLen) + i, data]\n",
    "\n",
    "            if stopIndex < len(stop):\n",
    "                if int(windowCount*winLen) == stop[stopIndex]:\n",
    "                    windowCount += 0.5\n",
    "                    stopIndex += 1\n",
    "            \n",
    "            if win == windows-1:\n",
    "                for data in range(length):\n",
    "                    X[win, i*(length) + data] = dataset[int((windowCount-0.5)*winLen) + i, data]\n",
    "        \n",
    "        windowCount += 0.5\n",
    "\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_3Darray(array):\n",
    "    \"\"\"_summary_\n",
    "\n",
    "    Args:\n",
    "        array (_type_): _description_\n",
    "\n",
    "    Returns:\n",
    "        _type_: _description_\n",
    "    \"\"\"\n",
    "    arr_3d = np.empty((len(array), winLen, 3))\n",
    "    arr_3d = np.reshape(array, (len(array), winLen, 3))\n",
    "    return arr_3d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setModel(n_length, n_features):\n",
    "    model = Sequential()\n",
    "    model.add(TimeDistributed(Conv1D(filters=64, kernel_size=3,\n",
    "              activation='relu'), input_shape=(None, n_length, n_features)))\n",
    "\n",
    "    model.add(TimeDistributed(\n",
    "        Conv1D(filters=64, kernel_size=3, activation='relu')))\n",
    "    model.add(TimeDistributed(Dropout(0.5)))\n",
    "    model.add(TimeDistributed(MaxPooling1D(pool_size=2)))\n",
    "    model.add(TimeDistributed(Flatten()))\n",
    "    model.add(LSTM(100))\n",
    "    model.add(Dropout(0.5))\n",
    "    hunderdOutput = Dense(100, activation='relu')\n",
    "    model.add(hunderdOutput)  # feature\n",
    "    # 試著輸出長度為100的向量(feature) 並絳維 看他的分布有無分開\n",
    "    model.add(Dense(3, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainingModel(skf, X, y, parameters, class_weight):\n",
    "    losses = []\n",
    "    scores_in_fold = []\n",
    "    for i, (train_index, val_index) in enumerate(skf.split(X, y)):\n",
    "        print(\"==> Fold #%d\" % i)\n",
    "        X_train, X_val = X[train_index], X[val_index]\n",
    "        y_train, y_val = y[train_index], y[val_index]\n",
    "\n",
    "        X_train = to_3Darray(X_train)\n",
    "        y_train = to_categorical(y_train)\n",
    "\n",
    "        X_val = to_3Darray(X_val)\n",
    "        y_val = to_categorical(y_val)\n",
    "\n",
    "\n",
    "        \n",
    "        verbose, epochs, batch_size = 0, parameters[0], parameters[1]\n",
    "        n_timesteps, n_features, n_outputs = X_train.shape[1], X_train.shape[2], y_train.shape[1]\n",
    "\n",
    "        n_steps, n_length = parameters[2], parameters[3]\n",
    "        X_train = X_train.reshape((X_train.shape[0], n_steps, n_length, n_features))\n",
    "        X_val = X_val.reshape((X_val.shape[0], n_steps, n_length, n_features))\n",
    "\n",
    "\n",
    "        model = setModel(n_length, n_features)\n",
    "        \n",
    "        model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, verbose=verbose, class_weight = class_weight)\n",
    "        loss, accuracy = model.evaluate(X_val, y_val, batch_size=batch_size, verbose=0)\n",
    "        \n",
    "\n",
    "        y_pred = (model.predict(X_val) > 0.5).astype(\"int32\")\n",
    "        y_val = np.argmax(y_val, axis=1)\n",
    "        y_pred = np.argmax(y_pred, axis=1)\n",
    "\n",
    "        confus = confusion_matrix(y_val, y_pred, labels=None, sample_weight=None)\n",
    "        tp = confus[1][1]\n",
    "        tn = confus[0][0] + confus[0][2] + confus[2][0] + confus[2][2]\n",
    "        fp = confus[1][0] + confus[1][2]\n",
    "        fn = confus[0][1] + confus[2][1]\n",
    "\n",
    "        precision = (tp/(tp + fp))*100\n",
    "        recall =  (tp / (tp + fn))*100   #sensitivity\n",
    "        F1_score = ((2*tp) / (2*tp + fp + fn))*100\n",
    "\n",
    "        score = accuracy\n",
    "        losses.append(loss)\n",
    "        \n",
    "        score = score * 100.0\n",
    "        scores_in_fold.append(score)\n",
    "        \n",
    "        print('Loss: %.3f%% Accuracy: %.3f%% ' % (loss, score))\n",
    "        print('precision: %.3f%% recall: %.3f%% F1 score: %.3f%%' % (precision, recall, F1_score))\n",
    "\n",
    "    return model, losses, scores_in_fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictData(model, testX, testy, parameters):\n",
    "\n",
    "    testX = to_3Darray(testX)\n",
    "    testX = testX.reshape((testX.shape[0], parameters[2], parameters[3], testX.shape[2]))\n",
    "\n",
    "    testy = to_categorical(testy)\n",
    "    y_pred = (model.predict(testX) > 0.5).astype(\"int32\")\n",
    "    testy = np.argmax(testy, axis=1)\n",
    "    y_pred = np.argmax(y_pred, axis=1)\n",
    "\n",
    "    confus = confusion_matrix(testy, y_pred, labels=None, sample_weight=None)\n",
    "\n",
    "    return confus, y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def performance(state:int, confus:list):\n",
    "    performList = []\n",
    "    if state == 1:\n",
    "        tp = confus[1][1]\n",
    "        tn = confus[0][0] + confus[0][2] + confus[2][0] + confus[2][2]\n",
    "        fp = confus[0][1] + confus[2][1]\n",
    "        fn = confus[1][0] + confus[1][2]\n",
    "    elif state == 2:\n",
    "        tp = confus[2][2]\n",
    "        tn = confus[0][0] + confus[0][1] + confus[1][0] + confus[1][1]\n",
    "        fp = confus[0][2] + confus[1][2]\n",
    "        fn = confus[2][0] + confus[2][1]\n",
    "\n",
    "    precision = (tp/(tp + fp))*100\n",
    "    sensitivity = (tp / (tp + fn))*100  # sensitivity\n",
    "    specificity = (tn/(tn + fp))*100\n",
    "    F1_score = ((2*tp) / (2*tp + fp + fn))*100\n",
    "\n",
    "    performList.append(precision)\n",
    "    performList.append(sensitivity)\n",
    "    performList.append(specificity)\n",
    "    performList.append(F1_score)\n",
    "\n",
    "    return performList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Fold #0\n",
      "Loss: 0.282% Accuracy: 89.746% \n",
      "precision: 85.538% recall: 87.018% F1 score: 86.271%\n",
      "==> Fold #1\n",
      "Loss: 0.256% Accuracy: 90.683% \n",
      "precision: 88.476% recall: 85.119% F1 score: 86.765%\n",
      "==> Fold #2\n",
      "Loss: 0.268% Accuracy: 90.979% \n",
      "precision: 87.007% recall: 88.097% F1 score: 87.549%\n",
      "==> Fold #3\n",
      "Loss: 0.273% Accuracy: 90.436% \n",
      "precision: 90.023% recall: 85.337% F1 score: 87.618%\n",
      "==> Fold #4\n",
      "Loss: 0.252% Accuracy: 91.866% \n",
      "precision: 90.333% recall: 87.556% F1 score: 88.923%\n",
      "==> Fold #5\n",
      "Loss: 0.252% Accuracy: 91.642% \n",
      "precision: 86.775% recall: 90.120% F1 score: 88.416%\n",
      "==> Fold #6\n",
      "Loss: 0.271% Accuracy: 90.409% \n",
      "precision: 87.626% recall: 86.225% F1 score: 86.920%\n",
      "==> Fold #7\n",
      "Loss: 0.278% Accuracy: 89.127% \n",
      "precision: 83.063% recall: 87.745% F1 score: 85.340%\n",
      "==> Fold #8\n",
      "Loss: 0.314% Accuracy: 84.320% \n",
      "precision: 90.797% recall: 82.910% F1 score: 86.674%\n",
      "==> Fold #9\n",
      "Loss: 0.270% Accuracy: 90.557% \n",
      "precision: 89.172% recall: 85.789% F1 score: 87.448%\n",
      "==> Fold #0\n",
      "Loss: 0.264% Accuracy: 90.900% \n",
      "precision: 89.585% recall: 85.437% F1 score: 87.462%\n",
      "==> Fold #1\n",
      "Loss: 0.268% Accuracy: 90.852% \n",
      "precision: 89.115% recall: 85.116% F1 score: 87.070%\n",
      "==> Fold #2\n",
      "Loss: 0.242% Accuracy: 92.032% \n",
      "precision: 88.185% recall: 88.601% F1 score: 88.392%\n",
      "==> Fold #3\n",
      "Loss: 0.247% Accuracy: 91.911% \n",
      "precision: 86.776% recall: 90.827% F1 score: 88.756%\n",
      "==> Fold #4\n",
      "Loss: 0.271% Accuracy: 89.622% \n",
      "precision: 90.133% recall: 81.805% F1 score: 85.768%\n",
      "==> Fold #5\n",
      "Loss: 0.245% Accuracy: 90.922% \n",
      "precision: 86.609% recall: 87.224% F1 score: 86.916%\n",
      "==> Fold #6\n",
      "Loss: 0.248% Accuracy: 91.019% \n",
      "precision: 84.652% recall: 88.971% F1 score: 86.758%\n",
      "==> Fold #7\n",
      "Loss: 0.235% Accuracy: 92.126% \n",
      "precision: 89.898% recall: 87.234% F1 score: 88.546%\n",
      "==> Fold #8\n",
      "Loss: 0.246% Accuracy: 91.524% \n",
      "precision: 88.019% recall: 87.471% F1 score: 87.744%\n",
      "==> Fold #9\n",
      "Loss: 0.242% Accuracy: 91.837% \n",
      "precision: 85.904% recall: 90.886% F1 score: 88.325%\n",
      "==> Fold #0\n",
      "Loss: 0.249% Accuracy: 91.392% \n",
      "precision: 84.952% recall: 91.119% F1 score: 87.927%\n",
      "==> Fold #1\n",
      "Loss: 0.236% Accuracy: 91.880% \n",
      "precision: 84.554% recall: 91.081% F1 score: 87.696%\n",
      "==> Fold #2\n",
      "Loss: 0.234% Accuracy: 91.831% \n",
      "precision: 86.465% recall: 89.975% F1 score: 88.185%\n",
      "==> Fold #3\n",
      "Loss: 0.251% Accuracy: 91.392% \n",
      "precision: 89.411% recall: 86.252% F1 score: 87.803%\n",
      "==> Fold #4\n",
      "Loss: 0.264% Accuracy: 91.465% \n",
      "precision: 87.261% recall: 88.745% F1 score: 87.997%\n",
      "==> Fold #5\n",
      "Loss: 0.219% Accuracy: 92.049% \n",
      "precision: 89.889% recall: 87.792% F1 score: 88.828%\n",
      "==> Fold #6\n",
      "Loss: 0.217% Accuracy: 92.049% \n",
      "precision: 89.411% recall: 87.598% F1 score: 88.495%\n",
      "==> Fold #7\n",
      "Loss: 0.259% Accuracy: 90.976% \n",
      "precision: 89.092% recall: 85.813% F1 score: 87.422%\n",
      "==> Fold #8\n",
      "Loss: 0.237% Accuracy: 92.146% \n",
      "precision: 87.898% recall: 90.122% F1 score: 88.996%\n",
      "==> Fold #9\n",
      "Loss: 0.242% Accuracy: 91.146% \n",
      "precision: 88.217% recall: 88.146% F1 score: 88.181%\n",
      "==> Fold #0\n",
      "Loss: 0.220% Accuracy: 92.063% \n",
      "precision: 90.394% recall: 86.625% F1 score: 88.469%\n",
      "==> Fold #1\n",
      "Loss: 0.227% Accuracy: 92.809% \n",
      "precision: 89.984% recall: 88.387% F1 score: 89.178%\n",
      "==> Fold #2\n",
      "Loss: 0.231% Accuracy: 92.162% \n",
      "precision: 90.566% recall: 86.656% F1 score: 88.568%\n",
      "==> Fold #3\n",
      "Loss: 0.230% Accuracy: 92.336% \n",
      "precision: 89.171% recall: 89.613% F1 score: 89.391%\n",
      "==> Fold #4\n",
      "Loss: 0.232% Accuracy: 91.789% \n",
      "precision: 89.253% recall: 87.530% F1 score: 88.383%\n",
      "==> Fold #5\n",
      "Loss: 0.263% Accuracy: 90.495% \n",
      "precision: 87.285% recall: 87.213% F1 score: 87.249%\n",
      "==> Fold #6\n",
      "Loss: 0.223% Accuracy: 92.013% \n",
      "precision: 87.777% recall: 89.840% F1 score: 88.797%\n",
      "==> Fold #7\n",
      "Loss: 0.240% Accuracy: 91.590% \n",
      "precision: 88.023% recall: 87.094% F1 score: 87.556%\n",
      "==> Fold #8\n",
      "Loss: 0.218% Accuracy: 92.882% \n",
      "precision: 88.670% recall: 89.850% F1 score: 89.256%\n",
      "==> Fold #9\n",
      "Loss: 0.223% Accuracy: 92.583% \n",
      "precision: 89.984% recall: 89.178% F1 score: 89.579%\n",
      "==> Fold #0\n",
      "Loss: 0.262% Accuracy: 91.758% \n",
      "precision: 87.801% recall: 89.046% F1 score: 88.419%\n",
      "==> Fold #1\n",
      "Loss: 0.248% Accuracy: 90.880% \n",
      "precision: 88.889% recall: 87.328% F1 score: 88.102%\n",
      "==> Fold #2\n",
      "Loss: 0.252% Accuracy: 91.661% \n",
      "precision: 90.909% recall: 86.538% F1 score: 88.670%\n",
      "==> Fold #3\n",
      "Loss: 0.255% Accuracy: 90.222% \n",
      "precision: 85.703% recall: 88.099% F1 score: 86.885%\n",
      "==> Fold #4\n",
      "Loss: 0.245% Accuracy: 91.465% \n",
      "precision: 90.450% recall: 87.397% F1 score: 88.897%\n",
      "==> Fold #5\n",
      "Loss: 0.234% Accuracy: 91.587% \n",
      "precision: 90.295% recall: 86.726% F1 score: 88.475%\n",
      "==> Fold #6\n",
      "Loss: 0.237% Accuracy: 91.634% \n",
      "precision: 86.713% recall: 89.927% F1 score: 88.291%\n",
      "==> Fold #7\n",
      "Loss: 0.250% Accuracy: 91.049% \n",
      "precision: 84.382% recall: 89.236% F1 score: 86.741%\n",
      "==> Fold #8\n",
      "Loss: 0.239% Accuracy: 91.561% \n",
      "precision: 87.179% recall: 88.836% F1 score: 88.000%\n",
      "==> Fold #9\n",
      "Loss: 0.234% Accuracy: 91.902% \n",
      "precision: 89.744% recall: 86.973% F1 score: 88.337%\n",
      "==> Fold #0\n",
      "Loss: 0.258% Accuracy: 90.352% \n",
      "precision: 85.428% recall: 86.092% F1 score: 85.759%\n",
      "==> Fold #1\n",
      "Loss: 0.252% Accuracy: 91.519% \n",
      "precision: 86.507% recall: 89.760% F1 score: 88.104%\n",
      "==> Fold #2\n",
      "Loss: 0.255% Accuracy: 90.911% \n",
      "precision: 88.589% recall: 86.652% F1 score: 87.610%\n",
      "==> Fold #3\n",
      "Loss: 0.290% Accuracy: 90.255% \n",
      "precision: 90.370% recall: 85.997% F1 score: 88.129%\n",
      "==> Fold #4\n",
      "Loss: 0.295% Accuracy: 89.037% \n",
      "precision: 90.517% recall: 80.798% F1 score: 85.382%\n",
      "==> Fold #5\n",
      "Loss: 0.237% Accuracy: 91.517% \n",
      "precision: 87.433% recall: 88.594% F1 score: 88.009%\n",
      "==> Fold #6\n",
      "Loss: 0.251% Accuracy: 90.982% \n",
      "precision: 87.741% recall: 86.804% F1 score: 87.270%\n",
      "==> Fold #7\n",
      "Loss: 0.249% Accuracy: 90.544% \n",
      "precision: 89.823% recall: 85.099% F1 score: 87.397%\n",
      "==> Fold #8\n",
      "Loss: 0.246% Accuracy: 90.933% \n",
      "precision: 89.668% recall: 85.389% F1 score: 87.476%\n",
      "==> Fold #9\n",
      "Loss: 0.268% Accuracy: 90.326% \n",
      "precision: 89.591% recall: 85.504% F1 score: 87.500%\n",
      "==> Fold #0\n",
      "Loss: 0.244% Accuracy: 91.501% \n",
      "precision: 88.452% recall: 87.900% F1 score: 88.175%\n",
      "==> Fold #1\n",
      "Loss: 0.268% Accuracy: 91.455% \n",
      "precision: 91.123% recall: 85.294% F1 score: 88.112%\n",
      "==> Fold #2\n",
      "Loss: 0.249% Accuracy: 91.618% \n",
      "precision: 91.123% recall: 86.632% F1 score: 88.821%\n",
      "==> Fold #3\n",
      "Loss: 0.218% Accuracy: 92.552% \n",
      "precision: 89.552% recall: 87.625% F1 score: 88.578%\n",
      "==> Fold #4\n",
      "Loss: 0.264% Accuracy: 90.777% \n",
      "precision: 91.595% recall: 84.800% F1 score: 88.066%\n",
      "==> Fold #5\n",
      "Loss: 0.223% Accuracy: 92.389% \n",
      "precision: 91.444% recall: 86.746% F1 score: 89.033%\n",
      "==> Fold #6\n",
      "Loss: 0.253% Accuracy: 91.408% \n",
      "precision: 91.052% recall: 84.982% F1 score: 87.912%\n",
      "==> Fold #7\n",
      "Loss: 0.240% Accuracy: 91.709% \n",
      "precision: 89.474% recall: 86.288% F1 score: 87.852%\n",
      "==> Fold #8\n",
      "Loss: 0.229% Accuracy: 92.667% \n",
      "precision: 89.945% recall: 88.829% F1 score: 89.383%\n",
      "==> Fold #9\n",
      "Loss: 0.255% Accuracy: 91.219% \n",
      "precision: 88.138% recall: 86.175% F1 score: 87.146%\n",
      "==> Fold #0\n",
      "Loss: 0.253% Accuracy: 91.758% \n",
      "precision: 88.006% recall: 88.498% F1 score: 88.252%\n",
      "==> Fold #1\n",
      "Loss: 0.259% Accuracy: 91.930% \n",
      "precision: 87.292% recall: 89.714% F1 score: 88.486%\n",
      "==> Fold #2\n",
      "Loss: 0.218% Accuracy: 92.541% \n",
      "precision: 90.389% recall: 88.560% F1 score: 89.465%\n",
      "==> Fold #3\n",
      "Loss: 0.250% Accuracy: 91.196% \n",
      "precision: 87.460% recall: 89.015% F1 score: 88.231%\n",
      "==> Fold #4\n",
      "Loss: 0.242% Accuracy: 91.856% \n",
      "precision: 86.270% recall: 90.282% F1 score: 88.231%\n",
      "==> Fold #5\n",
      "Loss: 0.259% Accuracy: 91.465% \n",
      "precision: 90.476% recall: 86.560% F1 score: 88.475%\n",
      "==> Fold #6\n",
      "Loss: 0.239% Accuracy: 92.101% \n",
      "precision: 90.556% recall: 86.702% F1 score: 88.587%\n",
      "==> Fold #7\n",
      "Loss: 0.259% Accuracy: 91.438% \n",
      "precision: 88.245% recall: 88.455% F1 score: 88.350%\n",
      "==> Fold #8\n",
      "Loss: 0.232% Accuracy: 92.099% \n",
      "precision: 90.071% recall: 87.771% F1 score: 88.906%\n",
      "==> Fold #9\n",
      "Loss: 0.247% Accuracy: 90.680% \n",
      "precision: 89.992% recall: 86.160% F1 score: 88.034%\n"
     ]
    }
   ],
   "source": [
    "parameters = [50, 64, 2, 32] # epoch, batch size, n_steps, n_length\n",
    "date, currTime = str(datetime.date.today()), str(time.strftime(\"%H-%M\", time.localtime()))\n",
    "\n",
    "resultPath = os.getcwd() +\"/result/\" + date + '/' \n",
    "if not os.path.exists(resultPath):\n",
    "    os.mkdir(resultPath)\n",
    "file = open(resultPath + currTime + \".txt\", \"a+\")\n",
    "\n",
    "file.write(\"Epoch=\" + str(parameters[0]) + \" Batch Size=\" + str(parameters[1]) + \" steps=\" + str(parameters[2]) + \" length=\" + str(parameters[3]) + '\\n')\n",
    "file.close()\n",
    "\n",
    "for testSub in testSubs:\n",
    "    makeDataset(fromPath, savePath, testSub)\n",
    "\n",
    "    saveDFtocsv(savePath, savePath, \"not_windowed_train\", False, \"total_train\")\n",
    "    saveDFtocsv(savePath, savePath, \"not_windowed_test\", False, \"total_test\")\n",
    "\n",
    "    trainTime, trainData, trainAction = getTimeAndDF(train_data_path)\n",
    "    testTime, testData, testAction = getTimeAndDF(test_data_path)\n",
    "    newTime, newData, newAction = getTimeAndDF(new_data_path)\n",
    "\n",
    "    trainData = (trainData-trainData.mean())/(trainData.std())\n",
    "    testData = (testData-testData.mean())/(testData.std())\n",
    "    newData = (newData-newData.mean())/(newData.std())\n",
    "\n",
    "    trainWindows, trainStop = getTotalWindows(trainTime, winLen, 20) \n",
    "    newWindows, newStop = getTotalWindows(newTime, winLen, 0.02)\n",
    "    testWindows, testStop = getTotalWindows(testTime, winLen, 20)\n",
    "\n",
    "    del trainTime, newTime, testTime\n",
    "\n",
    "    trainX, trainy = XySplit(trainData, trainWindows, 3, trainStop, trainAction)\n",
    "    newX, newy = XySplit(newData, newWindows, 3, newStop, newAction)\n",
    "    testX, testy = XySplit(testData, testWindows, 3, testStop, testAction)\n",
    "\n",
    "    X = np.concatenate((trainX, newX))\n",
    "    y = np.concatenate((trainy, newy))\n",
    "\n",
    "    del trainX, trainy, newX, newy, trainData, trainWindows, newData, newWindows, testWindows, trainStop, trainAction, newStop, newAction, testStop, testAction\n",
    "\n",
    "    unique, counts = np.unique(y, return_counts=True)\n",
    "    class_weight = {0:(1/counts[0])*len(y)/2, 1:(1/counts[1])*len(y)/2, 2:(1/counts[2])*len(y)/2}\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=10, shuffle = True, random_state=42)\n",
    "    skf.get_n_splits(X, y)\n",
    "\n",
    "    scores = []\n",
    "\n",
    "    model, losses, scores_in_fold = trainingModel(skf, X, y, parameters, class_weight)\n",
    "\n",
    "    file = open(resultPath + currTime + \".txt\", \"a+\")\n",
    "    file.write(testSub + \"\\n\" + \"\\n\")\n",
    "\n",
    "    m, s = np.mean(scores_in_fold), np.std(scores_in_fold)\n",
    "    temp = 'Training Accuracy: ' + str(round(m, 3)) +' (+/-' + str(round(s, 3)) +')'\n",
    "    file.write(temp + \"\\n\")\n",
    "\n",
    "    m, s = np.mean(losses), np.std(losses)\n",
    "    temp = 'Training Loss: ' + str(round(m, 3)) +' (+/-' + str(round(s, 3)) +')'\n",
    "    file.write(temp + \"\\n\" + \"\\n\")\n",
    "\n",
    "    confus, y_pred = predictData(model, testX, testy, parameters)\n",
    "\n",
    "    accuracy = accuracy_score(testy, y_pred)*100\n",
    "    perform = performance(1, confus)\n",
    "\n",
    "    temp = 'Test Accuracy: ' + str(round(accuracy, 3))\n",
    "    file.write(temp + \"\\n\")\n",
    "    temp = 'FOG: \\nspecificity: ' + str(round(perform[1], 3)) + ' sensitivity: ' + str(round(perform[2], 3)) + ' F1 score: ' + str(str(round(perform[3], 3)))\n",
    "    file.write(temp + \"\\n\")\n",
    "\n",
    "    perform = performance(2, confus)\n",
    "    temp = 'PreFOG: \\nspecificity: ' + str(round(perform[1], 3)) + ' sensitivity: ' + str(round(perform[2], 3)) + ' F1 score: ' + str(str(round(perform[3], 3)))\n",
    "    file.write(temp + \"\\n\")\n",
    "    file.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.11 ('HighHeelWhatever')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8986fb416174cc2474d1ce69838b7b56508ac61c47a66825c7584556038319d3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
