{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D\n",
    "from tensorflow.keras.layers import LSTM, TimeDistributed, ConvLSTM2D\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = os.getcwd() + \"/dataset/total.csv\"\n",
    "features = [\"A_F\", \"A_V\", \"A_L\", \"L_F\", \"L_T\", \"L_L\", \"T_F\", \"T_V\", \"T_L\"]\n",
    "\n",
    "dataset = pd.read_csv(data_path)\n",
    "dataframe = pd.read_csv(data_path)\n",
    "dataset = dataset[[\"A_F\", \"A_V\", \"A_L\", \"L_F\", \"L_V\", \"L_L\", \"T_F\", \"T_V\", \"T_L\", \"Action\"]].values\n",
    "\n",
    "window_length = int(4*64)\n",
    "total_windows = int((len(dataset))/window_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = list(dataframe['time'])\n",
    "groups = []\n",
    "group_count = 0\n",
    "temp = []\n",
    "lenOfGroup = []\n",
    "length_count = 0\n",
    "\n",
    "for i in range(len(indices)):\n",
    "    if i == (len(indices) - 1):\n",
    "        temp.append(indices[i])\n",
    "        length_count = length_count + 1\n",
    "\n",
    "        groups.append(temp)\n",
    "        lenOfGroup.append(length_count)\n",
    "        length_count = 0\n",
    "        temp = []\n",
    "        break\n",
    "    temp.append(indices[i])\n",
    "    length_count = length_count + 1\n",
    "    if (indices[i+1] - 20 > indices[i]):\n",
    "        group_count = group_count + 1\n",
    "\n",
    "        lenOfGroup.append(length_count)\n",
    "        length_count = 0\n",
    "\n",
    "        groups.append(temp)\n",
    "        temp = []\n",
    "\n",
    "del temp, indices, group_count, length_count, i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "countOfUndivisible = 0\n",
    "total_windows_with_overlap = 0\n",
    "\n",
    "start_index = 0\n",
    "\n",
    "stop_Indexs = []\n",
    "stop_Index = -1 * window_length\n",
    "\n",
    "\n",
    "for lengths in lenOfGroup:\n",
    "    stop_Index = stop_Index + lengths\n",
    "    stop_Indexs.append(stop_Index)\n",
    "    total_windows_with_overlap = total_windows_with_overlap + int(lengths/window_length*2 -1)\n",
    "    \n",
    "    if lengths % (window_length/2) != 0:\n",
    "        countOfUndivisible = countOfUndivisible + 1\n",
    "        print(lengths)\n",
    "\n",
    "print(countOfUndivisible)\n",
    "del countOfUndivisible, lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.empty((total_windows_with_overlap, window_length, 9))\n",
    "y = np.empty((total_windows_with_overlap, 1))\n",
    "\n",
    "stop_Index = 0\n",
    "window_count = 0\n",
    "\n",
    "for window in range(total_windows_with_overlap):\n",
    "    for i in range(window_length):\n",
    "        if i == 0:\n",
    "            #if int(window_count*window_length)<len(dataset)-window_length-1:\n",
    "            y[window] = dataset[int(window_count*window_length), 9]\n",
    "        \n",
    "        if int(window_count*window_length)<len(dataset)-window_length-1:\n",
    "            for data in range(9):\n",
    "               X[window, i, data] = dataset[int(window_count*window_length) + i, data]\n",
    "        if stop_Index < len(stop_Indexs):\n",
    "            if int(window_count*window_length) == stop_Indexs[stop_Index]:\n",
    "                window_count = window_count + 0.5\n",
    "                stop_Index = stop_Index + 1\n",
    "    window_count = window_count + 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = to_categorical(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(trainX, trainy, testX, testy):\n",
    "    verbose, epochs, batch_size = 0, 50, 64\n",
    "    n_timesteps, n_features, n_outputs = trainX.shape[1], trainX.shape[2], trainy.shape[1]\n",
    "\n",
    "    n_steps, n_length = 2, 128\n",
    "    trainX = trainX.reshape((trainX.shape[0], n_steps, n_length, n_features))\n",
    "    testX = testX.reshape((testX.shape[0], n_steps, n_length, n_features))\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(TimeDistributed(Conv1D(filters=64, kernel_size=3, activation='relu'), \n",
    "                              input_shape=(None, n_length, n_features)))\n",
    "    model.add(TimeDistributed(Conv1D(filters=64, kernel_size=3, activation='relu')))\n",
    "    model.add(TimeDistributed(Dropout(0.5)))\n",
    "    model.add(TimeDistributed(MaxPooling1D(pool_size=2)))\n",
    "    model.add(TimeDistributed(Flatten()))\n",
    "    model.add(LSTM(100))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(100, activation='relu'))\n",
    "    model.add(Dense(n_outputs, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    model.fit(trainX, trainy, epochs=epochs, batch_size=batch_size, verbose=verbose)\n",
    "\n",
    "    _, accuracy = model.evaluate(testX, testy, batch_size=batch_size, verbose=0)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(trainX, trainy, testX, testy, repeats=10):\n",
    "\n",
    "    scores = list()\n",
    "    for r in range(repeats):\n",
    "        score = evaluate_model(trainX, trainy, testX, testy)\n",
    "        score = score * 100.0\n",
    "        print('>#%d: %.3f' % (r+1, score))\n",
    "        scores.append(score)\n",
    "    \n",
    "    m, s = np.mean(scores), np.std(scores)\n",
    "    print('Accuracy: %.3f%% (+/-%.3f)' % (m, s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y ,test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape:(4520, 256, 9),y_train.shape:(4520, 3)\n",
      "X_test.shape:(1130, 256, 9),y_test.shape:(1130, 3)\n",
      "\n",
      ">#1: 87.434\n",
      "Accuracy: 87.434% (+/-0.000)\n"
     ]
    }
   ],
   "source": [
    "print('X_train.shape:{},y_train.shape:{}'.format(X_train.shape,y_train.shape))\n",
    "print('X_test.shape:{},y_test.shape:{}\\n'.format(X_test.shape,y_test.shape))\n",
    "\n",
    "run_experiment(X_train, y_train, X_test, y_test, repeats=1)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "cdb0dce53ee342b4df2ec29da00f64db1b0b5ad6c6a758debba5208518f3513a"
  },
  "kernelspec": {
   "display_name": "Python 3.8.11 64-bit ('HighHeel': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
