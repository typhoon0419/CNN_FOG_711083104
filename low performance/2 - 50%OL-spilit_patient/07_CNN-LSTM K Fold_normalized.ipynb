{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ESA LAB\\anaconda3\\envs\\HighHeel\\lib\\site-packages\\pydot.py:17: UserWarning: `pydot` could not import `dot_parser`, so `pydot` will be unable to parse DOT files. The error was:  No module named 'pyparsing'\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D\n",
    "from tensorflow.keras.layers import LSTM, TimeDistributed, ConvLSTM2D\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix,accuracy_score,f1_score,roc_auc_score,recall_score,precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = os.getcwd() + \"/dataset/total_concentrated.csv\"\n",
    "# features = [\"A_F\", \"A_V\", \"A_L\", \"L_F\", \"L_T\", \"L_L\", \"T_F\", \"T_V\", \"T_L\"]\n",
    "\n",
    "dataset = pd.read_csv(data_path)\n",
    "time = list(dataset['time'])\n",
    "dataframe = dataset[[\"A_F\", \"A_V\", \"A_L\", \"L_F\", \"L_V\", \"L_L\", \"T_F\", \"T_V\", \"T_L\"]].values\n",
    "dataset = dataset[[\"A_F\", \"A_V\", \"A_L\", \"L_F\", \"L_V\", \"L_L\", \"T_F\", \"T_V\", \"T_L\", \"Action\"]].values\n",
    "\n",
    "window_length = int(1*64)\n",
    "total_windows = int((len(dataset))/window_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = (dataframe-dataframe.mean())/dataframe.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nX = np.empty((total_windows, window_length, 9))\\ny = np.empty((total_windows, 1))\\nj = 0\\n\\nwindow_count = 0\\nfor items in range(total_windows):\\n    for i in range(window_length):\\n        if i == 0:\\n            y[j] = dataset[int(window_count*window_length), 9]\\n            j = j + 1\\n        for data in range(9):\\n            X[items, i, data] = dataset[int(window_count*window_length)+i, data]\\n    window_count = window_count+1\\n\\ndel window_count, i, j, items, data\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "X = np.empty((total_windows, window_length, 9))\n",
    "y = np.empty((total_windows, 1))\n",
    "j = 0\n",
    "\n",
    "window_count = 0\n",
    "for items in range(total_windows):\n",
    "    for i in range(window_length):\n",
    "        if i == 0:\n",
    "            y[j] = dataset[int(window_count*window_length), 9]\n",
    "            j = j + 1\n",
    "        for data in range(9):\n",
    "            X[items, i, data] = dataset[int(window_count*window_length)+i, data]\n",
    "    window_count = window_count+1\n",
    "\n",
    "del window_count, i, j, items, data\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = time\n",
    "groups = []\n",
    "group_count = 0\n",
    "temp = []\n",
    "lenOfGroup = []\n",
    "length_count = 0\n",
    "\n",
    "for i in range(len(indices)):\n",
    "    if i == (len(indices) - 1):\n",
    "        temp.append(indices[i])\n",
    "        length_count = length_count + 1\n",
    "\n",
    "        groups.append(temp)\n",
    "        lenOfGroup.append(length_count)\n",
    "        length_count = 0\n",
    "        temp = []\n",
    "        break\n",
    "    temp.append(indices[i])\n",
    "    length_count = length_count + 1\n",
    "    if (indices[i+1] - 20 > indices[i]):\n",
    "        group_count = group_count + 1\n",
    "\n",
    "        lenOfGroup.append(length_count)\n",
    "        length_count = 0\n",
    "\n",
    "        groups.append(temp)\n",
    "        temp = []\n",
    "\n",
    "del temp, indices, group_count, length_count, i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "確認每段資料能不能整除64/2, 並輸出不能整除的個數"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "countOfUndivisible = 0\n",
    "total_windows_with_overlap = 0\n",
    "\n",
    "start_index = 0\n",
    "\n",
    "stop_Indexs = []\n",
    "stop_Index = -window_length\n",
    "\n",
    "\n",
    "for lengths in lenOfGroup:\n",
    "    stop_Index = stop_Index + lengths\n",
    "    stop_Indexs.append(stop_Index)\n",
    "    total_windows_with_overlap = total_windows_with_overlap + int(lengths/window_length*2 -1)\n",
    "    \n",
    "    if lengths % (window_length/2) != 0:\n",
    "        countOfUndivisible = countOfUndivisible + 1\n",
    "        print(lengths)\n",
    "\n",
    "print(countOfUndivisible)\n",
    "del countOfUndivisible, lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ = np.empty((total_windows_with_overlap, window_length*9))\n",
    "y_ = np.empty((total_windows_with_overlap, 1))\n",
    "\n",
    "stop_Index = 0\n",
    "window_count = 0\n",
    "\n",
    "for window in range(total_windows_with_overlap):\n",
    "    for i in range(window_length):\n",
    "        if i == 0:\n",
    "            y_[window] = dataset[int(window_count*window_length), 9]\n",
    "        \n",
    "        if int(window_count*window_length)<len(dataset)-window_length-1:\n",
    "            for data in range(9):\n",
    "               X_[window, i*9 + data] = dataframe[int(window_count*window_length) + i, data]\n",
    "        if stop_Index < len(stop_Indexs):\n",
    "            if int(window_count*window_length) == stop_Indexs[stop_Index]:\n",
    "                window_count = window_count + 0.5\n",
    "                stop_Index = stop_Index + 1\n",
    "    window_count = window_count + 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, X_test, y, y_test = train_test_split(X_, y_, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique, counts = np.unique(y, return_counts=True)\n",
    "\n",
    "class_weight = {0:(1/counts[0])*len(y)/2, 1:(1/counts[1])*len(y)/2, 2:(1/counts[2])*len(y)/2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StratifiedKFold(n_splits=10, random_state=42, shuffle=True)\n"
     ]
    }
   ],
   "source": [
    "skf = StratifiedKFold(n_splits=10, shuffle = True, random_state=42)\n",
    "skf.get_n_splits(X, y)\n",
    "print(skf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_3Darray(array):\n",
    "    arr_3d = np.empty((len(array), window_length, 9))\n",
    "\n",
    "\n",
    "    arr_3d = np.reshape(array, (len(array), window_length, 9))\n",
    "    return arr_3d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"def evaluate_model(trainX, trainy, testX, testy):\\n    verbose, epochs, batch_size = 0, 50, 64\\n    n_timesteps, n_features, n_outputs = trainX.shape[1], trainX.shape[2], trainy.shape[1]\\n\\n    n_steps, n_length = 2, 32\\n    trainX = trainX.reshape((trainX.shape[0], n_steps, n_length, n_features))\\n    testX = testX.reshape((testX.shape[0], n_steps, n_length, n_features))\\n\\n    model = Sequential()\\n    model.add(TimeDistributed(Conv1D(filters=64, kernel_size=3, activation='relu'), \\n                              input_shape=(None, n_length, n_features)))\\n    model.add(TimeDistributed(Conv1D(filters=64, kernel_size=3, activation='relu')))\\n    model.add(TimeDistributed(Dropout(0.5)))\\n    model.add(TimeDistributed(MaxPooling1D(pool_size=2)))\\n    model.add(TimeDistributed(Flatten()))\\n    model.add(LSTM(100))\\n    model.add(Dropout(0.5))\\n    model.add(Dense(100, activation='relu'))\\n    model.add(Dense(n_outputs, activation='softmax'))\\n    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\\n    \\n    model.fit(trainX, trainy, epochs=epochs, batch_size=batch_size, verbose=verbose)\\n\\n    _, accuracy = model.evaluate(testX, testy, batch_size=batch_size, verbose=0)\\n    return accuracy\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''def evaluate_model(trainX, trainy, testX, testy):\n",
    "    verbose, epochs, batch_size = 0, 50, 64\n",
    "    n_timesteps, n_features, n_outputs = trainX.shape[1], trainX.shape[2], trainy.shape[1]\n",
    "\n",
    "    n_steps, n_length = 2, 32\n",
    "    trainX = trainX.reshape((trainX.shape[0], n_steps, n_length, n_features))\n",
    "    testX = testX.reshape((testX.shape[0], n_steps, n_length, n_features))\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(TimeDistributed(Conv1D(filters=64, kernel_size=3, activation='relu'), \n",
    "                              input_shape=(None, n_length, n_features)))\n",
    "    model.add(TimeDistributed(Conv1D(filters=64, kernel_size=3, activation='relu')))\n",
    "    model.add(TimeDistributed(Dropout(0.5)))\n",
    "    model.add(TimeDistributed(MaxPooling1D(pool_size=2)))\n",
    "    model.add(TimeDistributed(Flatten()))\n",
    "    model.add(LSTM(100))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(100, activation='relu'))\n",
    "    model.add(Dense(n_outputs, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    model.fit(trainX, trainy, epochs=epochs, batch_size=batch_size, verbose=verbose)\n",
    "\n",
    "    _, accuracy = model.evaluate(testX, testy, batch_size=batch_size, verbose=0)\n",
    "    return accuracy'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"def run_experiment(trainX, trainy, testX, testy, repeats):\\n\\n    scores = list()\\n    for r in range(repeats):\\n        score = evaluate_model(trainX, trainy, testX, testy)\\n        score = score * 100.0\\n        print('>#%d: %.3f' % (r+1, score))\\n        scores.append(score)\\n    \\n    m, s = np.mean(scores), np.std(scores)\\n    print('Accuracy: %.3f%% (+/-%.3f)' % (m, s))\\n\\n    return m\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''def run_experiment(trainX, trainy, testX, testy, repeats):\n",
    "\n",
    "    scores = list()\n",
    "    for r in range(repeats):\n",
    "        score = evaluate_model(trainX, trainy, testX, testy)\n",
    "        score = score * 100.0\n",
    "        print('>#%d: %.3f' % (r+1, score))\n",
    "        scores.append(score)\n",
    "    \n",
    "    m, s = np.mean(scores), np.std(scores)\n",
    "    print('Accuracy: %.3f%% (+/-%.3f)' % (m, s))\n",
    "\n",
    "    return m'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_in_fold = []\n",
    "losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_outside_fold = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Fold #0\n",
      "WARNING:tensorflow:From C:\\Users\\ESA LAB\\anaconda3\\envs\\HighHeel\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py:5043: calling gather (from tensorflow.python.ops.array_ops) with validate_indices is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "The `validate_indices` argument has no effect. Indices are always validated on CPU and never validated on GPU.\n",
      "Loss: 0.279% Accuracy: 90.091% \n",
      "precision: 91.270% recall: 58.376% F1 score: 71.207%\n",
      "==> Fold #1\n",
      "Loss: 0.368% Accuracy: 83.753% \n",
      "precision: 88.889% recall: 44.711% F1 score: 59.495%\n",
      "==> Fold #2\n",
      "Loss: 0.414% Accuracy: 83.954% \n",
      "precision: 90.476% recall: 45.328% F1 score: 60.397%\n",
      "==> Fold #3\n",
      "Loss: 0.318% Accuracy: 88.274% \n",
      "precision: 81.746% recall: 54.068% F1 score: 65.087%\n",
      "==> Fold #4\n",
      "Loss: 0.374% Accuracy: 85.657% \n",
      "precision: 90.079% recall: 48.195% F1 score: 62.794%\n",
      "==> Fold #5\n",
      "Loss: 0.393% Accuracy: 84.902% \n",
      "precision: 94.841% recall: 46.863% F1 score: 62.730%\n",
      "==> Fold #6\n",
      "Loss: 0.355% Accuracy: 87.267% \n",
      "precision: 88.845% recall: 51.264% F1 score: 65.015%\n",
      "==> Fold #7\n",
      "Loss: 0.356% Accuracy: 86.059% \n",
      "precision: 87.251% recall: 49.324% F1 score: 63.022%\n",
      "==> Fold #8\n",
      "Loss: 0.284% Accuracy: 89.129% \n",
      "precision: 79.681% recall: 57.307% F1 score: 66.667%\n",
      "==> Fold #9\n",
      "Loss: 0.329% Accuracy: 88.374% \n",
      "precision: 86.056% recall: 53.865% F1 score: 66.258%\n"
     ]
    }
   ],
   "source": [
    "for i, (train_index, val_index) in enumerate(skf.split(X, y)):\n",
    "    print(\"==> Fold #%d\" % i)\n",
    "    X_train, X_val = X[train_index], X[val_index]\n",
    "    y_train, y_val = y[train_index], y[val_index]\n",
    "\n",
    "    X_train = to_3Darray(X_train)\n",
    "    y_train = to_categorical(y_train)\n",
    "\n",
    "    X_val = to_3Darray(X_val)\n",
    "    y_val = to_categorical(y_val)\n",
    "\n",
    "\n",
    "    \n",
    "    verbose, epochs, batch_size = 0, 50, 64\n",
    "    n_timesteps, n_features, n_outputs = X_train.shape[1], X_train.shape[2], y_train.shape[1]\n",
    "\n",
    "    n_steps, n_length = 2, 32\n",
    "    X_train = X_train.reshape((X_train.shape[0], n_steps, n_length, n_features))\n",
    "    X_val = X_val.reshape((X_val.shape[0], n_steps, n_length, n_features))\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(TimeDistributed(Conv1D(filters=64, kernel_size=3, activation='relu'), \n",
    "                              input_shape=(None, n_length, n_features)))\n",
    "    model.add(TimeDistributed(Conv1D(filters=64, kernel_size=3, activation='relu')))\n",
    "    model.add(TimeDistributed(Dropout(0.5)))\n",
    "    model.add(TimeDistributed(MaxPooling1D(pool_size=2)))\n",
    "    model.add(TimeDistributed(Flatten()))\n",
    "    model.add(LSTM(100))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(100, activation='relu')) #feature\n",
    "    model.add(Dense(n_outputs, activation='softmax')) #試著輸出長度為100的向量(feature) 並絳維 看他的分布有無分開\n",
    "    model.compile(loss='categorical_crossentropy',optimizer='adam', metrics=['accuracy']) #可能可以調weighting\n",
    "    \n",
    "    model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, verbose=verbose, class_weight = class_weight)\n",
    "    \n",
    "    loss, accuracy = model.evaluate(X_val, y_val, batch_size=batch_size, verbose=0)\n",
    "    \n",
    "\n",
    "    y_pred = (model.predict(X_val) > 0.5).astype(\"int32\")\n",
    "    y_val = np.argmax(y_val, axis=1)\n",
    "    y_pred = np.argmax(y_pred, axis=1)\n",
    "\n",
    "    confus = confusion_matrix(y_val, y_pred, labels=None, sample_weight=None)\n",
    "    tp = confus[1][1]\n",
    "    tn = confus[0][0] + confus[0][2] + confus[2][0] + confus[2][2]\n",
    "    fp = confus[1][0] + confus[1][2]\n",
    "    fn = confus[0][1] + confus[2][1]\n",
    "\n",
    "    precision = (tp/(tp + fp))*100\n",
    "    recall =  (tp / (tp + fn))*100   #sensitivity\n",
    "    F1_score = ((2*tp) / (2*tp + fp + fn))*100\n",
    "\n",
    "    score = accuracy\n",
    "    losses.append(loss)\n",
    "    \n",
    "    score = score * 100.0\n",
    "    scores_in_fold.append(score)\n",
    "    \n",
    "    print('Loss: %.3f%% Accuracy: %.3f%% ' % (loss, score))\n",
    "    print('precision: %.3f%% recall: %.3f%% F1 score: %.3f%%' % (precision, recall, F1_score))\n",
    "    # print(confus)\n",
    "\n",
    "\n",
    "    \n",
    "    '''score = evaluate_model(X_train, y_train, X_val, y_val)\n",
    "    score = score * 100.0\n",
    "    print(score)\n",
    "    scores.append(score)'''\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 86.746% (+/-2.096)\n"
     ]
    }
   ],
   "source": [
    "m, s = np.mean(scores_in_fold), np.std(scores_in_fold)\n",
    "print('Accuracy: %.3f%% (+/-%.3f)' % (m, s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = to_3Darray(X_test)\n",
    "#y_test = to_categorical(y_test)\n",
    "X_test = X_test.reshape((X_test.shape[0], n_steps, n_length, n_features))\n",
    "\n",
    "\n",
    "# loss, accuracy = model.evaluate(X_test, y_test, batch_size=batch_size, verbose=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = to_categorical(y_test)\n",
    "y_pred = (model.predict(X_test) > 0.5).astype(\"int32\")\n",
    "y_test = np.argmax(y_test, axis=1)\n",
    "y_pred = np.argmax(y_pred, axis=1)\n",
    "\n",
    "confus = confusion_matrix(y_test, y_pred, labels=None, sample_weight=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3894  408    7]\n",
      " [  76  530    5]\n",
      " [  15   31    3]]\n"
     ]
    }
   ],
   "source": [
    "print(confus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "89.092372710807"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, y_pred)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 88.374% \n"
     ]
    }
   ],
   "source": [
    "accuracy = accuracy*100\n",
    "print('Accuracy: %.3f%% ' % (score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('my_model.h5')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "cdb0dce53ee342b4df2ec29da00f64db1b0b5ad6c6a758debba5208518f3513a"
  },
  "kernelspec": {
   "display_name": "Python 3.8.11 64-bit ('HighHeel': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
