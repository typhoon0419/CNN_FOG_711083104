{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = os.getcwd() + \"/dataset/original_data\"\n",
    "first_processing_path = os.getcwd() +\"/dataset/raw_labelled\"\n",
    "second_processing_path = os.getcwd() +\"/dataset/windows\"\n",
    "final_dataset_path = os.getcwd() +\"/dataset\"\n",
    "test_patients = [\"S01\", \"S02\"]\n",
    "\n",
    "window_length = 2*64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "people[] -> 存所有在資料夾中的txt檔\n",
    "\n",
    "'''\n",
    "people = [] #S01R01.txt~S10R01.txt\n",
    "\n",
    "for person in os.listdir(data_path):\n",
    "    if '.txt' in person:\n",
    "        people.append(person)\n",
    "\n",
    "\n",
    "del person"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_prefog(dataset, window_length):\n",
    "# def label_prefog(dataset, window_length = 1):\n",
    "    # Drop all the rows for which Action is 0 or the rows which are not part of the experiment\n",
    "    # 將所有的Action==0(未實驗)的部分刪除，並不回傳任何值(implace的部分)\n",
    "    dataset.drop(index = list(dataset[dataset['Action'] == 0].index),\n",
    "                 inplace = True)\n",
    "\n",
    "    time = dataset[\"time\"].values\n",
    "    dataset.drop([\"time\"], axis=1, inplace=True)\n",
    "    time = (time-time.min())/1000\n",
    "    temp = pd.Series(time)\n",
    "    dataset[\"time\"] = temp.values\n",
    "    dataset = dataset[['time',\n",
    "                                'A_F', 'A_V', 'A_L',\n",
    "                                'L_F', 'L_V', 'L_L',\n",
    "                                'T_F', 'T_V', 'T_L',\n",
    "                                'Action']]\n",
    "    # window_length = int(64*window_length)\n",
    "    #print(window_length)\n",
    "    fog_index = []\n",
    "    for i in dataset.index:\n",
    "        if dataset.loc[i, 'Action'] == 2: # 取index和'Action'對應到的資料\n",
    "            fog_index.append(i) # 將發生fog的index存入fog_index這個array\n",
    "    fog_index\n",
    "    \n",
    "    # print(fog_index)\n",
    "    \n",
    "    start_indices = []\n",
    "    for i in fog_index:\n",
    "        if (dataset.loc[i-1, 'Action'] != dataset.loc[i, 'Action']):\n",
    "            start_indices.append(i) # 將發生fog之前的index存入start_indices這個array\n",
    "    \n",
    "    # print(start_indices)\n",
    "\n",
    "    prefog = []\n",
    "    for start in start_indices:\n",
    "        prefog_start = [x for x in range(start-window_length, start)] # prefog_start把FOG之前的windows標記起來\n",
    "        prefog.append(prefog_start)\n",
    "        \n",
    "    # print(prefog) # 2D array\n",
    "\n",
    "    prefog = [item for sublist in prefog for item in sublist] # 把[][]轉換成[]\n",
    "\n",
    "    # print(prefog) # 1D array\n",
    "\n",
    "    for i in prefog:\n",
    "        if dataset.loc[i, \"Action\"] == 2:\n",
    "            print (i)\n",
    "        dataset.loc[i,'Action'] = 3 # 把紀錄在Pre-fog的index action 記錄成3\n",
    "    dataset['Action'] = dataset['Action'] - 1 # 所有action編號-1 使0=walk 1=fog 2= pre-fog\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S01\n",
      "S01R01.txt  is read\tAdding S01R01.txt to dataset\tS01R01.txt is labelled\n",
      "\n",
      "S01\n",
      "S01R02.txt  is read\tAdding S01R02.txt to dataset\tS01R02.txt is labelled\n",
      "\n",
      "S02\n",
      "S02R01.txt  is read\tAdding S02R01.txt to dataset\t56092\n",
      "56093\n",
      "56094\n",
      "56095\n",
      "56096\n",
      "56097\n",
      "56098\n",
      "56099\n",
      "56100\n",
      "56101\n",
      "56102\n",
      "56103\n",
      "56104\n",
      "56105\n",
      "56106\n",
      "56107\n",
      "56108\n",
      "56109\n",
      "56110\n",
      "56111\n",
      "56112\n",
      "56113\n",
      "56114\n",
      "56115\n",
      "56116\n",
      "56117\n",
      "56118\n",
      "56119\n",
      "56120\n",
      "56121\n",
      "56122\n",
      "56123\n",
      "56124\n",
      "56125\n",
      "56126\n",
      "56127\n",
      "56128\n",
      "56129\n",
      "56130\n",
      "56131\n",
      "56132\n",
      "56133\n",
      "56134\n",
      "56135\n",
      "56136\n",
      "56137\n",
      "56138\n",
      "56139\n",
      "56140\n",
      "56141\n",
      "56142\n",
      "S02R01.txt is labelled\n",
      "\n",
      "S02\n",
      "S02R02.txt  is read\tAdding S02R02.txt to dataset\t34763\n",
      "34764\n",
      "34765\n",
      "34766\n",
      "34767\n",
      "34768\n",
      "34769\n",
      "34770\n",
      "34771\n",
      "34772\n",
      "34773\n",
      "34774\n",
      "34775\n",
      "34776\n",
      "34777\n",
      "34778\n",
      "34779\n",
      "34780\n",
      "34781\n",
      "34782\n",
      "34783\n",
      "34784\n",
      "34785\n",
      "34786\n",
      "34787\n",
      "34788\n",
      "34789\n",
      "34790\n",
      "34791\n",
      "34792\n",
      "34793\n",
      "34794\n",
      "34795\n",
      "34796\n",
      "34797\n",
      "34798\n",
      "34799\n",
      "34800\n",
      "34801\n",
      "34802\n",
      "34803\n",
      "34804\n",
      "34805\n",
      "34806\n",
      "34807\n",
      "34808\n",
      "S02R02.txt is labelled\n",
      "\n",
      "S03\n",
      "S03R01.txt  is read\tAdding S03R01.txt to dataset\t75661\n",
      "75662\n",
      "75663\n",
      "75664\n",
      "75665\n",
      "75666\n",
      "75667\n",
      "75668\n",
      "75669\n",
      "75670\n",
      "75671\n",
      "75672\n",
      "75673\n",
      "75674\n",
      "75675\n",
      "75676\n",
      "75677\n",
      "75678\n",
      "75679\n",
      "75680\n",
      "75681\n",
      "75810\n",
      "75811\n",
      "75812\n",
      "75813\n",
      "75814\n",
      "75815\n",
      "75816\n",
      "75817\n",
      "75818\n",
      "75819\n",
      "75820\n",
      "75821\n",
      "75822\n",
      "75823\n",
      "75824\n",
      "75825\n",
      "75826\n",
      "75827\n",
      "75828\n",
      "75829\n",
      "75830\n",
      "75831\n",
      "75832\n",
      "75833\n",
      "75834\n",
      "75835\n",
      "75836\n",
      "75837\n",
      "75838\n",
      "75839\n",
      "75840\n",
      "75841\n",
      "75842\n",
      "75843\n",
      "75844\n",
      "75845\n",
      "75846\n",
      "75847\n",
      "75848\n",
      "75849\n",
      "75850\n",
      "75851\n",
      "75852\n",
      "75853\n",
      "75854\n",
      "75855\n",
      "75856\n",
      "75857\n",
      "75858\n",
      "75859\n",
      "75860\n",
      "75861\n",
      "75862\n",
      "75863\n",
      "75864\n",
      "75865\n",
      "83692\n",
      "83693\n",
      "83694\n",
      "83695\n",
      "83696\n",
      "83697\n",
      "83698\n",
      "83699\n",
      "83700\n",
      "83701\n",
      "83702\n",
      "83703\n",
      "83704\n",
      "83705\n",
      "83706\n",
      "83707\n",
      "83708\n",
      "83709\n",
      "83710\n",
      "83711\n",
      "83712\n",
      "83713\n",
      "83714\n",
      "105215\n",
      "105216\n",
      "105217\n",
      "105218\n",
      "105219\n",
      "105220\n",
      "105221\n",
      "105222\n",
      "105223\n",
      "105224\n",
      "105225\n",
      "105226\n",
      "105227\n",
      "105228\n",
      "105229\n",
      "105230\n",
      "105231\n",
      "105232\n",
      "105233\n",
      "105234\n",
      "105235\n",
      "105236\n",
      "105237\n",
      "105238\n",
      "105239\n",
      "105240\n",
      "105241\n",
      "105242\n",
      "105243\n",
      "105244\n",
      "105245\n",
      "105246\n",
      "131198\n",
      "131199\n",
      "131200\n",
      "131201\n",
      "131202\n",
      "131203\n",
      "131204\n",
      "131205\n",
      "131206\n",
      "131207\n",
      "131208\n",
      "131209\n",
      "131210\n",
      "131211\n",
      "131212\n",
      "131213\n",
      "131214\n",
      "131215\n",
      "131216\n",
      "131217\n",
      "131218\n",
      "131219\n",
      "131220\n",
      "131221\n",
      "131222\n",
      "131223\n",
      "131224\n",
      "131225\n",
      "131226\n",
      "131227\n",
      "131228\n",
      "131229\n",
      "131230\n",
      "131231\n",
      "131232\n",
      "131233\n",
      "131234\n",
      "131235\n",
      "S03R01.txt is labelled\n",
      "\n",
      "S03\n",
      "S03R02.txt  is read\tAdding S03R02.txt to dataset\t22392\n",
      "22393\n",
      "22394\n",
      "22395\n",
      "22396\n",
      "22397\n",
      "22398\n",
      "22399\n",
      "22400\n",
      "22401\n",
      "22402\n",
      "22403\n",
      "22404\n",
      "22405\n",
      "22406\n",
      "22407\n",
      "22408\n",
      "22409\n",
      "22410\n",
      "22411\n",
      "22412\n",
      "22413\n",
      "22414\n",
      "22415\n",
      "22416\n",
      "22417\n",
      "22418\n",
      "22419\n",
      "22420\n",
      "22421\n",
      "22422\n",
      "22423\n",
      "22424\n",
      "22425\n",
      "22426\n",
      "22427\n",
      "22428\n",
      "22429\n",
      "22430\n",
      "22431\n",
      "22432\n",
      "22433\n",
      "22434\n",
      "22435\n",
      "22436\n",
      "22437\n",
      "22438\n",
      "22439\n",
      "22440\n",
      "22441\n",
      "22442\n",
      "22443\n",
      "22444\n",
      "22445\n",
      "22446\n",
      "22447\n",
      "22448\n",
      "22449\n",
      "22450\n",
      "22451\n",
      "22452\n",
      "22453\n",
      "22454\n",
      "22455\n",
      "22456\n",
      "S03R02.txt is labelled\n",
      "\n",
      "S03\n",
      "S03R03.txt  is read\t\n",
      "S04\n",
      "S04R01.txt  is read\t\n",
      "S05\n",
      "S05R01.txt  is read\tAdding S05R01.txt to dataset\t59060\n",
      "59061\n",
      "59062\n",
      "59063\n",
      "59064\n",
      "59065\n",
      "59066\n",
      "59067\n",
      "59068\n",
      "59069\n",
      "59070\n",
      "59071\n",
      "59072\n",
      "59073\n",
      "59074\n",
      "59075\n",
      "59076\n",
      "65712\n",
      "65713\n",
      "65714\n",
      "65715\n",
      "65716\n",
      "65717\n",
      "65718\n",
      "65719\n",
      "65720\n",
      "65721\n",
      "65722\n",
      "65723\n",
      "65724\n",
      "65725\n",
      "65726\n",
      "65727\n",
      "80596\n",
      "80597\n",
      "80598\n",
      "80599\n",
      "80600\n",
      "80601\n",
      "80602\n",
      "80603\n",
      "80604\n",
      "80605\n",
      "80606\n",
      "80607\n",
      "80608\n",
      "80609\n",
      "80610\n",
      "80611\n",
      "80612\n",
      "80613\n",
      "80614\n",
      "80615\n",
      "80616\n",
      "80617\n",
      "80618\n",
      "80619\n",
      "80620\n",
      "80621\n",
      "80622\n",
      "80623\n",
      "80624\n",
      "80625\n",
      "80626\n",
      "80627\n",
      "80628\n",
      "80629\n",
      "80630\n",
      "80631\n",
      "80632\n",
      "80633\n",
      "80634\n",
      "80635\n",
      "80636\n",
      "80637\n",
      "80638\n",
      "80639\n",
      "80640\n",
      "80641\n",
      "80642\n",
      "98548\n",
      "98549\n",
      "98550\n",
      "98551\n",
      "98552\n",
      "98553\n",
      "98554\n",
      "98555\n",
      "98556\n",
      "98557\n",
      "98558\n",
      "98559\n",
      "98560\n",
      "98561\n",
      "98562\n",
      "98563\n",
      "98564\n",
      "98565\n",
      "98566\n",
      "98567\n",
      "98568\n",
      "98569\n",
      "98570\n",
      "98571\n",
      "98572\n",
      "98573\n",
      "98574\n",
      "98575\n",
      "98576\n",
      "98577\n",
      "98578\n",
      "98579\n",
      "98580\n",
      "98581\n",
      "98582\n",
      "98583\n",
      "98584\n",
      "98585\n",
      "98586\n",
      "98587\n",
      "98588\n",
      "98589\n",
      "98590\n",
      "98591\n",
      "98592\n",
      "98593\n",
      "98594\n",
      "98595\n",
      "98596\n",
      "98597\n",
      "98598\n",
      "98599\n",
      "98600\n",
      "98601\n",
      "98602\n",
      "98603\n",
      "98604\n",
      "98605\n",
      "98606\n",
      "98607\n",
      "98608\n",
      "98609\n",
      "98610\n",
      "98611\n",
      "102669\n",
      "102670\n",
      "102671\n",
      "102672\n",
      "102673\n",
      "102674\n",
      "102675\n",
      "102676\n",
      "102677\n",
      "102678\n",
      "102679\n",
      "102680\n",
      "102681\n",
      "102682\n",
      "102683\n",
      "S05R01.txt is labelled\n",
      "\n",
      "S05\n",
      "S05R02.txt  is read\tAdding S05R02.txt to dataset\t91232\n",
      "91233\n",
      "91234\n",
      "91235\n",
      "91236\n",
      "91237\n",
      "91238\n",
      "91239\n",
      "91240\n",
      "91241\n",
      "91242\n",
      "91243\n",
      "91244\n",
      "91245\n",
      "91246\n",
      "91247\n",
      "91248\n",
      "91249\n",
      "91250\n",
      "91251\n",
      "91252\n",
      "S05R02.txt is labelled\n",
      "\n",
      "S06\n",
      "S06R01.txt  is read\tAdding S06R01.txt to dataset\tS06R01.txt is labelled\n",
      "\n",
      "S06\n",
      "S06R02.txt  is read\t\n",
      "S07\n",
      "S07R01.txt  is read\tAdding S07R01.txt to dataset\tS07R01.txt is labelled\n",
      "\n",
      "S07\n",
      "S07R02.txt  is read\tAdding S07R02.txt to dataset\tS07R02.txt is labelled\n",
      "\n",
      "S08\n",
      "S08R01.txt  is read\tAdding S08R01.txt to dataset\tS08R01.txt is labelled\n",
      "\n",
      "S09\n",
      "S09R01.txt  is read\tAdding S09R01.txt to dataset\tS09R01.txt is labelled\n",
      "\n",
      "S10\n",
      "S10R01.txt  is read\t\n"
     ]
    }
   ],
   "source": [
    "dataset = pd.DataFrame()\n",
    "last_name = '' # 紀錄哪個是最終的病患\n",
    "patients = [] # 病患ID list\n",
    "i = 1\n",
    "for person in people:\n",
    "    name = person.split('R')[0]\n",
    "\n",
    "    if name != last_name:\n",
    "        dataset = dataset.drop(index=dataset.index)\n",
    "        patients.append(name)\n",
    "    last_name = name\n",
    "    print(name)\n",
    "\n",
    "    file = data_path + \"\\\\\" + person\n",
    "\n",
    "    if name in person:\n",
    "        temp = pd.read_csv(file, delimiter = \" \", header = None)\n",
    "        print(person, ' is read', end = \"\\t\")\n",
    "        \n",
    "        if 2 in temp[max(temp.columns)].unique():\n",
    "            print(\"Adding {} to dataset\".format(person), end = \"\\t\")\n",
    "            temp.columns = [\n",
    "                                'time',\n",
    "                                'A_F', 'A_V', 'A_L',\n",
    "                                'L_F', 'L_V', 'L_L',\n",
    "                                'T_F', 'T_V', 'T_L',\n",
    "                                'Action'                           \n",
    "                                ]\n",
    "            \n",
    "            temp = label_prefog(temp, window_length).reset_index(drop = True)\n",
    "            temp['name'] = name\n",
    "            print(\"{} is labelled\".format(person))\n",
    "            dataset = pd.concat([dataset, temp],axis = 0)\n",
    "        print('')\n",
    "\n",
    "        dataset.reset_index(drop = True, inplace = True)\n",
    "        \n",
    "        to_name = first_processing_path + \"/win_\" + name + \".csv\"\n",
    "    # 存入/raw_labelled中\n",
    "\n",
    "    if not dataset.empty:\n",
    "        dataset.to_csv(to_name, index = False)\n",
    "\n",
    "del last_name, name, person, people, dataset, temp, to_name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_window(act,window_length,dataframe):\n",
    "\n",
    "  indices = list(dataframe[dataframe.Action == act].index)\n",
    "  time = []\n",
    "  time_count = 0\n",
    "  for j in indices:\n",
    "    time.append(dataframe.loc[j, 'time'])\n",
    "\n",
    "\n",
    "  # indices記錄所有Action==act的index\n",
    "  groups = [] # 用來暫存一組(同action)資料的, 型態是[][]\n",
    "  temp = [] # 用來暫存一行資料的\n",
    "  group_count = 0\n",
    "  for i in range(len(indices)):\n",
    "    if i == len(indices)-1:\n",
    "      temp.append(indices[i])\n",
    "      groups.append(temp)\n",
    "      temp = []\n",
    "      break # 如果i已經來到最後的話就break\n",
    "    temp.append(indices[i])\n",
    "    #time_count = time_count + 1\n",
    "    if time[i+1]-16 > time[i]: #如果下個index不是連續的話, 就將前面這些存成第一組\n",
    "      group_count+=1\n",
    "      #time_count = time_count + 1\n",
    "      groups.append(temp)\n",
    "      temp = []\n",
    "\n",
    "  #print(groups)\n",
    "\n",
    "  fs = 64\n",
    "  # window_length = 1\n",
    "  # window_length = int(window_length*fs)\n",
    "\n",
    "  final_dataframe = pd.DataFrame()\n",
    "  sumOfAct=0\n",
    "\n",
    "  for i in groups: # group[][]的每一行i\n",
    "    required = math.floor(len(i)/int(window_length/2))\n",
    "\n",
    "    \n",
    "    sumOfAct= sumOfAct+required\n",
    "\n",
    "    req_index = i[0:(required*int(window_length/2))]\n",
    "\n",
    "    #print(req_index)\n",
    "    # concat([要結合的data集合], axis=0是方向為直的)\n",
    "    final_dataframe = pd.concat([final_dataframe,dataframe.iloc[req_index,:]],axis = 0)\n",
    "  \n",
    "  \n",
    "\n",
    "  return final_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for patient in patients:\n",
    "\n",
    "  name = first_processing_path + \"/win_\"+ patient +\".csv\"\n",
    "\n",
    "  if os.path.isfile(name):\n",
    "    dataset = pd.read_csv(name)\n",
    "\n",
    "    activities = []\n",
    "    # print(patient)\n",
    "    for act in range(3):\n",
    "      activities.append(create_window(act,window_length,dataset))\n",
    "    to_write = pd.concat(activities,axis = 0)\n",
    "    to_path = second_processing_path + \"/windowed_\" + patient + \".csv\"\n",
    "    to_write.to_csv(to_path,index = False)\n",
    "\n",
    "del name, to_path, to_write, dataset, activities, patient, act"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 將 train test 以 patients 區分\n",
    "\n",
    "train_patients_dataset = pd.DataFrame()\n",
    "test_patients_dataset = pd.DataFrame()\n",
    "\n",
    "for patient in patients:\n",
    "    name = second_processing_path + \"\\\\windowed_\" + patient + \".csv\"\n",
    "\n",
    "    if not os.path.isfile(name):\n",
    "        continue\n",
    "    \n",
    "    dataset = pd.read_csv(name)\n",
    "\n",
    "    if patient == test_patients[0] or patient == test_patients[1]:\n",
    "        test_patients_dataset = test_patients_dataset.append(dataset)\n",
    "    else:    \n",
    "        train_patients_dataset = train_patients_dataset.append(dataset)\n",
    "\n",
    "to_path = final_dataset_path + \"/not_windowed_train.csv\"\n",
    "train_patients_dataset.to_csv(to_path, index=False)\n",
    "\n",
    "to_path = final_dataset_path + \"/not_windowed_test.csv\"\n",
    "test_patients_dataset.to_csv(to_path, index=False)\n",
    "\n",
    "del name, dataset, patient, test_patients_dataset, train_patients_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = os.getcwd() + \"/dataset/not_windowed_train.csv\"\n",
    "dataset = pd.read_csv(data_path)\n",
    "\n",
    "import math\n",
    "\n",
    "activities = []\n",
    "for act in range(3):\n",
    "    activities.append(create_window(act,window_length,dataset))\n",
    "to_write = pd.concat(activities, axis = 0)\n",
    "to_path = os.getcwd() + \"/dataset/total_train.csv\"\n",
    "to_write.to_csv(to_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = os.getcwd() + \"/dataset/not_windowed_test.csv\"\n",
    "dataset = pd.read_csv(data_path)\n",
    "\n",
    "import math\n",
    "\n",
    "activities = []\n",
    "for act in range(3):\n",
    "    activities.append(create_window(act,window_length,dataset))\n",
    "to_write = pd.concat(activities, axis = 0)\n",
    "to_path = os.getcwd() + \"/dataset/total_test.csv\"\n",
    "to_write.to_csv(to_path)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8986fb416174cc2474d1ce69838b7b56508ac61c47a66825c7584556038319d3"
  },
  "kernelspec": {
   "display_name": "Python 3.8.11 ('HighHeelWhatever')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
