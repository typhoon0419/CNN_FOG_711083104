{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FOG 事件的長度範圍為 0.5 至 40.5 s（平均 7.3 s [SD 6.7 s]）。50% 的 FOG 發作持續時間少於 5.4 秒，大多數（93.2%）的時間少於 20 秒（見圖6）。這些結果類似於早期的 FOG 持續時間表徵[8]。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, SeparableConv1D\n",
    "from tensorflow.keras.layers import LSTM, TimeDistributed, ConvLSTM2D\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix,accuracy_score,f1_score,roc_auc_score,recall_score,precision_score\n",
    "from keras import backend as k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_path = os.getcwd() + \"/dataset/total_train.csv\"\n",
    "test_data_path = os.getcwd() + \"/dataset/total_test.csv\"\n",
    "# features = [\"A_F\", \"A_V\", \"A_L\", \"L_F\", \"L_T\", \"L_L\", \"T_F\", \"T_V\", \"T_L\"]\n",
    "\n",
    "train_dataset = pd.read_csv(train_data_path)\n",
    "train_time = list(train_dataset['time'])\n",
    "train_dataframe = train_dataset[[\"A_F\", \"A_V\", \"A_L\", \"L_F\", \"L_V\", \"L_L\", \"T_F\", \"T_V\", \"T_L\"]].values\n",
    "train_dataset = train_dataset[[\"A_F\", \"A_V\", \"A_L\", \"L_F\", \"L_V\", \"L_L\", \"T_F\", \"T_V\", \"T_L\", \"Action\"]].values\n",
    "\n",
    "window_length = int(1*64)\n",
    "train_total_windows = int((len(train_dataset))/window_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_patients = [\"S02\"]\n",
    "\n",
    "test_dataset = pd.read_csv(test_data_path)\n",
    "test_time = list(test_dataset['time'])\n",
    "test_dataframe = test_dataset[[\"A_F\", \"A_V\", \"A_L\", \"L_F\", \"L_V\", \"L_L\", \"T_F\", \"T_V\", \"T_L\"]].values\n",
    "test_dataset = test_dataset[[\"A_F\", \"A_V\", \"A_L\", \"L_F\", \"L_V\", \"L_L\", \"T_F\", \"T_V\", \"T_L\", \"Action\"]].values\n",
    "\n",
    "test_total_windows = int((len(test_dataset))/window_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataframe = (train_dataframe-train_dataframe.mean())/(train_dataframe.std())\n",
    "test_dataframe = (test_dataframe-test_dataframe.mean())/(test_dataframe.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "確認每段資料能不能整除64/2, 並輸出不能整除的個數"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "indices = train_time\n",
    "groups = []\n",
    "group_count = 0\n",
    "temp = []\n",
    "lenOfGroup = []\n",
    "length_count = 0\n",
    "\n",
    "for i in range(len(indices)):\n",
    "    if i == (len(indices) - 1):\n",
    "        temp.append(indices[i])\n",
    "        length_count = length_count + 1\n",
    "\n",
    "        groups.append(temp)\n",
    "        lenOfGroup.append(length_count)\n",
    "        length_count = 0\n",
    "        temp = []\n",
    "        break\n",
    "    temp.append(indices[i])\n",
    "    length_count = length_count + 1\n",
    "    if (indices[i+1] - 20 > indices[i]):\n",
    "        group_count = group_count + 1\n",
    "\n",
    "        lenOfGroup.append(length_count)\n",
    "        length_count = 0\n",
    "\n",
    "        groups.append(temp)\n",
    "        temp = []\n",
    "\n",
    "del temp, indices, group_count, length_count, i\n",
    "\n",
    "countOfUndivisible = 0\n",
    "total_windows_with_overlap_train = 0\n",
    "\n",
    "start_index = 0\n",
    "\n",
    "stop_Indexs = []\n",
    "stop_Index = -window_length\n",
    "\n",
    "\n",
    "for lengths in lenOfGroup:\n",
    "    stop_Index = stop_Index + lengths\n",
    "    stop_Indexs.append(stop_Index)\n",
    "    total_windows_with_overlap_train = total_windows_with_overlap_train + int(float(lengths/window_length)*2 -1)\n",
    "    \n",
    "    if lengths % (window_length/2) != 0:\n",
    "        countOfUndivisible = countOfUndivisible + 1\n",
    "        print(lengths)\n",
    "\n",
    "print(countOfUndivisible)\n",
    "del countOfUndivisible, lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.empty((total_windows_with_overlap_train, window_length*9))\n",
    "y = np.empty((total_windows_with_overlap_train, 1))\n",
    "\n",
    "stop_Index = 0\n",
    "window_count = 0\n",
    "\n",
    "for window in range(total_windows_with_overlap_train):\n",
    "    for i in range(window_length):\n",
    "        if i == 0:\n",
    "            y[window] = train_dataset[int(window_count*window_length), 9]\n",
    "        \n",
    "        if int(window_count*window_length)<len(train_dataset)-window_length-1:\n",
    "            for data in range(9):\n",
    "               X[window, i*9 + data] = train_dataframe[int(window_count*window_length) + i, data]\n",
    "        if stop_Index < len(stop_Indexs):\n",
    "            if int(window_count*window_length) == stop_Indexs[stop_Index]:\n",
    "                window_count = window_count + 0.5\n",
    "                stop_Index = stop_Index + 1\n",
    "        if window == total_windows_with_overlap_train-1:\n",
    "            X[window, i*9 + data] = train_dataframe[int((window_count-0.5)*window_length) + i, data]\n",
    "    window_count = window_count + 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "indices = test_time\n",
    "groups = []\n",
    "group_count = 0\n",
    "temp = []\n",
    "lenOfGroup = []\n",
    "length_count = 0\n",
    "\n",
    "for i in range(len(indices)):\n",
    "    if i == (len(indices) - 1):\n",
    "        temp.append(indices[i])\n",
    "        length_count = length_count + 1\n",
    "\n",
    "        groups.append(temp)\n",
    "        lenOfGroup.append(length_count)\n",
    "        length_count = 0\n",
    "        temp = []\n",
    "        break\n",
    "    temp.append(indices[i])\n",
    "    length_count = length_count + 1\n",
    "    if (indices[i+1] - 20 > indices[i]):\n",
    "        group_count = group_count + 1\n",
    "\n",
    "        lenOfGroup.append(length_count)\n",
    "        length_count = 0\n",
    "\n",
    "        groups.append(temp)\n",
    "        temp = []\n",
    "\n",
    "del temp, indices, group_count, length_count, i\n",
    "\n",
    "countOfUndivisible = 0\n",
    "total_windows_with_overlap_test = 0\n",
    "\n",
    "start_index = 0\n",
    "\n",
    "stop_Indexs = []\n",
    "stop_Index = -window_length\n",
    "\n",
    "\n",
    "for lengths in lenOfGroup:\n",
    "    stop_Index = stop_Index + lengths\n",
    "    stop_Indexs.append(stop_Index)\n",
    "    total_windows_with_overlap_test = total_windows_with_overlap_test + int(float(lengths/window_length)*2 -1)\n",
    "    \n",
    "    if lengths % (window_length/2) != 0:\n",
    "        countOfUndivisible = countOfUndivisible + 1\n",
    "        print(lengths)\n",
    "\n",
    "print(countOfUndivisible)\n",
    "del countOfUndivisible, lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = np.empty((total_windows_with_overlap_test, window_length*9))\n",
    "y_test = np.empty((total_windows_with_overlap_test, 1))\n",
    "\n",
    "stop_Index = 0\n",
    "window_count = 0\n",
    "\n",
    "for window in range(total_windows_with_overlap_test):\n",
    "    for i in range(window_length):\n",
    "        if i == 0:\n",
    "            y_test[window] = test_dataset[int(window_count*window_length), 9]\n",
    "        \n",
    "        if int(window_count*window_length)<len(test_dataset)-window_length-1:\n",
    "            for data in range(9):\n",
    "               X_test[window, i*9 + data] = test_dataframe[int(window_count*window_length) + i, data]\n",
    "        if stop_Index < len(stop_Indexs):\n",
    "            if int(window_count*window_length) == stop_Indexs[stop_Index]:\n",
    "                window_count = window_count + 0.5\n",
    "                stop_Index = stop_Index + 1\n",
    "        if window == total_windows_with_overlap_test-1:\n",
    "            X_test[window, i*9 + data] = test_dataframe[int((window_count-0.5)*window_length) + i, data]\n",
    "    window_count = window_count + 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique, counts = np.unique(y, return_counts=True)\n",
    "\n",
    "class_weight = {0:(1/counts[0])*len(y)/2, 1:(1/counts[1])*len(y)/2, 2:(1/counts[2])*len(y)/2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StratifiedKFold(n_splits=10, random_state=42, shuffle=True)\n"
     ]
    }
   ],
   "source": [
    "skf = StratifiedKFold(n_splits=10, shuffle = True, random_state=42)\n",
    "skf.get_n_splits(X, y)\n",
    "print(skf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_3Darray(array):\n",
    "    arr_3d = np.empty((len(array), window_length, 9))\n",
    "\n",
    "\n",
    "    arr_3d = np.reshape(array, (len(array), window_length, 9))\n",
    "    return arr_3d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_in_fold = []\n",
    "losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_outside_fold = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Fold #0\n",
      "Loss: 0.282% Accuracy: 88.988% \n",
      "precision: 89.841% recall: 49.736% F1 score: 64.027%\n",
      "==> Fold #1\n",
      "Loss: 0.281% Accuracy: 87.933% \n",
      "precision: 87.937% recall: 49.464% F1 score: 63.314%\n",
      "==> Fold #2\n",
      "Loss: 0.299% Accuracy: 86.845% \n",
      "precision: 95.238% recall: 46.802% F1 score: 62.762%\n",
      "==> Fold #3\n",
      "Loss: 0.334% Accuracy: 87.669% \n",
      "precision: 89.206% recall: 48.365% F1 score: 62.723%\n",
      "==> Fold #4\n",
      "Loss: 0.333% Accuracy: 87.405% \n",
      "precision: 89.524% recall: 48.621% F1 score: 63.017%\n",
      "==> Fold #5\n",
      "Loss: 0.309% Accuracy: 87.076% \n",
      "precision: 91.456% recall: 46.019% F1 score: 61.229%\n",
      "==> Fold #6\n",
      "Loss: 0.374% Accuracy: 83.647% \n",
      "precision: 95.253% recall: 40.897% F1 score: 57.224%\n",
      "==> Fold #7\n",
      "Loss: 0.312% Accuracy: 87.141% \n",
      "precision: 90.506% recall: 46.278% F1 score: 61.242%\n",
      "==> Fold #8\n",
      "Loss: 0.317% Accuracy: 87.076% \n",
      "precision: 89.873% recall: 46.405% F1 score: 61.207%\n",
      "==> Fold #9\n",
      "Loss: 0.311% Accuracy: 88.094% \n",
      "precision: 92.698% recall: 48.993% F1 score: 64.105%\n"
     ]
    }
   ],
   "source": [
    "for i, (train_index, val_index) in enumerate(skf.split(X, y)):\n",
    "    print(\"==> Fold #%d\" % i)\n",
    "    X_train, X_val = X[train_index], X[val_index]\n",
    "    y_train, y_val = y[train_index], y[val_index]\n",
    "\n",
    "    X_train = to_3Darray(X_train)\n",
    "    y_train = to_categorical(y_train)\n",
    "\n",
    "    X_val = to_3Darray(X_val)\n",
    "    y_val = to_categorical(y_val)\n",
    "\n",
    "    verbose, epochs, batch_size = 0, 50, 64\n",
    "    n_timesteps, n_features, n_outputs = X_train.shape[1], X_train.shape[2], y_train.shape[1]\n",
    "\n",
    "    n_steps, n_length = 2, 32\n",
    "    X_train = X_train.reshape(\n",
    "        (X_train.shape[0], n_steps, n_length, n_features))\n",
    "    X_val = X_val.reshape((X_val.shape[0], n_steps, n_length, n_features))\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(TimeDistributed(Conv1D(filters=64, kernel_size=3, activation='relu'),\n",
    "                              input_shape=(None, n_length, n_features)))\n",
    "    model.add(TimeDistributed(\n",
    "        Conv1D(filters=64, kernel_size=3, activation='relu')))\n",
    "    model.add(TimeDistributed(\n",
    "        Conv1D(filters=64, kernel_size=3, activation='relu')))\n",
    "    model.add(TimeDistributed(\n",
    "        Conv1D(filters=64, kernel_size=3, activation='relu')))\n",
    "    model.add(TimeDistributed(Dropout(0.5)))\n",
    "    model.add(TimeDistributed(MaxPooling1D(pool_size=2)))\n",
    "    model.add(TimeDistributed(Flatten()))\n",
    "    model.add(LSTM(100))\n",
    "    model.add(Dropout(0.5))\n",
    "    hunderdOutput = Dense(100, activation='relu')\n",
    "    model.add(hunderdOutput)  # feature\n",
    "    # 試著輸出長度為100的向量(feature) 並絳維 看他的分布有無分開\n",
    "    model.add(Dense(3, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam',\n",
    "                  metrics=['accuracy'])  # 可能可以調weighting\n",
    "\n",
    "    model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size,\n",
    "              verbose=verbose, class_weight=class_weight)\n",
    "\n",
    "    loss, accuracy = model.evaluate(\n",
    "        X_val, y_val, batch_size=batch_size, verbose=0)\n",
    "\n",
    "    y_pred = (model.predict(X_val) > 0.5).astype(\"int32\")\n",
    "    y_val = np.argmax(y_val, axis=1)\n",
    "    y_pred = np.argmax(y_pred, axis=1)\n",
    "\n",
    "    confus = confusion_matrix(y_val, y_pred, labels=None, sample_weight=None)\n",
    "    tp = confus[1][1]\n",
    "    tn = confus[0][0] + confus[0][2] + confus[2][0] + confus[2][2]\n",
    "    fp = confus[1][0] + confus[1][2]\n",
    "    fn = confus[0][1] + confus[2][1]\n",
    "\n",
    "    precision = (tp/(tp + fp))*100\n",
    "    recall = (tp / (tp + fn))*100  # sensitivity\n",
    "    F1_score = ((2*tp) / (2*tp + fp + fn))*100\n",
    "\n",
    "    score = accuracy\n",
    "    losses.append(loss)\n",
    "\n",
    "    score = score * 100.0\n",
    "    scores_in_fold.append(score)\n",
    "\n",
    "    print('Loss: %.3f%% Accuracy: %.3f%% ' % (loss, score))\n",
    "    print('precision: %.3f%% recall: %.3f%% F1 score: %.3f%%' %\n",
    "          (precision, recall, F1_score))\n",
    "    # print(confus)\n",
    "\n",
    "    '''score = evaluate_model(X_train, y_train, X_val, y_val)\n",
    "    score = score * 100.0\n",
    "    print(score)\n",
    "    scores.append(score)'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "layerIndex = 7\n",
    "func = k.function([model.get_layer(index=0).input], model.get_layer(index=layerIndex).output)\n",
    "layerOutput = func([X_val])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "\n",
    "principalComponents = pca.fit_transform(layerOutput)\n",
    "\n",
    "principalDf = pd.DataFrame(data = principalComponents, columns = ['principal component 1', 'principal component 2'])\n",
    "y_val = pd.DataFrame(data = y_val, columns=['targets'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "finalDF = pd.concat([principalDf, y_val], axis=1, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# fig = plt.figure(figsize = (8,8))\n",
    "# ax = fig.add_subplot(1,1,1) \n",
    "# ax.set_xlabel('Principal Component 1', fontsize = 15)\n",
    "# ax.set_ylabel('Principal Component 2', fontsize = 15)\n",
    "# ax.set_title('2 component PCA', fontsize = 20)\n",
    "# targets = [0, 1, 2]\n",
    "# colors = ['r', 'g', 'b']\n",
    "# for target, color in zip(targets,colors):\n",
    "#     indicesToKeep = finalDF[2] == target\n",
    "#     ax.scatter(finalDF.loc[indicesToKeep, 0]\n",
    "#                , finalDF.loc[indicesToKeep, 1]\n",
    "#                , c = color\n",
    "#                , s = 50)\n",
    "# ax.legend(targets)\n",
    "# ax.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 87.187% (+/-1.326)\n"
     ]
    }
   ],
   "source": [
    "m, s = np.mean(scores_in_fold), np.std(scores_in_fold)\n",
    "print('Accuracy: %.3f%% (+/-%.3f)' % (m, s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = to_3Darray(X_test)\n",
    "#y_test = to_categorical(y_test)\n",
    "X_test = X_test.reshape((X_test.shape[0], n_steps, n_length, n_features))\n",
    "\n",
    "\n",
    "# loss, accuracy = model.evaluate(X_test, y_test, batch_size=batch_size, verbose=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = to_categorical(y_test)\n",
    "y_pred = (model.predict(X_test) > 0.5).astype(\"int32\")\n",
    "y_test = np.argmax(y_test, axis=1)\n",
    "y_pred = np.argmax(y_pred, axis=1)\n",
    "\n",
    "confus = confusion_matrix(y_test, y_pred, labels=None, sample_weight=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2389 1107   47]\n",
      " [  27  163    3]\n",
      " [   3   31    1]]\n"
     ]
    }
   ],
   "source": [
    "print(confus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp = confus[1][1]\n",
    "tn = confus[0][0] + confus[0][2] + confus[2][0] + confus[2][2]\n",
    "fp = confus[1][0] + confus[1][2]\n",
    "fn = confus[0][1] + confus[2][1]\n",
    "\n",
    "precision = (tp/(tp + fp))*100\n",
    "sensitivity = (tp / (tp + fn))*100  # sensitivity\n",
    "specificity = (tn/(tn + fp))*100\n",
    "F1_score = ((2*tp) / (2*tp + fp + fn))*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = accuracy_score(y_test, y_pred)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 67.701% \n",
      "FOG: specificity: 98.785% sensitivity: 12.529% F1 score: 21.821%\n"
     ]
    }
   ],
   "source": [
    "accuracy = accuracy\n",
    "print('Accuracy: %.3f%% ' % (accuracy))\n",
    "print('FOG: specificity: %.3f%% sensitivity: %.3f%% F1 score: %.3f%%' % (specificity, sensitivity, F1_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PreFOG: specificity: 99.086% sensitivity: 1.961% F1 score: 2.326%\n"
     ]
    }
   ],
   "source": [
    "tp = confus[2][2]\n",
    "tn = confus[0][0] + confus[0][1] + confus[1][0] + confus[1][1]\n",
    "fp = confus[2][0] + confus[2][1]\n",
    "fn = confus[0][2] + confus[1][2]\n",
    "\n",
    "precision = (tp/(tp + fp))*100\n",
    "sensitivity = (tp / (tp + fn))*100  # sensitivity\n",
    "specificity = (tn/(tn + fp))*100\n",
    "F1_score = ((2*tp) / (2*tp + fp + fn))*100\n",
    "print('PreFOG: specificity: %.3f%% sensitivity: %.3f%% F1 score: %.3f%%' % (specificity, sensitivity, F1_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('my_model.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.11 ('HighHeelWhatever')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8986fb416174cc2474d1ce69838b7b56508ac61c47a66825c7584556038319d3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
