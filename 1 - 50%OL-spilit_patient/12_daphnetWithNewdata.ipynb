{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D\n",
    "from tensorflow.keras.layers import LSTM, TimeDistributed, ConvLSTM2D\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "from sklearn.metrics import confusion_matrix,accuracy_score\n",
    "from keras import backend as k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_path = os.getcwd() + \"/dataset/total_train.csv\"\n",
    "test_data_path = os.getcwd() + \"/dataset/total_test.csv\"\n",
    "new_data_path = os.getcwd() + \"/dataset/merge.csv\"\n",
    "\n",
    "\n",
    "winLen = int(1*64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTimeAndDF(path:str):\n",
    "    \"\"\"get dataset and dataset's time list\n",
    "       some dataset's time is not continued, so get the time is for split windows\n",
    "\n",
    "    Args:\n",
    "        path (str): path of the dataset(for .csv)\n",
    "\n",
    "    Returns:\n",
    "        time (list): the list of dataset time\n",
    "        df (np.array): dataset, columns include [\"A_F\", \"A_V\", \"A_L\", \"Action\"]\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(path)\n",
    "    time = list(df['time'])\n",
    "    action = list(df['Action'])\n",
    "    df = df[[\"A_F\", \"A_V\", \"A_L\"]].values\n",
    "\n",
    "    return time, df, action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainTime, trainData, trainAction = getTimeAndDF(train_data_path)\n",
    "testTime, testData, testAction = getTimeAndDF(test_data_path)\n",
    "newTime, newData, newAction = getTimeAndDF(new_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainData = (trainData-trainData.mean())/(trainData.std())\n",
    "testData = (testData-testData.mean())/(testData.std())\n",
    "newData = (newData-newData.mean())/(newData.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTotalWindows(indices:list, windowSize:int , gap: float):\n",
    "    \"\"\"because\n",
    "    check every part of time\n",
    "\n",
    "    Args:\n",
    "        indices (list): _description_\n",
    "        windowSize (int): _description_\n",
    "        gap (float): _description_\n",
    "\n",
    "    Returns:\n",
    "        _type_: _description_\n",
    "    \"\"\"\n",
    "    groups = []\n",
    "    group_count = 0\n",
    "    temp = []\n",
    "    lenOfGroup = []\n",
    "    length_count = 0\n",
    "    for i in range(len(indices)):\n",
    "        if i == (len(indices) - 1):\n",
    "            temp.append(indices[i])\n",
    "            length_count = length_count + 1\n",
    "\n",
    "            groups.append(temp)\n",
    "            lenOfGroup.append(length_count)\n",
    "            length_count = 0\n",
    "            temp = []\n",
    "            break\n",
    "        temp.append(indices[i])\n",
    "        length_count = length_count + 1\n",
    "        if (indices[i+1] - gap > indices[i]):\n",
    "            group_count = group_count + 1\n",
    "\n",
    "            lenOfGroup.append(length_count)\n",
    "            length_count = 0\n",
    "\n",
    "            groups.append(temp)\n",
    "            temp = []\n",
    "\n",
    "    countOfUndivisible = 0\n",
    "    totalWindows = 0\n",
    "\n",
    "    stop_Indexs = []\n",
    "    stop_Index = -windowSize\n",
    "\n",
    "\n",
    "    for lengths in lenOfGroup:\n",
    "        stop_Index = stop_Index + lengths\n",
    "        stop_Indexs.append(stop_Index)\n",
    "        totalWindows = totalWindows + int(float(lengths/windowSize)*2 -1)\n",
    "        \n",
    "        if lengths % (windowSize/2) != 0:\n",
    "            countOfUndivisible = countOfUndivisible + 1\n",
    "            print(lengths)\n",
    "\n",
    "    return totalWindows, stop_Indexs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainWindows, trainStop = getTotalWindows(trainTime, winLen, 20) \n",
    "newWindows, newStop = getTotalWindows(newTime, winLen, 0.02)\n",
    "testWindows, testStop = getTotalWindows(testTime, winLen, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [],
   "source": [
    "del trainTime, newTime, testTime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [],
   "source": [
    "def XySplit(dataset:np.array, windows:int, length:int, stop:list, action:list):\n",
    "    \"\"\"split dataset into X and y, \n",
    "    X is 2D array, size of X is [windows, 64*3]\n",
    "    y is 1D array, size of y is [windows, 1]\n",
    "\n",
    "    Args:\n",
    "        dataset (np.array): dataset\n",
    "        windows (int): total windows that get from getTotalWindows()\n",
    "        length (int): length of a piece of data, here is 3\n",
    "        stop (list): stopList that get from getTotalWindows()\n",
    "        action (list): action list\n",
    "\n",
    "    Returns:\n",
    "        X(np.array): X is 2D array, size of X is [windows, 64*3]\n",
    "        y(np.array): y is 1D array, size of y is [windows, 1]\n",
    "    \"\"\"\n",
    "    X = np.empty((windows, winLen*(length)))\n",
    "    y = np.empty((windows, 1))\n",
    "\n",
    "    stopIndex = 0\n",
    "    windowCount = 0\n",
    "    for win in range(windows):\n",
    "        for i in range(winLen):\n",
    "            if i == 0:\n",
    "                y[win] = action[int(windowCount*winLen)]\n",
    "\n",
    "            if int(windowCount*winLen)<len(dataset)-winLen-1:\n",
    "                for data in range(length):\n",
    "                    X[win, i*(length)+data] = dataset[int(windowCount*winLen) + i, data]\n",
    "\n",
    "            if stopIndex < len(stop):\n",
    "                if int(windowCount*winLen) == stop[stopIndex]:\n",
    "                    windowCount += 0.5\n",
    "                    stopIndex += 1\n",
    "            \n",
    "            if win == windows-1:\n",
    "                for data in range(length):\n",
    "                    X[win, i*(length) + data] = dataset[int((windowCount-0.5)*winLen) + i, data]\n",
    "        \n",
    "        windowCount += 0.5\n",
    "\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX, trainy = XySplit(trainData, trainWindows, 3, trainStop, trainAction)\n",
    "newX, newy = XySplit(newData, newWindows, 3, newStop, newAction)\n",
    "testX, testy = XySplit(testData, testWindows, 3, testStop, testAction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.concatenate((trainX, newX))\n",
    "y = np.concatenate((trainy, newy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [],
   "source": [
    "del trainX, trainy, newX, newy, trainData, trainWindows, newData, newWindows, testData, testWindows, trainStop, trainAction, newStop, newAction, testStop, testAction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique, counts = np.unique(y, return_counts=True)\n",
    "class_weight = {0:(1/counts[0])*len(y)/2, 1:(1/counts[1])*len(y)/2, 2:(1/counts[2])*len(y)/2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 339,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skf = StratifiedKFold(n_splits=10, shuffle = True, random_state=42)\n",
    "skf.get_n_splits(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_3Darray(array):\n",
    "    \"\"\"_summary_\n",
    "\n",
    "    Args:\n",
    "        array (_type_): _description_\n",
    "\n",
    "    Returns:\n",
    "        _type_: _description_\n",
    "    \"\"\"\n",
    "    arr_3d = np.empty((len(array), winLen, 3))\n",
    "    arr_3d = np.reshape(array, (len(array), winLen, 3))\n",
    "    return arr_3d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = []\n",
    "scores_in_fold = []\n",
    "losses = []\n",
    "scores_outside_fold = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Fold #0\n",
      "Loss: 0.225% Accuracy: 91.766% \n",
      "precision: 90.578% recall: 84.804% F1 score: 87.596%\n",
      "==> Fold #1\n",
      "Loss: 0.220% Accuracy: 92.353% \n",
      "precision: 90.895% recall: 86.446% F1 score: 88.614%\n",
      "==> Fold #2\n",
      "Loss: 0.223% Accuracy: 91.937% \n",
      "precision: 91.132% recall: 85.386% F1 score: 88.165%\n",
      "==> Fold #3\n",
      "Loss: 0.244% Accuracy: 91.082% \n",
      "precision: 90.261% recall: 83.700% F1 score: 86.857%\n",
      "==> Fold #4\n",
      "Loss: 0.213% Accuracy: 91.693% \n",
      "precision: 90.816% recall: 85.026% F1 score: 87.825%\n",
      "==> Fold #5\n",
      "Loss: 0.236% Accuracy: 91.080% \n",
      "precision: 90.420% recall: 83.419% F1 score: 86.778%\n",
      "==> Fold #6\n",
      "Loss: 0.214% Accuracy: 92.449% \n",
      "precision: 91.601% recall: 86.657% F1 score: 89.060%\n",
      "==> Fold #7\n",
      "Loss: 0.224% Accuracy: 91.984% \n",
      "precision: 92.076% recall: 85.191% F1 score: 88.500%\n",
      "==> Fold #8\n",
      "Loss: 0.225% Accuracy: 91.153% \n",
      "precision: 91.125% recall: 82.973% F1 score: 86.858%\n",
      "==> Fold #9\n",
      "Loss: 0.220% Accuracy: 92.033% \n",
      "precision: 90.103% recall: 86.343% F1 score: 88.183%\n"
     ]
    }
   ],
   "source": [
    "for i, (train_index, val_index) in enumerate(skf.split(X, y)):\n",
    "    print(\"==> Fold #%d\" % i)\n",
    "    X_train, X_val = X[train_index], X[val_index]\n",
    "    y_train, y_val = y[train_index], y[val_index]\n",
    "\n",
    "    X_train = to_3Darray(X_train)\n",
    "    y_train = to_categorical(y_train)\n",
    "\n",
    "    X_val = to_3Darray(X_val)\n",
    "    y_val = to_categorical(y_val)\n",
    "\n",
    "\n",
    "    \n",
    "    verbose, epochs, batch_size = 0, 50, 64\n",
    "    n_timesteps, n_features, n_outputs = X_train.shape[1], X_train.shape[2], y_train.shape[1]\n",
    "\n",
    "    n_steps, n_length = 2, 32\n",
    "    X_train = X_train.reshape((X_train.shape[0], n_steps, n_length, n_features))\n",
    "    X_val = X_val.reshape((X_val.shape[0], n_steps, n_length, n_features))\n",
    "\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(TimeDistributed(Conv1D(filters=64, kernel_size=3, activation='relu'),\n",
    "                            input_shape=(None, n_length, n_features)))\n",
    "                            \n",
    "    model.add(TimeDistributed(\n",
    "        Conv1D(filters=64, kernel_size=3, activation='relu')))\n",
    "    model.add(TimeDistributed(Dropout(0.5)))\n",
    "    model.add(TimeDistributed(MaxPooling1D(pool_size=2)))\n",
    "    model.add(TimeDistributed(Flatten()))\n",
    "    model.add(LSTM(100))\n",
    "    model.add(Dropout(0.5))\n",
    "    hunderdOutput = Dense(100, activation='relu')\n",
    "    model.add(hunderdOutput)  # feature\n",
    "    # 試著輸出長度為100的向量(feature) 並絳維 看他的分布有無分開\n",
    "    model.add(Dense(3, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy',optimizer='adam', metrics=['accuracy']) #可能可以調weighting\n",
    "    \n",
    "    model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, verbose=verbose, class_weight = class_weight)\n",
    "    \n",
    "    loss, accuracy = model.evaluate(X_val, y_val, batch_size=batch_size, verbose=0)\n",
    "    \n",
    "\n",
    "    y_pred = (model.predict(X_val) > 0.5).astype(\"int32\")\n",
    "    y_val = np.argmax(y_val, axis=1)\n",
    "    y_pred = np.argmax(y_pred, axis=1)\n",
    "\n",
    "    confus = confusion_matrix(y_val, y_pred, labels=None, sample_weight=None)\n",
    "    tp = confus[1][1]\n",
    "    tn = confus[0][0] + confus[0][2] + confus[2][0] + confus[2][2]\n",
    "    fp = confus[1][0] + confus[1][2]\n",
    "    fn = confus[0][1] + confus[2][1]\n",
    "\n",
    "    precision = (tp/(tp + fp))*100\n",
    "    recall =  (tp / (tp + fn))*100   #sensitivity\n",
    "    F1_score = ((2*tp) / (2*tp + fp + fn))*100\n",
    "\n",
    "    score = accuracy\n",
    "    losses.append(loss)\n",
    "    \n",
    "    score = score * 100.0\n",
    "    scores_in_fold.append(score)\n",
    "    \n",
    "    print('Loss: %.3f%% Accuracy: %.3f%% ' % (loss, score))\n",
    "    print('precision: %.3f%% recall: %.3f%% F1 score: %.3f%%' % (precision, recall, F1_score))\n",
    "    # print(confus)\n",
    "\n",
    "\n",
    "    \n",
    "    '''score = evaluate_model(X_train, y_train, X_val, y_val)\n",
    "    score = score * 100.0\n",
    "    print(score)\n",
    "    scores.append(score)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 91.753% (+/-0.477)\n",
      "Loss: 0.224% (+/-0.009)\n"
     ]
    }
   ],
   "source": [
    "m, s = np.mean(scores_in_fold), np.std(scores_in_fold)\n",
    "print('Accuracy: %.3f%% (+/-%.3f)' % (m, s))\n",
    "m, s = np.mean(losses), np.std(losses)\n",
    "print('Loss: %.3f%% (+/-%.3f)' % (m, s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [],
   "source": [
    "testX = to_3Darray(testX)\n",
    "testX = testX.reshape((testX.shape[0], n_steps, n_length, n_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [],
   "source": [
    "testy = to_categorical(testy)\n",
    "y_pred = (model.predict(testX) > 0.5).astype(\"int32\")\n",
    "testy = np.argmax(testy, axis=1)\n",
    "y_pred = np.argmax(y_pred, axis=1)\n",
    "\n",
    "confus = confusion_matrix(testy, y_pred, labels=None, sample_weight=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2985   23    1]\n",
      " [ 131   15    0]\n",
      " [  28    6    0]]\n"
     ]
    }
   ],
   "source": [
    "print(confus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp = confus[1][1]\n",
    "tn = confus[0][0] + confus[0][2] + confus[2][0] + confus[2][2]\n",
    "fp = confus[0][1] + confus[2][1]\n",
    "fn = confus[1][0] + confus[1][2]\n",
    "\n",
    "precision = (tp/(tp + fp))*100\n",
    "sensitivity = (tp / (tp + fn))*100  # sensitivity\n",
    "specificity = (tn/(tn + fp))*100\n",
    "F1_score = ((2*tp) / (2*tp + fp + fn))*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = accuracy_score(testy, y_pred)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 94.073% \n",
      "FOG: specificity: 99.047% sensitivity: 10.274% F1 score: 15.789%\n"
     ]
    }
   ],
   "source": [
    "accuracy = accuracy\n",
    "print('Accuracy: %.3f%% ' % (accuracy))\n",
    "print('FOG: specificity: %.3f%% sensitivity: %.3f%% F1 score: %.3f%%' % (specificity, sensitivity, F1_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PreFOG: specificity: 99.968% sensitivity: 0.000% F1 score: 0.000%\n"
     ]
    }
   ],
   "source": [
    "tp = confus[2][2]\n",
    "tn = confus[0][0] + confus[0][1] + confus[1][0] + confus[1][1]\n",
    "fp = confus[0][2] + confus[1][2]\n",
    "fn = confus[2][0] + confus[2][1]\n",
    "\n",
    "precision = (tp/(tp + fp))*100\n",
    "sensitivity = (tp / (tp + fn))*100  # sensitivity\n",
    "specificity = (tn/(tn + fp))*100\n",
    "F1_score = ((2*tp) / (2*tp + fp + fn))*100\n",
    "print('PreFOG: specificity: %.3f%% sensitivity: %.3f%% F1 score: %.3f%%' % (specificity, sensitivity, F1_score))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.11 ('HighHeelWhatever')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8986fb416174cc2474d1ce69838b7b56508ac61c47a66825c7584556038319d3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
