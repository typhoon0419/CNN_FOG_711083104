{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "import os\n",
    "import datetime, time\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, BatchNormalization\n",
    "from tensorflow.keras.layers import LSTM, TimeDistributed, ConvLSTM2D\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "from sklearn.metrics import confusion_matrix,accuracy_score\n",
    "from keras import backend as k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_path = os.getcwd() + \"/dataset/total_train.csv\"\n",
    "test_data_path = os.getcwd() + \"/dataset/total_test.csv\"\n",
    "new_data_path = os.getcwd() + \"/dataset/merge.csv\"\n",
    "\n",
    "fromPath = os.getcwd() +\"/dataset/windows\"\n",
    "savePath = os.getcwd() +\"/dataset\"\n",
    "\n",
    "paraPath = os.getcwd() +\"/parameters/parameters.txt\"\n",
    "\n",
    "testSubs = [\"S01\", \"S02\", \"S03\", \"S05\", \"S06\", \"S07\", \"S08\", \"S09\"]\n",
    "winLen = int(1*64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_window(window_length,dataframe):\n",
    "\n",
    "  indices = list(dataframe.index)\n",
    "  time = []\n",
    "  time_count = 0\n",
    "  for j in indices:\n",
    "    time.append(dataframe.loc[j, 'time'])\n",
    "\n",
    "\n",
    "  # indices記錄所有Action==act的index\n",
    "  groups = [] # 用來暫存一組(同action)資料的, 型態是[][]\n",
    "  temp = [] # 用來暫存一行資料的\n",
    "  group_count = 0\n",
    "  for i in range(len(indices)):\n",
    "    if i == len(indices)-1:\n",
    "      temp.append(indices[i])\n",
    "      groups.append(temp)\n",
    "      temp = []\n",
    "      break # 如果i已經來到最後的話就break\n",
    "    temp.append(indices[i])\n",
    "    #time_count = time_count + 1\n",
    "    if time[i+1]-16 > time[i]: #如果下個index不是連續的話, 就將前面這些存成第一組\n",
    "      group_count+=1\n",
    "      #time_count = time_count + 1\n",
    "      groups.append(temp)\n",
    "      temp = []\n",
    "\n",
    "  #print(groups)\n",
    "\n",
    "  fs = 64\n",
    "  # window_length = 1\n",
    "  # window_length = int(window_length*fs)\n",
    "\n",
    "  final_dataframe = pd.DataFrame()\n",
    "  sumOfAct=0\n",
    "\n",
    "  for i in groups: # group[][]的每一行i\n",
    "    required = math.floor(len(i)/int(window_length/2))\n",
    "\n",
    "    \n",
    "    sumOfAct= sumOfAct+required\n",
    "\n",
    "    req_index = i[0:(required*int(window_length/2))]\n",
    "\n",
    "    #print(req_index)\n",
    "    # concat([要結合的data集合], axis=0是方向為直的)\n",
    "    final_dataframe = pd.concat([final_dataframe,dataframe.iloc[req_index,:]],axis = 0)\n",
    "  \n",
    "  \n",
    "\n",
    "  return final_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveDFtocsv(fromPath: str, toPath: str, compare:str, useOriginal=True, saveName = \"\"):\n",
    "    \"\"\"_summary_\n",
    "\n",
    "    Args:\n",
    "        fromPath (str): _description_\n",
    "        toPath (str): _description_\n",
    "        compare (str): _description_\n",
    "    \"\"\"\n",
    "    \n",
    "    for file in os.listdir(fromPath):\n",
    "        if compare not in file:\n",
    "            continue\n",
    "\n",
    "        filePath = fromPath + '/' + file\n",
    "        dataset = create_window(winLen, pd.read_csv(filePath))\n",
    "\n",
    "        if useOriginal:\n",
    "            savePath = toPath + \"/\" + file\n",
    "        else:\n",
    "            savePath = toPath + \"/\" + saveName + \".csv\"\n",
    "        dataset.to_csv(savePath, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeDataset(fromPath:str, savePath:str, testSub:str):\n",
    "    train_patients_dataset = pd.DataFrame()\n",
    "    test_patients_dataset = pd.DataFrame()\n",
    "\n",
    "    for file in os.listdir(fromPath):\n",
    "        if \"win_\" not in file:\n",
    "            continue\n",
    "        \n",
    "        filePath = fromPath + '/' + file\n",
    "        dataset = pd.read_csv(filePath)\n",
    "\n",
    "        if testSub in file:\n",
    "            test_patients_dataset = test_patients_dataset.append(dataset)\n",
    "        else:    \n",
    "            train_patients_dataset = train_patients_dataset.append(dataset)\n",
    "\n",
    "    to_path = savePath + \"/not_windowed_train.csv\"\n",
    "    train_patients_dataset.to_csv(to_path, index=False)\n",
    "\n",
    "    to_path = savePath + \"/not_windowed_test.csv\"\n",
    "    test_patients_dataset.to_csv(to_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTimeAndDF(path:str):\n",
    "    \"\"\"get dataset and dataset's time list\n",
    "       some dataset's time is not continued, so get the time is for split windows\n",
    "\n",
    "    Args:\n",
    "        path (str): path of the dataset(for .csv)\n",
    "\n",
    "    Returns:\n",
    "        time (list): the list of dataset time\n",
    "        df (np.array): dataset, columns include [\"A_F\", \"A_V\", \"A_L\", \"Action\"]\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(path)\n",
    "    time = list(df['time'])\n",
    "    action = list(df['Action'])\n",
    "    df = df[[\"A_F\", \"A_V\", \"A_L\"]].values\n",
    "\n",
    "    return time, df, action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTotalWindows(indices:list, windowSize:int , gap: float):\n",
    "    \"\"\"because\n",
    "    check every part of time\n",
    "\n",
    "    Args:\n",
    "        indices (list): _description_\n",
    "        windowSize (int): _description_\n",
    "        gap (float): _description_\n",
    "\n",
    "    Returns:\n",
    "        _type_: _description_\n",
    "    \"\"\"\n",
    "    groups = []\n",
    "    group_count = 0\n",
    "    temp = []\n",
    "    lenOfGroup = []\n",
    "    length_count = 0\n",
    "    for i in range(len(indices)):\n",
    "        if i == (len(indices) - 1):\n",
    "            temp.append(indices[i])\n",
    "            length_count = length_count + 1\n",
    "\n",
    "            groups.append(temp)\n",
    "            lenOfGroup.append(length_count)\n",
    "            length_count = 0\n",
    "            temp = []\n",
    "            break\n",
    "        temp.append(indices[i])\n",
    "        length_count = length_count + 1\n",
    "        if (indices[i+1] - gap > indices[i]):\n",
    "            group_count = group_count + 1\n",
    "\n",
    "            lenOfGroup.append(length_count)\n",
    "            length_count = 0\n",
    "\n",
    "            groups.append(temp)\n",
    "            temp = []\n",
    "\n",
    "    countOfUndivisible = 0\n",
    "    totalWindows = 0\n",
    "\n",
    "    stop_Indexs = []\n",
    "    stop_Index = -windowSize\n",
    "\n",
    "\n",
    "    for lengths in lenOfGroup:\n",
    "        stop_Index = stop_Index + lengths\n",
    "        stop_Indexs.append(stop_Index)\n",
    "        totalWindows = totalWindows + int(float(lengths/windowSize)*2 -1)\n",
    "        \n",
    "        if lengths % (windowSize/2) != 0:\n",
    "            countOfUndivisible = countOfUndivisible + 1\n",
    "            print(lengths)\n",
    "\n",
    "    return totalWindows, stop_Indexs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def XySplit(dataset:np.array, windows:int, length:int, stop:list, action:list):\n",
    "    \"\"\"split dataset into X and y, \n",
    "    X is 2D array, size of X is [windows, 64*3]\n",
    "    y is 1D array, size of y is [windows, 1]\n",
    "\n",
    "    Args:\n",
    "        dataset (np.array): dataset\n",
    "        windows (int): total windows that get from getTotalWindows()\n",
    "        length (int): length of a piece of data, here is 3\n",
    "        stop (list): stopList that get from getTotalWindows()\n",
    "        action (list): action list\n",
    "\n",
    "    Returns:\n",
    "        X(np.array): X is 2D array, size of X is [windows, 64*3]\n",
    "        y(np.array): y is 1D array, size of y is [windows, 1]\n",
    "    \"\"\"\n",
    "    X = np.empty((windows, winLen*(length)))\n",
    "    y = np.empty((windows, 1))\n",
    "\n",
    "    stopIndex = 0\n",
    "    windowCount = 0\n",
    "    for win in range(windows):\n",
    "        for i in range(winLen):\n",
    "            if i == 0:\n",
    "                y[win] = action[int(windowCount*winLen + winLen/2)]\n",
    "\n",
    "            if int(windowCount*winLen)<len(dataset)-winLen-1:\n",
    "                for data in range(length):\n",
    "                    X[win, i*(length)+data] = dataset[int(windowCount*winLen) + i, data]\n",
    "\n",
    "            if stopIndex < len(stop):\n",
    "                if int(windowCount*winLen) == stop[stopIndex]:\n",
    "                    windowCount += 0.5\n",
    "                    stopIndex += 1\n",
    "            \n",
    "            if win == windows-1:\n",
    "                for data in range(length):\n",
    "                    X[win, i*(length) + data] = dataset[int((windowCount-0.5)*winLen) + i, data]\n",
    "        \n",
    "        windowCount += 0.5\n",
    "\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_3Darray(array):\n",
    "    \"\"\"_summary_\n",
    "\n",
    "    Args:\n",
    "        array (_type_): _description_\n",
    "\n",
    "    Returns:\n",
    "        _type_: _description_\n",
    "    \"\"\"\n",
    "    arr_3d = np.empty((len(array), winLen, 3))\n",
    "    arr_3d = np.reshape(array, (len(array), winLen, 3))\n",
    "    return arr_3d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setModel(n_length, n_features, layers, filterList):\n",
    "    model = Sequential()\n",
    "    model.add(TimeDistributed(Conv1D(filters=filterList[0], kernel_size=3, activation='relu'), input_shape=(None, n_length, n_features)))\n",
    "    model.add(TimeDistributed(BatchNormalization()))\n",
    "\n",
    "    for i in range(layers-1):\n",
    "        model.add(TimeDistributed(Conv1D(filters=filterList[1+i], kernel_size=3, activation='relu')))\n",
    "        if i%2 == 0:\n",
    "            model.add(TimeDistributed(MaxPooling1D(pool_size=2)))\n",
    "        model.add(TimeDistributed(BatchNormalization()))\n",
    "\n",
    "    model.add(TimeDistributed(Dropout(0.5)))\n",
    "    model.add(TimeDistributed(MaxPooling1D(pool_size=2)))\n",
    "    model.add(TimeDistributed(Flatten()))\n",
    "\n",
    "    model.add(LSTM(100))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(100, activation='relu'))\n",
    "    model.add(Dense(3, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    # print (model.summary())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainingModel(skf, X, y, parameters, class_weight, layers, filterList):\n",
    "    losses = []\n",
    "    scores_in_fold = []\n",
    "    for i, (train_index, val_index) in enumerate(skf.split(X, y)):\n",
    "        print(\"==> Fold #%d\" % i)\n",
    "        X_train, X_val = X[train_index], X[val_index]\n",
    "        y_train, y_val = y[train_index], y[val_index]\n",
    "\n",
    "        X_train = to_3Darray(X_train)\n",
    "        y_train = to_categorical(y_train)\n",
    "\n",
    "        X_val = to_3Darray(X_val)\n",
    "        y_val = to_categorical(y_val)\n",
    "\n",
    "        verbose, epochs, batch_size = 0, parameters[0], parameters[1]\n",
    "        n_timesteps, n_features, n_outputs = X_train.shape[1], X_train.shape[2], y_train.shape[1]\n",
    "\n",
    "        n_steps, n_length = parameters[2], parameters[3]\n",
    "        X_train = X_train.reshape((X_train.shape[0], n_steps, n_length, n_features))\n",
    "        X_val = X_val.reshape((X_val.shape[0], n_steps, n_length, n_features))\n",
    "\n",
    "\n",
    "        model = setModel(n_length, n_features, layers, filterList)\n",
    "        model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, verbose=verbose, class_weight = class_weight)\n",
    "        \n",
    "        loss, accuracy = model.evaluate(X_val, y_val, batch_size=batch_size, verbose=0)\n",
    "        \n",
    "        y_pred = (model.predict(X_val) > 0.5).astype(\"int32\")\n",
    "        y_val = np.argmax(y_val, axis=1)\n",
    "        y_pred = np.argmax(y_pred, axis=1)\n",
    "\n",
    "        confus = confusion_matrix(y_val, y_pred, labels=None, sample_weight=None)\n",
    "        tp = confus[1][1]\n",
    "        tn = confus[0][0] + confus[0][2] + confus[2][0] + confus[2][2]\n",
    "        fp = confus[1][0] + confus[1][2]\n",
    "        fn = confus[0][1] + confus[2][1]\n",
    "\n",
    "        precision = (tp/(tp + fp))*100\n",
    "        recall =  (tp / (tp + fn))*100   #sensitivity\n",
    "        F1_score = ((2*tp) / (2*tp + fp + fn))*100\n",
    "\n",
    "        score = accuracy\n",
    "        losses.append(loss)\n",
    "        \n",
    "        score = score * 100.0\n",
    "        scores_in_fold.append(score)\n",
    "        \n",
    "        print('Loss: %.3f%% Accuracy: %.3f%% ' % (loss, score))\n",
    "        print('precision: %.3f%% recall: %.3f%% F1 score: %.3f%%' % (precision, recall, F1_score))\n",
    "\n",
    "    return model, losses, scores_in_fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictData(model, testX, testy, parameters):\n",
    "\n",
    "    testX = to_3Darray(testX)\n",
    "    testX = testX.reshape((testX.shape[0], parameters[2], parameters[3], testX.shape[2]))\n",
    "\n",
    "    testy = to_categorical(testy)\n",
    "    y_pred = (model.predict(testX) > 0.5).astype(\"int32\")\n",
    "    testy = np.argmax(testy, axis=1)\n",
    "    y_pred = np.argmax(y_pred, axis=1)\n",
    "\n",
    "    confus = confusion_matrix(testy, y_pred, labels=None, sample_weight=None)\n",
    "\n",
    "    return confus, y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def performance(state:int, confus:list):\n",
    "    performList = []\n",
    "    if state == 1:\n",
    "        tp = confus[1][1]\n",
    "        tn = confus[0][0] + confus[0][2] + confus[2][0] + confus[2][2]\n",
    "        fp = confus[0][1] + confus[2][1]\n",
    "        fn = confus[1][0] + confus[1][2]\n",
    "    elif state == 2:\n",
    "        tp = confus[2][2]\n",
    "        tn = confus[0][0] + confus[0][1] + confus[1][0] + confus[1][1]\n",
    "        fp = confus[0][2] + confus[1][2]\n",
    "        fn = confus[2][0] + confus[2][1]\n",
    "\n",
    "    precision = (tp/(tp + fp))*100\n",
    "    sensitivity = (tp / (tp + fn))*100  # sensitivity\n",
    "    specificity = (tn/(tn + fp))*100\n",
    "    F1_score = ((2*tp) / (2*tp + fp + fn))*100\n",
    "\n",
    "    performList.append(precision)\n",
    "    performList.append(specificity)\n",
    "    performList.append(sensitivity)\n",
    "    performList.append(F1_score)\n",
    "\n",
    "    return performList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getParaMeters(line):\n",
    "    \"\"\"_summary_\n",
    "\n",
    "    Args:\n",
    "        line (_type_): _description_\n",
    "\n",
    "    Returns:\n",
    "        _type_: _description_\n",
    "    \"\"\"\n",
    "    parameters = []\n",
    "    filterList = []\n",
    "    line = line.split('\\n')[0]\n",
    "    for i in range(4):\n",
    "        parameters.append(int(line.split(' ')[i])) # epoch, batch size, n_steps, n_length\n",
    "    layers = int(line.split(' ')[4])\n",
    "    for i in range(layers):\n",
    "        filterList.append(int(line.split(' ')[5+i]))\n",
    "    return parameters, layers, filterList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "paras = open(paraPath)\n",
    "paraList = paras.readlines()\n",
    "paras.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getXy(testSub):\n",
    "    makeDataset(fromPath, savePath, testSub)\n",
    "\n",
    "    saveDFtocsv(savePath, savePath, \"not_windowed_train\", False, \"total_train\")\n",
    "    saveDFtocsv(savePath, savePath, \"not_windowed_test\", False, \"total_test\")\n",
    "\n",
    "    trainTime, trainData, trainAction = getTimeAndDF(train_data_path)\n",
    "    testTime, testData, testAction = getTimeAndDF(test_data_path)\n",
    "    newTime, newData, newAction = getTimeAndDF(new_data_path)\n",
    "\n",
    "    trainData = (trainData-trainData.mean())/(trainData.std())\n",
    "    testData = (testData-testData.mean())/(testData.std())\n",
    "    newData = (newData-newData.mean())/(newData.std())\n",
    "\n",
    "    trainWindows, trainStop = getTotalWindows(trainTime, winLen, 20)\n",
    "    newWindows, newStop = getTotalWindows(newTime, winLen, 0.02)\n",
    "    testWindows, testStop = getTotalWindows(testTime, winLen, 20)\n",
    "\n",
    "    trainX, trainy = XySplit(trainData, trainWindows, 3, trainStop, trainAction)\n",
    "    newX, newy = XySplit(newData, newWindows, 3, newStop, newAction)\n",
    "    testX, testy = XySplit(testData, testWindows, 3, testStop, testAction)\n",
    "\n",
    "    X = np.concatenate((trainX, newX))\n",
    "    y = np.concatenate((trainy, newy))\n",
    "\n",
    "    return X, y, testX, testy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S01\n",
      "==> Fold #0\n",
      "Loss: 0.365% Accuracy: 87.133% \n",
      "precision: 70.534% recall: 93.924% F1 score: 80.565%\n",
      "==> Fold #1\n",
      "Loss: 0.335% Accuracy: 88.711% \n",
      "precision: 74.014% recall: 95.700% F1 score: 83.471%\n",
      "==> Fold #2\n",
      "Loss: 0.334% Accuracy: 87.922% \n",
      "precision: 74.710% recall: 93.695% F1 score: 83.133%\n",
      "==> Fold #3\n",
      "Loss: 0.306% Accuracy: 89.919% \n",
      "precision: 78.577% recall: 95.940% F1 score: 86.395%\n",
      "==> Fold #4\n",
      "Loss: 0.347% Accuracy: 84.964% \n",
      "precision: 59.242% recall: 95.750% F1 score: 73.196%\n",
      "==> Fold #5\n",
      "Loss: 0.318% Accuracy: 88.462% \n",
      "precision: 73.550% recall: 93.972% F1 score: 82.516%\n",
      "==> Fold #6\n",
      "Loss: 0.379% Accuracy: 86.366% \n",
      "precision: 65.429% recall: 96.686% F1 score: 78.044%\n",
      "==> Fold #7\n",
      "Loss: 0.356% Accuracy: 86.538% \n",
      "precision: 66.280% recall: 94.696% F1 score: 77.980%\n",
      "==> Fold #8\n",
      "Loss: 0.380% Accuracy: 84.048% \n",
      "precision: 57.541% recall: 94.898% F1 score: 71.642%\n",
      "==> Fold #9\n",
      "Loss: 0.370% Accuracy: 84.443% \n",
      "precision: 61.021% recall: 94.718% F1 score: 74.224%\n",
      "S02\n",
      "==> Fold #0\n",
      "Loss: 0.344% Accuracy: 88.806% \n",
      "precision: 72.514% recall: 96.559% F1 score: 82.826%\n",
      "==> Fold #1\n",
      "Loss: 0.394% Accuracy: 83.702% \n",
      "precision: 63.978% recall: 91.183% F1 score: 75.196%\n",
      "==> Fold #2\n",
      "Loss: 0.401% Accuracy: 80.645% \n",
      "precision: 47.496% recall: 97.120% F1 score: 63.794%\n",
      "==> Fold #3\n",
      "Loss: 0.357% Accuracy: 84.377% \n",
      "precision: 59.781% recall: 94.321% F1 score: 73.180%\n",
      "==> Fold #4\n",
      "Loss: 0.394% Accuracy: 81.965% \n",
      "precision: 54.424% recall: 89.793% F1 score: 67.772%\n",
      "==> Fold #5\n",
      "Loss: 0.342% Accuracy: 87.623% \n",
      "precision: 74.080% recall: 90.962% F1 score: 81.657%\n",
      "==> Fold #6\n",
      "Loss: 0.332% Accuracy: 88.707% \n",
      "precision: 75.176% recall: 94.025% F1 score: 83.551%\n",
      "==> Fold #7\n",
      "Loss: 0.303% Accuracy: 89.550% \n",
      "precision: 74.628% recall: 94.450% F1 score: 83.377%\n",
      "==> Fold #8\n",
      "Loss: 0.363% Accuracy: 87.070% \n",
      "precision: 66.719% recall: 93.833% F1 score: 77.986%\n",
      "==> Fold #9\n",
      "Loss: 0.319% Accuracy: 87.575% \n",
      "precision: 70.478% recall: 94.737% F1 score: 80.826%\n",
      "S03\n",
      "==> Fold #0\n",
      "Loss: 0.347% Accuracy: 87.588% \n",
      "precision: 66.879% recall: 95.238% F1 score: 78.578%\n",
      "==> Fold #1\n",
      "Loss: 0.364% Accuracy: 84.662% \n",
      "precision: 60.748% recall: 93.390% F1 score: 73.613%\n",
      "==> Fold #2\n",
      "Loss: 0.351% Accuracy: 87.783% \n",
      "precision: 66.481% recall: 95.320% F1 score: 78.330%\n",
      "==> Fold #3\n",
      "Loss: 0.366% Accuracy: 87.637% \n",
      "precision: 68.631% recall: 96.205% F1 score: 80.112%\n",
      "==> Fold #4\n",
      "Loss: 0.327% Accuracy: 89.149% \n",
      "precision: 74.522% recall: 95.219% F1 score: 83.609%\n",
      "==> Fold #5\n",
      "Loss: 0.328% Accuracy: 87.829% \n",
      "precision: 68.551% recall: 95.560% F1 score: 79.833%\n",
      "==> Fold #6\n",
      "Loss: 0.373% Accuracy: 85.024% \n",
      "precision: 64.729% recall: 93.772% F1 score: 76.590%\n",
      "==> Fold #7\n",
      "Loss: 0.371% Accuracy: 83.098% \n",
      "precision: 54.459% recall: 93.061% F1 score: 68.709%\n",
      "==> Fold #8\n",
      "Loss: 0.328% Accuracy: 88.220% \n",
      "precision: 68.471% recall: 96.197% F1 score: 80.000%\n",
      "==> Fold #9\n",
      "Loss: 0.302% Accuracy: 90.000% \n",
      "precision: 78.901% recall: 91.844% F1 score: 84.882%\n",
      "S05\n",
      "==> Fold #0\n",
      "Loss: 0.379% Accuracy: 85.320% \n",
      "precision: 65.107% recall: 95.657% F1 score: 77.479%\n",
      "==> Fold #1\n",
      "Loss: 0.327% Accuracy: 85.569% \n",
      "precision: 60.345% recall: 95.331% F1 score: 73.906%\n",
      "==> Fold #2\n",
      "Loss: 0.279% Accuracy: 90.197% \n",
      "precision: 74.077% recall: 95.556% F1 score: 83.457%\n",
      "==> Fold #3\n",
      "Loss: 0.312% Accuracy: 89.301% \n",
      "precision: 71.944% recall: 97.013% F1 score: 82.619%\n",
      "==> Fold #4\n",
      "Loss: 0.286% Accuracy: 90.843% \n",
      "precision: 77.851% recall: 95.090% F1 score: 85.611%\n",
      "==> Fold #5\n",
      "Loss: 0.293% Accuracy: 88.753% \n",
      "precision: 69.729% recall: 96.701% F1 score: 81.030%\n",
      "==> Fold #6\n",
      "Loss: 0.300% Accuracy: 87.136% \n",
      "precision: 64.889% recall: 95.416% F1 score: 77.246%\n",
      "==> Fold #7\n",
      "Loss: 0.291% Accuracy: 89.774% \n",
      "precision: 73.749% recall: 95.740% F1 score: 83.318%\n",
      "==> Fold #8\n",
      "Loss: 0.358% Accuracy: 84.097% \n",
      "precision: 53.366% recall: 95.168% F1 score: 68.385%\n",
      "==> Fold #9\n",
      "Loss: 0.331% Accuracy: 86.486% \n",
      "precision: 61.576% recall: 96.899% F1 score: 75.301%\n",
      "S06\n",
      "==> Fold #0\n",
      "Loss: 0.338% Accuracy: 88.344% \n",
      "precision: 75.602% recall: 93.468% F1 score: 83.591%\n",
      "==> Fold #1\n",
      "Loss: 0.329% Accuracy: 88.027% \n",
      "precision: 70.085% recall: 94.056% F1 score: 80.321%\n",
      "==> Fold #2\n",
      "Loss: 0.339% Accuracy: 89.490% \n",
      "precision: 74.592% recall: 94.862% F1 score: 83.515%\n",
      "==> Fold #3\n",
      "Loss: 0.390% Accuracy: 81.639% \n",
      "precision: 50.816% recall: 95.754% F1 score: 66.396%\n",
      "==> Fold #4\n",
      "Loss: 0.408% Accuracy: 80.810% \n",
      "precision: 50.543% recall: 94.898% F1 score: 65.957%\n",
      "==> Fold #5\n",
      "Loss: 0.361% Accuracy: 86.711% \n",
      "precision: 68.711% recall: 95.161% F1 score: 79.802%\n",
      "==> Fold #6\n",
      "Loss: 0.383% Accuracy: 82.707% \n",
      "precision: 54.002% recall: 92.297% F1 score: 68.137%\n",
      "==> Fold #7\n",
      "Loss: 0.341% Accuracy: 86.195% \n",
      "precision: 65.890% recall: 94.537% F1 score: 77.656%\n",
      "==> Fold #8\n",
      "Loss: 0.360% Accuracy: 87.195% \n",
      "precision: 71.795% recall: 95.950% F1 score: 82.133%\n",
      "==> Fold #9\n",
      "Loss: 0.311% Accuracy: 88.878% \n",
      "precision: 77.622% recall: 94.513% F1 score: 85.239%\n",
      "S07\n",
      "==> Fold #0\n",
      "Loss: 0.360% Accuracy: 85.759% \n",
      "precision: 64.996% recall: 93.980% F1 score: 76.846%\n",
      "==> Fold #1\n",
      "Loss: 0.346% Accuracy: 88.505% \n",
      "precision: 76.715% recall: 93.340% F1 score: 84.215%\n",
      "==> Fold #2\n",
      "Loss: 0.410% Accuracy: 80.583% \n",
      "precision: 47.648% recall: 94.931% F1 score: 63.450%\n",
      "==> Fold #3\n",
      "Loss: 0.355% Accuracy: 86.780% \n",
      "precision: 70.031% recall: 93.905% F1 score: 80.229%\n",
      "==> Fold #4\n",
      "Loss: 0.380% Accuracy: 85.318% \n",
      "precision: 62.066% recall: 94.484% F1 score: 74.919%\n",
      "==> Fold #5\n",
      "Loss: 0.361% Accuracy: 87.409% \n",
      "precision: 72.321% recall: 92.051% F1 score: 81.002%\n",
      "==> Fold #6\n",
      "Loss: 0.343% Accuracy: 87.628% \n",
      "precision: 70.470% recall: 95.010% F1 score: 80.921%\n",
      "==> Fold #7\n",
      "Loss: 0.333% Accuracy: 87.944% \n",
      "precision: 70.933% recall: 96.134% F1 score: 81.633%\n",
      "==> Fold #8\n",
      "Loss: 0.398% Accuracy: 80.579% \n",
      "precision: 52.506% recall: 87.420% F1 score: 65.607%\n",
      "==> Fold #9\n",
      "Loss: 0.363% Accuracy: 83.787% \n",
      "precision: 58.520% recall: 93.015% F1 score: 71.841%\n",
      "S08\n",
      "==> Fold #0\n",
      "Loss: 0.316% Accuracy: 88.956% \n",
      "precision: 75.491% recall: 94.031% F1 score: 83.747%\n",
      "==> Fold #1\n",
      "Loss: 0.297% Accuracy: 89.914% \n",
      "precision: 75.805% recall: 93.057% F1 score: 83.550%\n",
      "==> Fold #2\n",
      "Loss: 0.320% Accuracy: 89.096% \n",
      "precision: 73.291% recall: 93.675% F1 score: 82.239%\n",
      "==> Fold #3\n",
      "Loss: 0.313% Accuracy: 88.139% \n",
      "precision: 71.092% recall: 94.075% F1 score: 80.984%\n",
      "==> Fold #4\n",
      "Loss: 0.330% Accuracy: 89.400% \n",
      "precision: 75.412% recall: 93.567% F1 score: 83.515%\n",
      "==> Fold #5\n",
      "Loss: 0.342% Accuracy: 85.921% \n",
      "precision: 66.484% recall: 90.011% F1 score: 76.479%\n",
      "==> Fold #6\n",
      "Loss: 0.333% Accuracy: 89.073% \n",
      "precision: 76.374% recall: 94.466% F1 score: 84.462%\n",
      "==> Fold #7\n",
      "Loss: 0.326% Accuracy: 87.015% \n",
      "precision: 67.871% recall: 93.913% F1 score: 78.796%\n",
      "==> Fold #8\n",
      "Loss: 0.362% Accuracy: 85.404% \n",
      "precision: 63.629% recall: 92.360% F1 score: 75.349%\n",
      "==> Fold #9\n",
      "Loss: 0.335% Accuracy: 86.992% \n",
      "precision: 67.400% recall: 89.655% F1 score: 76.951%\n",
      "S09\n",
      "==> Fold #0\n",
      "Loss: 0.335% Accuracy: 88.652% \n",
      "precision: 71.088% recall: 94.409% F1 score: 81.106%\n",
      "==> Fold #1\n",
      "Loss: 0.353% Accuracy: 86.843% \n",
      "precision: 68.705% recall: 94.951% F1 score: 79.724%\n",
      "==> Fold #2\n",
      "Loss: 0.318% Accuracy: 89.729% \n",
      "precision: 77.601% recall: 94.123% F1 score: 85.067%\n",
      "==> Fold #3\n",
      "Loss: 0.385% Accuracy: 82.832% \n",
      "precision: 54.524% recall: 93.852% F1 score: 68.976%\n",
      "==> Fold #4\n",
      "Loss: 0.311% Accuracy: 89.264% \n",
      "precision: 76.429% recall: 95.536% F1 score: 84.921%\n",
      "==> Fold #5\n",
      "Loss: 0.354% Accuracy: 86.134% \n",
      "precision: 66.429% recall: 92.794% F1 score: 77.428%\n",
      "==> Fold #6\n",
      "Loss: 0.291% Accuracy: 89.826% \n",
      "precision: 75.397% recall: 94.059% F1 score: 83.700%\n",
      "==> Fold #7\n",
      "Loss: 0.349% Accuracy: 88.038% \n",
      "precision: 71.644% recall: 93.763% F1 score: 81.225%\n",
      "==> Fold #8\n",
      "Loss: 0.337% Accuracy: 87.451% \n",
      "precision: 69.500% recall: 93.383% F1 score: 79.690%\n",
      "==> Fold #9\n",
      "Loss: 0.386% Accuracy: 80.186% \n",
      "precision: 51.628% recall: 87.366% F1 score: 64.903%\n",
      "S01\n",
      "==> Fold #0\n",
      "Loss: 0.382% Accuracy: 83.387% \n",
      "precision: 56.613% recall: 94.819% F1 score: 70.896%\n",
      "==> Fold #1\n",
      "Loss: 0.330% Accuracy: 87.799% \n",
      "precision: 71.771% recall: 93.737% F1 score: 81.297%\n",
      "==> Fold #2\n",
      "Loss: 0.330% Accuracy: 87.183% \n",
      "precision: 70.379% recall: 93.047% F1 score: 80.141%\n",
      "==> Fold #3\n",
      "Loss: 0.365% Accuracy: 85.654% \n",
      "precision: 64.346% recall: 94.977% F1 score: 76.717%\n",
      "==> Fold #4\n",
      "Loss: 0.312% Accuracy: 86.665% \n",
      "precision: 66.203% recall: 95.749% F1 score: 78.281%\n",
      "==> Fold #5\n",
      "Loss: 0.334% Accuracy: 85.355% \n",
      "precision: 61.176% recall: 95.301% F1 score: 74.517%\n",
      "==> Fold #6\n",
      "Loss: 0.390% Accuracy: 82.002% \n",
      "precision: 54.524% recall: 91.085% F1 score: 68.215%\n",
      "==> Fold #7\n",
      "Loss: 0.355% Accuracy: 86.908% \n",
      "precision: 64.965% recall: 95.130% F1 score: 77.206%\n",
      "==> Fold #8\n",
      "Loss: 0.378% Accuracy: 82.396% \n",
      "precision: 53.828% recall: 93.674% F1 score: 68.369%\n",
      "==> Fold #9\n",
      "Loss: 0.399% Accuracy: 80.301% \n",
      "precision: 45.476% recall: 93.038% F1 score: 61.091%\n",
      "S02\n",
      "==> Fold #0\n",
      "Loss: 0.320% Accuracy: 88.300% \n",
      "precision: 72.984% recall: 94.332% F1 score: 82.296%\n",
      "==> Fold #1\n",
      "Loss: 0.353% Accuracy: 86.375% \n",
      "precision: 70.948% recall: 92.073% F1 score: 80.142%\n",
      "==> Fold #2\n",
      "Loss: 0.303% Accuracy: 88.806% \n",
      "precision: 75.665% recall: 91.572% F1 score: 82.862%\n",
      "==> Fold #3\n",
      "Loss: 0.304% Accuracy: 89.841% \n",
      "precision: 78.013% recall: 92.486% F1 score: 84.635%\n",
      "==> Fold #4\n",
      "Loss: 0.325% Accuracy: 87.551% \n",
      "precision: 72.357% recall: 91.576% F1 score: 80.840%\n",
      "==> Fold #5\n",
      "Loss: 0.376% Accuracy: 83.145% \n",
      "precision: 56.226% recall: 90.088% F1 score: 69.238%\n",
      "==> Fold #6\n",
      "Loss: 0.360% Accuracy: 87.359% \n",
      "precision: 66.954% recall: 92.134% F1 score: 77.551%\n",
      "==> Fold #7\n",
      "Loss: 0.323% Accuracy: 87.359% \n",
      "precision: 70.791% recall: 91.591% F1 score: 79.859%\n",
      "==> Fold #8\n",
      "Loss: 0.323% Accuracy: 88.996% \n",
      "precision: 74.080% recall: 93.110% F1 score: 82.512%\n",
      "==> Fold #9\n",
      "Loss: 0.359% Accuracy: 82.639% \n",
      "precision: 53.485% recall: 91.555% F1 score: 67.523%\n",
      "S03\n",
      "==> Fold #0\n",
      "Loss: 0.302% Accuracy: 88.442% \n",
      "precision: 70.780% recall: 94.274% F1 score: 80.855%\n",
      "==> Fold #1\n",
      "Loss: 0.340% Accuracy: 88.052% \n",
      "precision: 70.860% recall: 92.999% F1 score: 80.434%\n",
      "==> Fold #2\n",
      "Loss: 0.355% Accuracy: 83.882% \n",
      "precision: 57.086% recall: 90.645% F1 score: 70.054%\n",
      "==> Fold #3\n",
      "Loss: 0.281% Accuracy: 90.588% \n",
      "precision: 79.857% recall: 93.826% F1 score: 86.280%\n",
      "==> Fold #4\n",
      "Loss: 0.405% Accuracy: 82.736% \n",
      "precision: 57.404% recall: 90.351% F1 score: 70.204%\n",
      "==> Fold #5\n",
      "Loss: 0.327% Accuracy: 86.610% \n",
      "precision: 65.844% recall: 93.658% F1 score: 77.326%\n",
      "==> Fold #6\n",
      "Loss: 0.300% Accuracy: 87.561% \n",
      "precision: 68.631% recall: 93.696% F1 score: 79.228%\n",
      "==> Fold #7\n",
      "Loss: 0.368% Accuracy: 87.171% \n",
      "precision: 68.232% recall: 95.861% F1 score: 79.721%\n",
      "==> Fold #8\n",
      "Loss: 0.401% Accuracy: 80.805% \n",
      "precision: 44.904% recall: 93.069% F1 score: 60.580%\n",
      "==> Fold #9\n",
      "Loss: 0.370% Accuracy: 81.927% \n",
      "precision: 51.513% recall: 93.362% F1 score: 66.393%\n",
      "S05\n",
      "==> Fold #0\n",
      "Loss: 0.278% Accuracy: 91.018% \n",
      "precision: 80.542% recall: 94.418% F1 score: 86.930%\n",
      "==> Fold #1\n",
      "Loss: 0.352% Accuracy: 84.698% \n",
      "precision: 50.821% recall: 95.820% F1 score: 66.416%\n",
      "==> Fold #2\n",
      "Loss: 0.297% Accuracy: 89.301% \n",
      "precision: 74.405% recall: 94.087% F1 score: 83.097%\n",
      "==> Fold #3\n",
      "Loss: 0.304% Accuracy: 86.489% \n",
      "precision: 63.002% recall: 95.404% F1 score: 75.889%\n",
      "==> Fold #4\n",
      "Loss: 0.305% Accuracy: 86.464% \n",
      "precision: 63.987% recall: 93.413% F1 score: 75.949%\n",
      "==> Fold #5\n",
      "Loss: 0.295% Accuracy: 89.251% \n",
      "precision: 74.487% recall: 93.899% F1 score: 83.074%\n",
      "==> Fold #6\n",
      "Loss: 0.335% Accuracy: 85.170% \n",
      "precision: 57.506% recall: 93.842% F1 score: 71.312%\n",
      "==> Fold #7\n",
      "Loss: 0.317% Accuracy: 89.550% \n",
      "precision: 76.374% recall: 95.782% F1 score: 84.984%\n",
      "==> Fold #8\n",
      "Loss: 0.308% Accuracy: 88.004% \n",
      "precision: 66.256% recall: 94.607% F1 score: 77.933%\n",
      "==> Fold #9\n",
      "Loss: 0.331% Accuracy: 87.382% \n",
      "precision: 65.025% recall: 95.768% F1 score: 77.457%\n",
      "S06\n",
      "==> Fold #0\n",
      "Loss: 0.313% Accuracy: 86.418% \n",
      "precision: 66.278% recall: 95.095% F1 score: 78.114%\n",
      "==> Fold #1\n",
      "Loss: 0.341% Accuracy: 87.223% \n",
      "precision: 68.609% recall: 95.563% F1 score: 79.873%\n",
      "==> Fold #2\n",
      "Loss: 0.363% Accuracy: 81.858% \n",
      "precision: 50.583% recall: 92.735% F1 score: 65.460%\n",
      "==> Fold #3\n",
      "Loss: 0.347% Accuracy: 82.736% \n",
      "precision: 52.370% recall: 93.741% F1 score: 67.198%\n",
      "==> Fold #4\n",
      "Loss: 0.373% Accuracy: 84.297% \n",
      "precision: 61.335% recall: 92.941% F1 score: 73.901%\n",
      "==> Fold #5\n",
      "Loss: 0.328% Accuracy: 85.613% \n",
      "precision: 65.295% recall: 91.513% F1 score: 76.212%\n",
      "==> Fold #6\n",
      "Loss: 0.355% Accuracy: 86.293% \n",
      "precision: 65.579% recall: 96.568% F1 score: 78.112%\n",
      "==> Fold #7\n",
      "Loss: 0.327% Accuracy: 86.951% \n",
      "precision: 70.707% recall: 93.047% F1 score: 80.353%\n",
      "==> Fold #8\n",
      "Loss: 0.283% Accuracy: 89.756% \n",
      "precision: 77.389% recall: 93.258% F1 score: 84.586%\n",
      "==> Fold #9\n",
      "Loss: 0.404% Accuracy: 81.024% \n",
      "precision: 49.883% recall: 92.109% F1 score: 64.718%\n",
      "S07\n",
      "==> Fold #0\n",
      "Loss: 0.343% Accuracy: 86.245% \n",
      "precision: 69.314% recall: 89.364% F1 score: 78.072%\n",
      "==> Fold #1\n",
      "Loss: 0.379% Accuracy: 83.718% \n",
      "precision: 57.440% recall: 93.476% F1 score: 71.156%\n",
      "==> Fold #2\n",
      "Loss: 0.328% Accuracy: 87.266% \n",
      "precision: 72.012% recall: 90.944% F1 score: 80.379%\n",
      "==> Fold #3\n",
      "Loss: 0.370% Accuracy: 85.443% \n",
      "precision: 65.794% recall: 93.333% F1 score: 77.180%\n",
      "==> Fold #4\n",
      "Loss: 0.353% Accuracy: 85.561% \n",
      "precision: 64.765% recall: 93.645% F1 score: 76.572%\n",
      "==> Fold #5\n",
      "Loss: 0.365% Accuracy: 84.808% \n",
      "precision: 63.377% recall: 93.409% F1 score: 75.517%\n",
      "==> Fold #6\n",
      "Loss: 0.317% Accuracy: 87.409% \n",
      "precision: 70.085% recall: 92.850% F1 score: 79.877%\n",
      "==> Fold #7\n",
      "Loss: 0.401% Accuracy: 80.579% \n",
      "precision: 48.574% recall: 91.304% F1 score: 63.412%\n",
      "==> Fold #8\n",
      "Loss: 0.323% Accuracy: 87.846% \n",
      "precision: 73.169% recall: 94.054% F1 score: 82.307%\n",
      "==> Fold #9\n",
      "Loss: 0.348% Accuracy: 85.148% \n",
      "precision: 61.835% recall: 93.256% F1 score: 74.363%\n",
      "S08\n",
      "==> Fold #0\n",
      "Loss: 0.332% Accuracy: 88.536% \n",
      "precision: 74.548% recall: 92.047% F1 score: 82.378%\n",
      "==> Fold #1\n",
      "Loss: 0.311% Accuracy: 88.746% \n",
      "precision: 73.291% recall: 92.836% F1 score: 81.914%\n",
      "==> Fold #2\n",
      "Loss: 0.376% Accuracy: 85.781% \n",
      "precision: 64.493% recall: 93.295% F1 score: 76.266%\n",
      "==> Fold #3\n",
      "Loss: 0.316% Accuracy: 88.513% \n",
      "precision: 72.820% recall: 94.399% F1 score: 82.217%\n",
      "==> Fold #4\n",
      "Loss: 0.326% Accuracy: 87.882% \n",
      "precision: 71.642% recall: 93.538% F1 score: 81.139%\n",
      "==> Fold #5\n",
      "Loss: 0.360% Accuracy: 82.629% \n",
      "precision: 48.823% recall: 94.817% F1 score: 64.456%\n",
      "==> Fold #6\n",
      "Loss: 0.306% Accuracy: 87.485% \n",
      "precision: 70.016% recall: 93.208% F1 score: 79.964%\n",
      "==> Fold #7\n",
      "Loss: 0.382% Accuracy: 82.158% \n",
      "precision: 47.840% recall: 95.755% F1 score: 63.803%\n",
      "==> Fold #8\n",
      "Loss: 0.327% Accuracy: 84.470% \n",
      "precision: 60.566% recall: 88.519% F1 score: 71.922%\n",
      "==> Fold #9\n",
      "Loss: 0.334% Accuracy: 85.497% \n",
      "precision: 57.816% recall: 95.709% F1 score: 72.086%\n",
      "S09\n",
      "==> Fold #0\n",
      "Loss: 0.389% Accuracy: 83.223% \n",
      "precision: 57.824% recall: 90.211% F1 score: 70.474%\n",
      "==> Fold #1\n",
      "Loss: 0.358% Accuracy: 85.449% \n",
      "precision: 62.033% recall: 92.536% F1 score: 74.275%\n",
      "==> Fold #2\n",
      "Loss: 0.353% Accuracy: 87.136% \n",
      "precision: 69.897% recall: 93.617% F1 score: 80.036%\n",
      "==> Fold #3\n",
      "Loss: 0.370% Accuracy: 84.911% \n",
      "precision: 62.381% recall: 92.580% F1 score: 74.538%\n",
      "==> Fold #4\n",
      "Loss: 0.392% Accuracy: 83.174% \n",
      "precision: 54.286% recall: 95.132% F1 score: 69.126%\n",
      "==> Fold #5\n",
      "Loss: 0.375% Accuracy: 86.109% \n",
      "precision: 66.667% recall: 90.713% F1 score: 76.853%\n",
      "==> Fold #6\n",
      "Loss: 0.331% Accuracy: 86.818% \n",
      "precision: 64.524% recall: 94.316% F1 score: 76.626%\n",
      "==> Fold #7\n",
      "Loss: 0.364% Accuracy: 84.785% \n",
      "precision: 62.351% recall: 93.341% F1 score: 74.762%\n",
      "==> Fold #8\n",
      "Loss: 0.346% Accuracy: 84.149% \n",
      "precision: 58.062% recall: 93.959% F1 score: 71.772%\n",
      "==> Fold #9\n",
      "Loss: 0.373% Accuracy: 83.659% \n",
      "precision: 62.272% recall: 88.990% F1 score: 73.271%\n",
      "S01\n",
      "==> Fold #0\n",
      "Loss: 0.426% Accuracy: 78.605% \n",
      "precision: 44.934% recall: 96.672% F1 score: 61.352%\n",
      "==> Fold #1\n",
      "Loss: 0.407% Accuracy: 81.119% \n",
      "precision: 53.596% recall: 97.468% F1 score: 69.162%\n",
      "==> Fold #2\n",
      "Loss: 0.573% Accuracy: 73.108% \n",
      "precision: 27.688% recall: 97.019% F1 score: 43.081%\n",
      "==> Fold #3\n",
      "Loss: 0.466% Accuracy: 75.129% \n",
      "precision: 37.896% recall: 98.394% F1 score: 54.718%\n",
      "==> Fold #4\n",
      "Loss: 0.417% Accuracy: 84.225% \n",
      "precision: 62.722% recall: 97.359% F1 score: 76.294%\n",
      "==> Fold #5\n",
      "Loss: 0.546% Accuracy: 73.866% \n",
      "precision: 27.842% recall: 96.774% F1 score: 43.243%\n",
      "==> Fold #6\n",
      "Loss: 0.427% Accuracy: 82.347% \n",
      "precision: 59.087% recall: 97.949% F1 score: 73.710%\n",
      "==> Fold #7\n",
      "Loss: 0.443% Accuracy: 76.824% \n",
      "precision: 38.360% recall: 99.002% F1 score: 55.295%\n",
      "==> Fold #8\n",
      "Loss: 0.412% Accuracy: 81.928% \n",
      "precision: 56.922% recall: 96.970% F1 score: 71.735%\n",
      "==> Fold #9\n",
      "Loss: 0.514% Accuracy: 73.570% \n",
      "precision: 27.533% recall: 98.072% F1 score: 42.995%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ESALAB~1\\AppData\\Local\\Temp/ipykernel_15056/2448796993.py:14: RuntimeWarning: invalid value encountered in longlong_scalars\n",
      "  precision = (tp/(tp + fp))*100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S02\n",
      "==> Fold #0\n",
      "Loss: 0.423% Accuracy: 78.575% \n",
      "precision: 43.305% recall: 97.876% F1 score: 60.043%\n",
      "==> Fold #1\n",
      "Loss: 0.445% Accuracy: 80.958% \n",
      "precision: 51.135% recall: 97.317% F1 score: 67.043%\n",
      "==> Fold #2\n",
      "Loss: 0.428% Accuracy: 83.245% \n",
      "precision: 56.338% recall: 98.361% F1 score: 71.642%\n",
      "==> Fold #3\n",
      "Loss: 0.446% Accuracy: 82.258% \n",
      "precision: 60.955% recall: 97.375% F1 score: 74.976%\n",
      "==> Fold #4\n",
      "Loss: 0.419% Accuracy: 86.732% \n",
      "precision: 69.851% recall: 97.062% F1 score: 81.239%\n",
      "==> Fold #5\n",
      "Loss: 0.381% Accuracy: 84.614% \n",
      "precision: 64.135% recall: 97.384% F1 score: 77.337%\n",
      "==> Fold #6\n",
      "Loss: 0.450% Accuracy: 78.040% \n",
      "precision: 38.684% recall: 96.296% F1 score: 55.196%\n",
      "==> Fold #7\n",
      "Loss: 0.455% Accuracy: 74.886% \n",
      "precision: 27.721% recall: 98.061% F1 score: 43.223%\n",
      "==> Fold #8\n",
      "Loss: 0.500% Accuracy: 78.835% \n",
      "precision: 53.720% recall: 98.000% F1 score: 69.398%\n",
      "==> Fold #9\n",
      "Loss: 0.400% Accuracy: 80.833% \n",
      "precision: 45.889% recall: 97.181% F1 score: 62.340%\n",
      "S03\n",
      "==> Fold #0\n",
      "Loss: 0.474% Accuracy: 76.103% \n",
      "precision: 38.217% recall: 98.361% F1 score: 55.046%\n",
      "==> Fold #1\n",
      "Loss: 0.441% Accuracy: 78.908% \n",
      "precision: 44.268% recall: 97.887% F1 score: 60.965%\n",
      "==> Fold #2\n",
      "Loss: 0.449% Accuracy: 80.151% \n",
      "precision: 45.939% recall: 97.963% F1 score: 62.547%\n",
      "==> Fold #3\n",
      "Loss: 0.393% Accuracy: 84.101% \n",
      "precision: 63.296% recall: 97.786% F1 score: 76.849%\n",
      "==> Fold #4\n",
      "Loss: 0.407% Accuracy: 82.199% \n",
      "precision: 53.901% recall: 96.714% F1 score: 69.223%\n",
      "==> Fold #5\n",
      "Loss: 0.401% Accuracy: 82.756% \n",
      "precision: 59.395% recall: 96.883% F1 score: 73.643%\n",
      "==> Fold #6\n",
      "Loss: 0.346% Accuracy: 88.122% \n",
      "precision: 73.010% recall: 98.075% F1 score: 83.706%\n",
      "==> Fold #7\n",
      "Loss: 0.451% Accuracy: 77.829% \n",
      "precision: 43.312% recall: 96.797% F1 score: 59.846%\n",
      "==> Fold #8\n",
      "Loss: 0.522% Accuracy: 77.463% \n",
      "precision: 38.217% recall: 96.774% F1 score: 54.795%\n",
      "==> Fold #9\n",
      "Loss: 0.481% Accuracy: 75.707% \n",
      "precision: 34.395% recall: 98.405% F1 score: 50.973%\n",
      "S05\n",
      "==> Fold #0\n",
      "Loss: 0.405% Accuracy: 78.577% \n",
      "precision: 37.438% recall: 97.645% F1 score: 54.125%\n",
      "==> Fold #1\n",
      "Loss: 0.403% Accuracy: 79.000% \n",
      "precision: 38.506% recall: 97.505% F1 score: 55.209%\n",
      "==> Fold #2\n",
      "Loss: 0.418% Accuracy: 81.239% \n",
      "precision: 51.764% recall: 97.077% F1 score: 67.523%\n",
      "==> Fold #3\n",
      "Loss: 0.369% Accuracy: 86.091% \n",
      "precision: 64.561% recall: 97.041% F1 score: 77.537%\n",
      "==> Fold #4\n",
      "Loss: 0.515% Accuracy: 73.376% \n",
      "precision: 28.138% recall: 97.443% F1 score: 43.666%\n",
      "==> Fold #5\n",
      "Loss: 0.449% Accuracy: 75.367% \n",
      "precision: 28.466% recall: 98.023% F1 score: 44.120%\n",
      "==> Fold #6\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\ESALAB~1\\AppData\\Local\\Temp/ipykernel_15056/1235705947.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[0mskf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_n_splits\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m         \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlosses\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscores_in_fold\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrainingModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mskf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlayers\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilterList\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m         \u001b[0mm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscores_in_fold\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscores_in_fold\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\ESALAB~1\\AppData\\Local\\Temp/ipykernel_15056/1130480598.py\u001b[0m in \u001b[0;36mtrainingModel\u001b[1;34m(skf, X, y, parameters, class_weight, layers, filterList)\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msetModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_length\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_features\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlayers\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilterList\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m         \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclass_weight\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclass_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccuracy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\ESA LAB\\anaconda3\\envs\\HighHeelWhatever\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1181\u001b[0m                 _r=1):\n\u001b[0;32m   1182\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1183\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1184\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1185\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\ESA LAB\\anaconda3\\envs\\HighHeelWhatever\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    887\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    891\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\ESA LAB\\anaconda3\\envs\\HighHeelWhatever\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    915\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    916\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 917\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    918\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    919\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\ESA LAB\\anaconda3\\envs\\HighHeelWhatever\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3021\u001b[0m       (graph_function,\n\u001b[0;32m   3022\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m-> 3023\u001b[1;33m     return graph_function._call_flat(\n\u001b[0m\u001b[0;32m   3024\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0;32m   3025\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\ESA LAB\\anaconda3\\envs\\HighHeelWhatever\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1958\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1959\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1960\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1961\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1962\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32mc:\\Users\\ESA LAB\\anaconda3\\envs\\HighHeelWhatever\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    589\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    590\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 591\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    592\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    593\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\ESA LAB\\anaconda3\\envs\\HighHeelWhatever\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     57\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for line in paraList:\n",
    "    parameters, layers, filterList = getParaMeters(line)\n",
    "    paraStr = \"Epoch=\" + str(parameters[0]) + \" Batch Size=\" + str(parameters[1]) + \" steps=\" + str(parameters[2]) + \" length=\" + str(parameters[3])\n",
    "\n",
    "    date, currTime = str(datetime.date.today()), str(time.strftime(\"%H-%M\", time.localtime()))\n",
    "\n",
    "    resultPath = os.getcwd() +\"/result/\" + date + '/' \n",
    "    if not os.path.exists(resultPath):\n",
    "        os.mkdir(resultPath)\n",
    "\n",
    "    toFile = pd.DataFrame(columns = [\"test Subject\", \"Train Accuracy\", \"Train Loss\", \"Accuracy\", \"FOG Spec\", \"FOG Sen\", \"FOG F1\", \"PreFOG Spec\", \"PreFOG Sen\", \"PreFOG F1\"])\n",
    "\n",
    "    for j in range(len(testSubs)):\n",
    "        print(testSubs[j])\n",
    "        temp = []\n",
    "        temp.append(testSubs[j])\n",
    "        \n",
    "        X, y, testX, testy = getXy(testSubs[j])\n",
    "        \n",
    "        unique, counts = np.unique(y, return_counts=True)\n",
    "        class_weight = {0:(1/counts[0])*len(y)/2, 1:(1/counts[1])*len(y)/2, 2:(1/counts[2])*len(y)/2}\n",
    "\n",
    "        skf = StratifiedKFold(n_splits=10, shuffle = True, random_state=42)\n",
    "        skf.get_n_splits(X, y)\n",
    "\n",
    "        model, losses, scores_in_fold = trainingModel(skf, X, y, parameters, class_weight, layers, filterList)\n",
    "\n",
    "        m, s = np.mean(scores_in_fold), np.std(scores_in_fold)\n",
    "        temp.append(round(m, 3))\n",
    "\n",
    "        m, s = np.mean(losses), np.std(losses)\n",
    "        temp.append(round(m, 3))\n",
    "\n",
    "        confus, y_pred = predictData(model, testX, testy, parameters)\n",
    "        accuracy = accuracy_score(testy, y_pred)*100\n",
    "        perform = performance(1, confus)\n",
    "\n",
    "        temp.append(round(accuracy, 3))\n",
    "        for k in range(3):\n",
    "            temp.append(round(perform[1+k], 3))\n",
    "        \n",
    "        perform = performance(2, confus)\n",
    "        for k in range(3):\n",
    "            temp.append(round(perform[1+k], 3))\n",
    "\n",
    "        toFile.loc[j] = temp\n",
    "    toFile.to_csv(resultPath + currTime + paraStr + \".csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.11 ('HighHeelWhatever')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8986fb416174cc2474d1ce69838b7b56508ac61c47a66825c7584556038319d3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
