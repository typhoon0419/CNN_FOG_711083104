{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "import os\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D\n",
    "from tensorflow.keras.layers import LSTM, TimeDistributed, ConvLSTM2D\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "from sklearn.metrics import confusion_matrix,accuracy_score\n",
    "from keras import backend as k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_path = os.getcwd() + \"/dataset/total_train.csv\"\n",
    "test_data_path = os.getcwd() + \"/dataset/total_test.csv\"\n",
    "new_data_path = os.getcwd() + \"/dataset/merge.csv\"\n",
    "\n",
    "fromPath = os.getcwd() +\"/dataset/windows\"\n",
    "savePath = os.getcwd() +\"/dataset\"\n",
    "testSub = \"S09\"\n",
    "winLen = int(1*64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_window(window_length,dataframe):\n",
    "\n",
    "  indices = list(dataframe.index)\n",
    "  time = []\n",
    "  time_count = 0\n",
    "  for j in indices:\n",
    "    time.append(dataframe.loc[j, 'time'])\n",
    "\n",
    "\n",
    "  # indices記錄所有Action==act的index\n",
    "  groups = [] # 用來暫存一組(同action)資料的, 型態是[][]\n",
    "  temp = [] # 用來暫存一行資料的\n",
    "  group_count = 0\n",
    "  for i in range(len(indices)):\n",
    "    if i == len(indices)-1:\n",
    "      temp.append(indices[i])\n",
    "      groups.append(temp)\n",
    "      temp = []\n",
    "      break # 如果i已經來到最後的話就break\n",
    "    temp.append(indices[i])\n",
    "    #time_count = time_count + 1\n",
    "    if time[i+1]-16 > time[i]: #如果下個index不是連續的話, 就將前面這些存成第一組\n",
    "      group_count+=1\n",
    "      #time_count = time_count + 1\n",
    "      groups.append(temp)\n",
    "      temp = []\n",
    "\n",
    "  #print(groups)\n",
    "\n",
    "  fs = 64\n",
    "  # window_length = 1\n",
    "  # window_length = int(window_length*fs)\n",
    "\n",
    "  final_dataframe = pd.DataFrame()\n",
    "  sumOfAct=0\n",
    "\n",
    "  for i in groups: # group[][]的每一行i\n",
    "    required = math.floor(len(i)/int(window_length/2))\n",
    "\n",
    "    \n",
    "    sumOfAct= sumOfAct+required\n",
    "\n",
    "    req_index = i[0:(required*int(window_length/2))]\n",
    "\n",
    "    #print(req_index)\n",
    "    # concat([要結合的data集合], axis=0是方向為直的)\n",
    "    final_dataframe = pd.concat([final_dataframe,dataframe.iloc[req_index,:]],axis = 0)\n",
    "  \n",
    "  \n",
    "\n",
    "  return final_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveDFtocsv(fromPath: str, toPath: str, compare:str, useOriginal=True, saveName = \"\"):\n",
    "    \"\"\"_summary_\n",
    "\n",
    "    Args:\n",
    "        fromPath (str): _description_\n",
    "        toPath (str): _description_\n",
    "        compare (str): _description_\n",
    "    \"\"\"\n",
    "    \n",
    "    for file in os.listdir(fromPath):\n",
    "        if compare not in file:\n",
    "            continue\n",
    "\n",
    "        filePath = fromPath + '/' + file\n",
    "        dataset = create_window(winLen, pd.read_csv(filePath))\n",
    "\n",
    "        if useOriginal:\n",
    "            savePath = toPath + \"/\" + file\n",
    "        else:\n",
    "            savePath = toPath + \"/\" + saveName + \".csv\"\n",
    "        dataset.to_csv(savePath, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_patients_dataset = pd.DataFrame()\n",
    "test_patients_dataset = pd.DataFrame()\n",
    "\n",
    "for file in os.listdir(fromPath):\n",
    "    if \"win_\" not in file:\n",
    "        continue\n",
    "    \n",
    "    filePath = fromPath + '/' + file\n",
    "    dataset = pd.read_csv(filePath)\n",
    "\n",
    "    if testSub in file:\n",
    "        test_patients_dataset = test_patients_dataset.append(dataset)\n",
    "    else:    \n",
    "        train_patients_dataset = train_patients_dataset.append(dataset)\n",
    "\n",
    "to_path = savePath + \"/not_windowed_train.csv\"\n",
    "train_patients_dataset.to_csv(to_path, index=False)\n",
    "\n",
    "to_path = savePath + \"/not_windowed_test.csv\"\n",
    "test_patients_dataset.to_csv(to_path, index=False)\n",
    "\n",
    "del dataset, test_patients_dataset, train_patients_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "saveDFtocsv(savePath, savePath, \"not_windowed_train\", False, \"total_train\")\n",
    "saveDFtocsv(savePath, savePath, \"not_windowed_test\", False, \"total_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTimeAndDF(path:str):\n",
    "    \"\"\"get dataset and dataset's time list\n",
    "       some dataset's time is not continued, so get the time is for split windows\n",
    "\n",
    "    Args:\n",
    "        path (str): path of the dataset(for .csv)\n",
    "\n",
    "    Returns:\n",
    "        time (list): the list of dataset time\n",
    "        df (np.array): dataset, columns include [\"A_F\", \"A_V\", \"A_L\", \"Action\"]\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(path)\n",
    "    time = list(df['time'])\n",
    "    action = list(df['Action'])\n",
    "    df = df[[\"A_F\", \"A_V\", \"A_L\"]].values\n",
    "\n",
    "    return time, df, action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainTime, trainData, trainAction = getTimeAndDF(train_data_path)\n",
    "testTime, testData, testAction = getTimeAndDF(test_data_path)\n",
    "newTime, newData, newAction = getTimeAndDF(new_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainData = (trainData-trainData.mean())/(trainData.std())\n",
    "testData = (testData-testData.mean())/(testData.std())\n",
    "newData = (newData-newData.mean())/(newData.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTotalWindows(indices:list, windowSize:int , gap: float):\n",
    "    \"\"\"because\n",
    "    check every part of time\n",
    "\n",
    "    Args:\n",
    "        indices (list): _description_\n",
    "        windowSize (int): _description_\n",
    "        gap (float): _description_\n",
    "\n",
    "    Returns:\n",
    "        _type_: _description_\n",
    "    \"\"\"\n",
    "    groups = []\n",
    "    group_count = 0\n",
    "    temp = []\n",
    "    lenOfGroup = []\n",
    "    length_count = 0\n",
    "    for i in range(len(indices)):\n",
    "        if i == (len(indices) - 1):\n",
    "            temp.append(indices[i])\n",
    "            length_count = length_count + 1\n",
    "\n",
    "            groups.append(temp)\n",
    "            lenOfGroup.append(length_count)\n",
    "            length_count = 0\n",
    "            temp = []\n",
    "            break\n",
    "        temp.append(indices[i])\n",
    "        length_count = length_count + 1\n",
    "        if (indices[i+1] - gap > indices[i]):\n",
    "            group_count = group_count + 1\n",
    "\n",
    "            lenOfGroup.append(length_count)\n",
    "            length_count = 0\n",
    "\n",
    "            groups.append(temp)\n",
    "            temp = []\n",
    "\n",
    "    countOfUndivisible = 0\n",
    "    totalWindows = 0\n",
    "\n",
    "    stop_Indexs = []\n",
    "    stop_Index = -windowSize\n",
    "\n",
    "\n",
    "    for lengths in lenOfGroup:\n",
    "        stop_Index = stop_Index + lengths\n",
    "        stop_Indexs.append(stop_Index)\n",
    "        totalWindows = totalWindows + int(float(lengths/windowSize)*2 -1)\n",
    "        \n",
    "        if lengths % (windowSize/2) != 0:\n",
    "            countOfUndivisible = countOfUndivisible + 1\n",
    "            print(lengths)\n",
    "\n",
    "    return totalWindows, stop_Indexs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainWindows, trainStop = getTotalWindows(trainTime, winLen, 20) \n",
    "newWindows, newStop = getTotalWindows(newTime, winLen, 0.02)\n",
    "testWindows, testStop = getTotalWindows(testTime, winLen, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "del trainTime, newTime, testTime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "def XySplit(dataset:np.array, windows:int, length:int, stop:list, action:list):\n",
    "    \"\"\"split dataset into X and y, \n",
    "    X is 2D array, size of X is [windows, 64*3]\n",
    "    y is 1D array, size of y is [windows, 1]\n",
    "\n",
    "    Args:\n",
    "        dataset (np.array): dataset\n",
    "        windows (int): total windows that get from getTotalWindows()\n",
    "        length (int): length of a piece of data, here is 3\n",
    "        stop (list): stopList that get from getTotalWindows()\n",
    "        action (list): action list\n",
    "\n",
    "    Returns:\n",
    "        X(np.array): X is 2D array, size of X is [windows, 64*3]\n",
    "        y(np.array): y is 1D array, size of y is [windows, 1]\n",
    "    \"\"\"\n",
    "    X = np.empty((windows, winLen*(length)))\n",
    "    y = np.empty((windows, 1))\n",
    "\n",
    "    stopIndex = 0\n",
    "    windowCount = 0\n",
    "    for win in range(windows):\n",
    "        for i in range(winLen):\n",
    "            if i == 0:\n",
    "                y[win] = action[int(windowCount*winLen + winLen/2)]\n",
    "\n",
    "            if int(windowCount*winLen)<len(dataset)-winLen-1:\n",
    "                for data in range(length):\n",
    "                    X[win, i*(length)+data] = dataset[int(windowCount*winLen) + i, data]\n",
    "\n",
    "            if stopIndex < len(stop):\n",
    "                if int(windowCount*winLen) == stop[stopIndex]:\n",
    "                    windowCount += 0.5\n",
    "                    stopIndex += 1\n",
    "            \n",
    "            if win == windows-1:\n",
    "                for data in range(length):\n",
    "                    X[win, i*(length) + data] = dataset[int((windowCount-0.5)*winLen) + i, data]\n",
    "        \n",
    "        windowCount += 0.5\n",
    "\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX, trainy = XySplit(trainData, trainWindows, 3, trainStop, trainAction)\n",
    "newX, newy = XySplit(newData, newWindows, 3, newStop, newAction)\n",
    "testX, testy = XySplit(testData, testWindows, 3, testStop, testAction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.concatenate((trainX, newX))\n",
    "y = np.concatenate((trainy, newy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "del trainX, trainy, newX, newy, trainData, trainWindows, newData, newWindows, testWindows, trainStop, trainAction, newStop, newAction, testStop, testAction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique, counts = np.unique(y, return_counts=True)\n",
    "class_weight = {0:(1/counts[0])*len(y)/2, 1:(1/counts[1])*len(y)/2, 2:(1/counts[2])*len(y)/2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skf = StratifiedKFold(n_splits=10, shuffle = True, random_state=42)\n",
    "skf.get_n_splits(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_3Darray(array):\n",
    "    \"\"\"_summary_\n",
    "\n",
    "    Args:\n",
    "        array (_type_): _description_\n",
    "\n",
    "    Returns:\n",
    "        _type_: _description_\n",
    "    \"\"\"\n",
    "    arr_3d = np.empty((len(array), winLen, 3))\n",
    "    arr_3d = np.reshape(array, (len(array), winLen, 3))\n",
    "    return arr_3d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = []\n",
    "scores_in_fold = []\n",
    "losses = []\n",
    "scores_outside_fold = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Fold #0\n",
      "Loss: 0.255% Accuracy: 91.245% \n",
      "precision: 85.147% recall: 89.858% F1 score: 87.439%\n",
      "==> Fold #1\n",
      "Loss: 0.239% Accuracy: 92.052% \n",
      "precision: 88.245% recall: 89.309% F1 score: 88.773%\n",
      "==> Fold #2\n",
      "Loss: 0.242% Accuracy: 90.829% \n",
      "precision: 86.577% recall: 88.474% F1 score: 87.515%\n",
      "==> Fold #3\n",
      "Loss: 0.251% Accuracy: 90.560% \n",
      "precision: 84.603% recall: 88.759% F1 score: 86.631%\n",
      "==> Fold #4\n",
      "Loss: 0.236% Accuracy: 91.465% \n",
      "precision: 83.730% recall: 91.979% F1 score: 87.661%\n",
      "==> Fold #5\n",
      "Loss: 0.249% Accuracy: 90.291% \n",
      "precision: 84.444% recall: 88.741% F1 score: 86.539%\n",
      "==> Fold #6\n",
      "Loss: 0.260% Accuracy: 90.071% \n",
      "precision: 84.762% recall: 88.411% F1 score: 86.548%\n",
      "==> Fold #7\n",
      "Loss: 0.270% Accuracy: 89.971% \n",
      "precision: 81.493% recall: 92.432% F1 score: 86.619%\n",
      "==> Fold #8\n",
      "Loss: 0.248% Accuracy: 90.974% \n",
      "precision: 88.324% recall: 87.011% F1 score: 87.663%\n",
      "==> Fold #9\n",
      "Loss: 0.259% Accuracy: 90.264% \n",
      "precision: 85.703% recall: 89.100% F1 score: 87.368%\n"
     ]
    }
   ],
   "source": [
    "for i, (train_index, val_index) in enumerate(skf.split(X, y)):\n",
    "    print(\"==> Fold #%d\" % i)\n",
    "    X_train, X_val = X[train_index], X[val_index]\n",
    "    y_train, y_val = y[train_index], y[val_index]\n",
    "\n",
    "    X_train = to_3Darray(X_train)\n",
    "    y_train = to_categorical(y_train)\n",
    "\n",
    "    X_val = to_3Darray(X_val)\n",
    "    y_val = to_categorical(y_val)\n",
    "\n",
    "\n",
    "    \n",
    "    verbose, epochs, batch_size = 0, 50, 64\n",
    "    n_timesteps, n_features, n_outputs = X_train.shape[1], X_train.shape[2], y_train.shape[1]\n",
    "\n",
    "    n_steps, n_length = 2, 32\n",
    "    X_train = X_train.reshape((X_train.shape[0], n_steps, n_length, n_features))\n",
    "    X_val = X_val.reshape((X_val.shape[0], n_steps, n_length, n_features))\n",
    "\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(TimeDistributed(Conv1D(filters=64, kernel_size=3, activation='relu'), input_shape=(None, n_length, n_features)))\n",
    "                            \n",
    "    model.add(TimeDistributed(Conv1D(filters=64, kernel_size=3, activation='relu')))\n",
    "    model.add(TimeDistributed(Conv1D(filters=64, kernel_size=3, activation='relu')))\n",
    "    model.add(TimeDistributed(Conv1D(filters=64, kernel_size=3, activation='relu')))\n",
    "    model.add(TimeDistributed(Dropout(0.5)))\n",
    "    model.add(TimeDistributed(MaxPooling1D(pool_size=2)))\n",
    "    model.add(TimeDistributed(Flatten()))\n",
    "    model.add(LSTM(100))\n",
    "    model.add(Dropout(0.5))\n",
    "    hunderdOutput = Dense(100, activation='relu')\n",
    "    model.add(hunderdOutput)  # feature\n",
    "    # 試著輸出長度為100的向量(feature) 並絳維 看他的分布有無分開\n",
    "    model.add(Dense(3, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy',optimizer='adam', metrics=['accuracy']) #可能可以調weighting\n",
    "    \n",
    "    model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, verbose=verbose, class_weight = class_weight)\n",
    "    \n",
    "    loss, accuracy = model.evaluate(X_val, y_val, batch_size=batch_size, verbose=0)\n",
    "    \n",
    "\n",
    "    y_pred = (model.predict(X_val) > 0.5).astype(\"int32\")\n",
    "    y_val = np.argmax(y_val, axis=1)\n",
    "    y_pred = np.argmax(y_pred, axis=1)\n",
    "\n",
    "    confus = confusion_matrix(y_val, y_pred, labels=None, sample_weight=None)\n",
    "    tp = confus[1][1]\n",
    "    tn = confus[0][0] + confus[0][2] + confus[2][0] + confus[2][2]\n",
    "    fp = confus[1][0] + confus[1][2]\n",
    "    fn = confus[0][1] + confus[2][1]\n",
    "\n",
    "    precision = (tp/(tp + fp))*100\n",
    "    recall =  (tp / (tp + fn))*100   #sensitivity\n",
    "    F1_score = ((2*tp) / (2*tp + fp + fn))*100\n",
    "\n",
    "    score = accuracy\n",
    "    losses.append(loss)\n",
    "    \n",
    "    score = score * 100.0\n",
    "    scores_in_fold.append(score)\n",
    "    \n",
    "    print('Loss: %.3f%% Accuracy: %.3f%% ' % (loss, score))\n",
    "    print('precision: %.3f%% recall: %.3f%% F1 score: %.3f%%' % (precision, recall, F1_score))\n",
    "    # print(confus)\n",
    "\n",
    "\n",
    "    \n",
    "    '''score = evaluate_model(X_train, y_train, X_val, y_val)\n",
    "    score = score * 100.0\n",
    "    print(score)\n",
    "    scores.append(score)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 90.772% (+/-0.637)\n",
      "Loss: 0.251% (+/-0.010)\n"
     ]
    }
   ],
   "source": [
    "m, s = np.mean(scores_in_fold), np.std(scores_in_fold)\n",
    "print('Accuracy: %.3f%% (+/-%.3f)' % (m, s))\n",
    "m, s = np.mean(losses), np.std(losses)\n",
    "print('Loss: %.3f%% (+/-%.3f)' % (m, s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "testX = to_3Darray(testX)\n",
    "testX = testX.reshape((testX.shape[0], n_steps, n_length, n_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "testy = to_categorical(testy)\n",
    "y_pred = (model.predict(testX) > 0.5).astype(\"int32\")\n",
    "testy = np.argmax(testy, axis=1)\n",
    "y_pred = np.argmax(y_pred, axis=1)\n",
    "\n",
    "confus = confusion_matrix(testy, y_pred, labels=None, sample_weight=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2791   47   43]\n",
      " [ 438   94    9]\n",
      " [  39   11    4]]\n"
     ]
    }
   ],
   "source": [
    "print(confus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp = confus[1][1]\n",
    "tn = confus[0][0] + confus[0][2] + confus[2][0] + confus[2][2]\n",
    "fp = confus[0][1] + confus[2][1]\n",
    "fn = confus[1][0] + confus[1][2]\n",
    "\n",
    "precision = (tp/(tp + fp))*100\n",
    "sensitivity = (tp / (tp + fn))*100  # sensitivity\n",
    "specificity = (tn/(tn + fp))*100\n",
    "F1_score = ((2*tp) / (2*tp + fp + fn))*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = accuracy_score(testy, y_pred)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 83.113% \n",
      "FOG: specificity: 98.024% sensitivity: 17.375% F1 score: 27.128%\n"
     ]
    }
   ],
   "source": [
    "accuracy = accuracy\n",
    "print('Accuracy: %.3f%% ' % (accuracy))\n",
    "print('FOG: specificity: %.3f%% sensitivity: %.3f%% F1 score: %.3f%%' % (specificity, sensitivity, F1_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PreFOG: specificity: 98.480% sensitivity: 7.407% F1 score: 7.273%\n"
     ]
    }
   ],
   "source": [
    "tp = confus[2][2]\n",
    "tn = confus[0][0] + confus[0][1] + confus[1][0] + confus[1][1]\n",
    "fp = confus[0][2] + confus[1][2]\n",
    "fn = confus[2][0] + confus[2][1]\n",
    "\n",
    "precision = (tp/(tp + fp))*100\n",
    "sensitivity = (tp / (tp + fn))*100  # sensitivity\n",
    "specificity = (tn/(tn + fp))*100\n",
    "F1_score = ((2*tp) / (2*tp + fp + fn))*100\n",
    "print('PreFOG: specificity: %.3f%% sensitivity: %.3f%% F1 score: %.3f%%' % (specificity, sensitivity, F1_score))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.11 ('HighHeelWhatever')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8986fb416174cc2474d1ce69838b7b56508ac61c47a66825c7584556038319d3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
