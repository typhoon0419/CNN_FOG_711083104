{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import math\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D\n",
    "from tensorflow.keras.layers import LSTM, TimeDistributed, ConvLSTM2D\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.utils import np_utils\n",
    "from keras.constraints import maxnorm\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path=\"C:/Users/ESA LAB/Desktop/CNN_FOG/dataset\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "people[] -> 存所有在資料夾中的txt檔\n",
    "\n",
    "'''\n",
    "people = [] #S01R01.txt~S10R01.txt\n",
    "\n",
    "for person in os.listdir(data_path):\n",
    "    if '.txt' in person:\n",
    "        if 'activity_labels' in person:\n",
    "            continue\n",
    "        people.append(person)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "\n",
    "'''\n",
    "def label_prefog(dataset, window_length):\n",
    "# def label_prefog(dataset, window_length = 1):\n",
    "    # Drop all the rows for which Action is 0 or the rows which are not part of the experiment\n",
    "    # 將所有的Action==0(未實驗)的部分刪除，並不回傳任何值(implace的部分)\n",
    "    dataset.drop(index = list(dataset[dataset['Action'] == 0].index),\n",
    "                 inplace = True)\n",
    "    \n",
    "    window_length = int(64*window_length)\n",
    "    #print(window_length)\n",
    "    fog_index = []\n",
    "    for i in dataset.index:\n",
    "        if dataset.loc[i, 'Action'] == 2: # 取index和'Action'對應到的資料\n",
    "            fog_index.append(i) # 將發生fog的index存入fog_index這個array\n",
    "    fog_index\n",
    "    \n",
    "    # print(fog_index)\n",
    "    \n",
    "    start_indices = []\n",
    "    for i in fog_index:\n",
    "        if (dataset.loc[i-1, 'Action'] != dataset.loc[i, 'Action']):\n",
    "            start_indices.append(i) # 將發生fog之前的index存入start_indices這個array\n",
    "    \n",
    "    # print(start_indices)\n",
    "\n",
    "    prefog = []\n",
    "    for start in start_indices:\n",
    "        prefog_start = [x for x in range(start-window_length, start)] # prefog_start把FOG之前的windows標記起來\n",
    "        prefog.append(prefog_start)\n",
    "        \n",
    "    # print(prefog) # 2D array\n",
    "\n",
    "    prefog = [item for sublist in prefog for item in sublist] # 把[][]轉換成[]\n",
    "\n",
    "    # print(prefog) # 1D array\n",
    "\n",
    "    for i in prefog:\n",
    "        dataset.loc[i,'Action'] = 3 # 把紀錄在Pre-fog的index action 記錄成3\n",
    "    dataset['Action'] = dataset['Action'] - 1 # 所有action編號-1 使0=walk 1=fog 2= pre-fog\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S01\n",
      "S01R01.txt  is read\tAdding S01R01.txt to dataset\tS01R01.txt is labelled\n",
      "\n",
      "S01\n",
      "S01R02.txt  is read\tAdding S01R02.txt to dataset\tS01R02.txt is labelled\n",
      "\n",
      "S02\n",
      "S02R01.txt  is read\tAdding S02R01.txt to dataset\tS02R01.txt is labelled\n",
      "\n",
      "S02\n",
      "S02R02.txt  is read\tAdding S02R02.txt to dataset\tS02R02.txt is labelled\n",
      "\n",
      "S03\n",
      "S03R01.txt  is read\tAdding S03R01.txt to dataset\tS03R01.txt is labelled\n",
      "\n",
      "S03\n",
      "S03R02.txt  is read\tAdding S03R02.txt to dataset\tS03R02.txt is labelled\n",
      "\n",
      "S03\n",
      "S03R03.txt  is read\tAdding S03R03.txt to dataset\tS03R03.txt is labelled\n",
      "\n",
      "S04\n",
      "S04R01.txt  is read\tAdding S04R01.txt to dataset\tS04R01.txt is labelled\n",
      "\n",
      "S05\n",
      "S05R01.txt  is read\tAdding S05R01.txt to dataset\tS05R01.txt is labelled\n",
      "\n",
      "S05\n",
      "S05R02.txt  is read\tAdding S05R02.txt to dataset\tS05R02.txt is labelled\n",
      "\n",
      "S06\n",
      "S06R01.txt  is read\tAdding S06R01.txt to dataset\tS06R01.txt is labelled\n",
      "\n",
      "S06\n",
      "S06R02.txt  is read\tAdding S06R02.txt to dataset\tS06R02.txt is labelled\n",
      "\n",
      "S07\n",
      "S07R01.txt  is read\tAdding S07R01.txt to dataset\tS07R01.txt is labelled\n",
      "\n",
      "S07\n",
      "S07R02.txt  is read\tAdding S07R02.txt to dataset\tS07R02.txt is labelled\n",
      "\n",
      "S08\n",
      "S08R01.txt  is read\tAdding S08R01.txt to dataset\tS08R01.txt is labelled\n",
      "\n",
      "S09\n",
      "S09R01.txt  is read\tAdding S09R01.txt to dataset\tS09R01.txt is labelled\n",
      "\n",
      "S10\n",
      "S10R01.txt  is read\tAdding S10R01.txt to dataset\tS10R01.txt is labelled\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>A_F</th>\n",
       "      <th>A_V</th>\n",
       "      <th>A_L</th>\n",
       "      <th>L_F</th>\n",
       "      <th>L_V</th>\n",
       "      <th>L_L</th>\n",
       "      <th>T_F</th>\n",
       "      <th>T_V</th>\n",
       "      <th>T_L</th>\n",
       "      <th>Action</th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>600000</td>\n",
       "      <td>-353</td>\n",
       "      <td>950</td>\n",
       "      <td>178</td>\n",
       "      <td>172</td>\n",
       "      <td>953</td>\n",
       "      <td>232</td>\n",
       "      <td>87</td>\n",
       "      <td>1000</td>\n",
       "      <td>-116</td>\n",
       "      <td>0</td>\n",
       "      <td>S10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>600015</td>\n",
       "      <td>-383</td>\n",
       "      <td>960</td>\n",
       "      <td>198</td>\n",
       "      <td>181</td>\n",
       "      <td>944</td>\n",
       "      <td>212</td>\n",
       "      <td>97</td>\n",
       "      <td>990</td>\n",
       "      <td>-87</td>\n",
       "      <td>0</td>\n",
       "      <td>S10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>600031</td>\n",
       "      <td>-363</td>\n",
       "      <td>950</td>\n",
       "      <td>178</td>\n",
       "      <td>190</td>\n",
       "      <td>935</td>\n",
       "      <td>232</td>\n",
       "      <td>97</td>\n",
       "      <td>1000</td>\n",
       "      <td>-116</td>\n",
       "      <td>0</td>\n",
       "      <td>S10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>600046</td>\n",
       "      <td>-363</td>\n",
       "      <td>950</td>\n",
       "      <td>198</td>\n",
       "      <td>172</td>\n",
       "      <td>944</td>\n",
       "      <td>202</td>\n",
       "      <td>97</td>\n",
       "      <td>1000</td>\n",
       "      <td>-97</td>\n",
       "      <td>0</td>\n",
       "      <td>S10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>600062</td>\n",
       "      <td>-343</td>\n",
       "      <td>960</td>\n",
       "      <td>207</td>\n",
       "      <td>163</td>\n",
       "      <td>944</td>\n",
       "      <td>232</td>\n",
       "      <td>106</td>\n",
       "      <td>1000</td>\n",
       "      <td>-97</td>\n",
       "      <td>0</td>\n",
       "      <td>S10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     time  A_F  A_V  A_L  L_F  L_V  L_L  T_F   T_V  T_L  Action name\n",
       "0  600000 -353  950  178  172  953  232   87  1000 -116       0  S10\n",
       "1  600015 -383  960  198  181  944  212   97   990  -87       0  S10\n",
       "2  600031 -363  950  178  190  935  232   97  1000 -116       0  S10\n",
       "3  600046 -363  950  198  172  944  202   97  1000  -97       0  S10\n",
       "4  600062 -343  960  207  163  944  232  106  1000  -97       0  S10"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.DataFrame()\n",
    "last_name = ''\n",
    "patients = []\n",
    "for person in people:\n",
    "    name = person.split('R')[0]\n",
    "    if name != last_name:\n",
    "        dataset = dataset.drop(index=dataset.index)\n",
    "        patients.append(name)\n",
    "    last_name = name\n",
    "    print(name)\n",
    "    file = data_path + \"\\\\\" + person\n",
    "\n",
    "    if name in person:\n",
    "        temp = pd.read_csv(file, delimiter = \" \", header = None)\n",
    "        print(person, ' is read', end = \"\\t\")\n",
    "\n",
    "        #if 2 in temp[max(temp.columns)].unique():\n",
    "        print(\"Adding {} to dataset\".format(person), end = \"\\t\")\n",
    "        temp.columns = [\n",
    "                            'time',\n",
    "                            'A_F', 'A_V', 'A_L',\n",
    "                            'L_F', 'L_V', 'L_L',\n",
    "                            'T_F', 'T_V', 'T_L',\n",
    "                            'Action'                           \n",
    "                            ]\n",
    "        window_length = 2.5\n",
    "        temp = label_prefog(temp, window_length).reset_index(drop = True)\n",
    "        temp['name'] = name\n",
    "        print(\"{} is labelled\".format(person))\n",
    "        dataset = pd.concat([dataset, temp],axis = 0)\n",
    "        print('')\n",
    "    \n",
    "        dataset.reset_index(drop = True, inplace = True)\n",
    "        to_path = data_path+\"\\\\raw_labelled\"\n",
    "        to_name = to_path + \"\\\\win_\" + name + \".csv\"\n",
    "    # 存入/raw_labelled中\n",
    "    \n",
    "    dataset.to_csv(to_name, index = False)\n",
    "\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_window(act,window_length,dataframe):\n",
    "\n",
    "  indices = list(dataframe[dataframe.Action == act].index)\n",
    "  # indices記錄所有Action==act的index\n",
    "  groups = [] # 用來暫存一組(同action)資料的, 型態是[][]\n",
    "  temp = [] # 用來暫存一行資料的\n",
    "  group_count = 0\n",
    "  for i in range(len(indices)):\n",
    "    if i == len(indices)-1:\n",
    "      temp.append(indices[i])\n",
    "      groups.append(temp)\n",
    "      temp = []\n",
    "      break # 如果i已經來到最後的話就break\n",
    "    temp.append(indices[i])\n",
    "    if indices[i]+1 != indices[i+1]: #如果下個index不是連續的話, 就將前面這些存成第一組\n",
    "      group_count+=1\n",
    "      groups.append(temp)\n",
    "      temp = []\n",
    "\n",
    "  #print(groups)\n",
    "\n",
    "  fs = 64\n",
    "  # window_length = 1\n",
    "  window_length = int(window_length*fs)\n",
    "\n",
    "  final_dataframe = pd.DataFrame()\n",
    "  sumOfAct=0\n",
    "\n",
    "  for i in groups: # group[][]的每一行i\n",
    "    required = math.floor(len(i)/(window_length))\n",
    "\n",
    "    \n",
    "    sumOfAct= sumOfAct+required\n",
    "\n",
    "    req_index = i[0:(required*window_length)]\n",
    "\n",
    "    #print(req_index)\n",
    "    # concat([要結合的data集合], axis=0是方向為直的)\n",
    "    final_dataframe = pd.concat([final_dataframe,dataframe.iloc[req_index,:]],axis = 0)\n",
    "  \n",
    "  \n",
    "\n",
    "  return final_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "for patient in patients:\n",
    "  \n",
    "  path = os.getcwd()+\"\\\\dataset\"\n",
    "  name = path+\"\\\\raw_labelled\\\\win_\"+ patient +\".csv\"\n",
    "  dataframe = pd.read_csv(name)\n",
    "\n",
    "  activities = []\n",
    "  # print(patient)\n",
    "  for act in range(3):\n",
    "    activities.append(create_window(act,2.5,dataframe))\n",
    "  to_write = pd.concat(activities,axis = 0)\n",
    "  to_path = path + \"\\\\windows\"+\"\\\\windowed_\"+patient+\".csv\"\n",
    "  to_write.to_csv(to_path,index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_dataset=pd.DataFrame()\n",
    "for patient in patients:\n",
    "  \n",
    "  path = os.getcwd()+\"\\\\dataset\\\\windows\"\n",
    "  name = path+\"\\\\windowed_\"+ patient +\".csv\"\n",
    "  dataframe = pd.read_csv(name)\n",
    "  total_dataset = total_dataset.append(dataframe)\n",
    "\n",
    "to_path = path + \"\\\\total.csv\"\n",
    "total_dataset.to_csv(to_path,index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.describe of            time  A_F   A_V  A_L  L_F  L_V  L_L  T_F   T_V  T_L  Action name\n",
       "0        750000  -30   990  326  -45  972  181  -38  1000   29       0  S01\n",
       "1        750015  -30  1000  356  -18  981  212  -48  1028   29       0  S01\n",
       "2        750031  -20   990  336   18  981  222  -38  1038    9       0  S01\n",
       "3        750046  -20  1000  316   36  990  222  -19  1038    9       0  S01\n",
       "4        750062    0   990  316   36  990  212  -29  1038   29       0  S01\n",
       "...         ...  ...   ...  ...  ...  ...  ...  ...   ...  ...     ...  ...\n",
       "142715  2899906 -505   823  336    0    0    0    0     0    0       0  S10\n",
       "142716  2899921 -505   823  336    0    0    0    0     0    0       0  S10\n",
       "142717  2899937 -505   823  336    0    0    0    0     0    0       0  S10\n",
       "142718  2899953 -505   823  336    0    0    0    0     0    0       0  S10\n",
       "142719  2899968 -505   823  336    0    0    0    0     0    0       0  S10\n",
       "\n",
       "[1104960 rows x 12 columns]>"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_dataset.describe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = [\"A_F\", \"A_V\", \"A_L\", \"L_F\", \"L_T\", \"L_L\", \"T_F\", \"T_V\", \"T_L\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6906"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_windows = int((len(total_dataset))/160)\n",
    "total_windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.getcwd()+\"\\\\dataset\\\\final_windowed\\\\total.csv\"\n",
    "\n",
    "total_dataset = total_dataset[[\"A_F\", \"A_V\", \"A_L\", \"L_F\", \"L_V\", \"L_L\", \"T_F\", \"T_V\", \"T_L\", \"Action\"]].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "切資料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nX = np.empty((13811, 160, 9))\\ny = np.empty((13811, 1))\\nj = 0\\n\\nwindow_count = 0\\nfor items in range(13811):\\n    for i in range(160):\\n        if i == 0:\\n            y[j] = total_dataset[int(window_count*160), 9]\\n            j = j + 1\\n        for data in range(9):\\n            X[items, i, data] = total_dataset[int(window_count*160)+i, data]\\n    window_count = window_count+0.5\\n'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "X = np.empty((13811, 160, 9))\n",
    "y = np.empty((13811, 1))\n",
    "j = 0\n",
    "\n",
    "window_count = 0\n",
    "for items in range(13811):\n",
    "    for i in range(160):\n",
    "        if i == 0:\n",
    "            y[j] = total_dataset[int(window_count*160), 9]\n",
    "            j = j + 1\n",
    "        for data in range(9):\n",
    "            X[items, i, data] = total_dataset[int(window_count*160)+i, data]\n",
    "    window_count = window_count+0.5\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.empty((6906, 160, 9))\n",
    "y = np.empty((6906, 1))\n",
    "j = 0\n",
    "\n",
    "window_count = 0\n",
    "for items in range(6906):\n",
    "    for i in range(160):\n",
    "        if i == 0:\n",
    "            y[j] = total_dataset[int(window_count*160), 9]\n",
    "            j = j + 1\n",
    "        for data in range(9):\n",
    "            X[items, i, data] = total_dataset[int(window_count*160)+i, data]\n",
    "    window_count = window_count+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = to_categorical(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y ,test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(trainX, trainy, testX, testy):\n",
    "    verbose, epochs, batch_size = 0, 25, 64\n",
    "    n_timesteps, n_features, n_outputs = trainX.shape[1], trainX.shape[2], trainy.shape[1]\n",
    "\n",
    "    n_steps, n_length = 4, 40\n",
    "    trainX = trainX.reshape((trainX.shape[0], n_steps, n_length, n_features))\n",
    "    testX = testX.reshape((testX.shape[0], n_steps, n_length, n_features))\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(TimeDistributed(Conv1D(filters=64, kernel_size=3, activation='relu'), \n",
    "                              input_shape=(None, n_length, n_features)))\n",
    "    model.add(TimeDistributed(Conv1D(filters=64, kernel_size=3, activation='relu')))\n",
    "    model.add(TimeDistributed(Dropout(0.5)))\n",
    "    model.add(TimeDistributed(MaxPooling1D(pool_size=2)))\n",
    "    model.add(TimeDistributed(Flatten()))\n",
    "    model.add(LSTM(100))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(100, activation='relu'))\n",
    "    model.add(Dense(n_outputs, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    model.fit(trainX, trainy, epochs=epochs, batch_size=batch_size, verbose=verbose)\n",
    "\n",
    "    _, accuracy = model.evaluate(testX, testy, batch_size=batch_size, verbose=0)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(trainX, trainy, testX, testy, repeats=10):\n",
    "\n",
    "    scores = list()\n",
    "    for r in range(repeats):\n",
    "        score = evaluate_model(trainX, trainy, testX, testy)\n",
    "        score = score * 100.0\n",
    "        print('>#%d: %.3f' % (r+1, score))\n",
    "        scores.append(score)\n",
    "    \n",
    "    m, s = np.mean(scores), np.std(scores)\n",
    "    print('Accuracy: %.3f%% (+/-%.3f)' % (m, s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">#1: 89.114\n",
      "Accuracy: 89.114% (+/-0.000)\n"
     ]
    }
   ],
   "source": [
    "run_experiment(X_train, y_train, X_test, y_test, repeats=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelC1 = Sequential()\n",
    "modelC1.add(Conv1D(filters=64, kernel_size=2, activation='relu',\n",
    "                     strides=1, padding='valid', data_format='channels_last',\n",
    "                     input_shape=(160, 9)))\n",
    "modelC1.add(MaxPooling1D(pool_size=2, strides=None, padding='valid', \n",
    "                           data_format='channels_last')) \n",
    "modelC1.add(Flatten())\n",
    "modelC1.add(Dense(units=50, activation='relu',\n",
    "                use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros',))\n",
    "modelC1.add(Dense(units=1))\n",
    "modelC1.compile(optimizer='adam', loss='mse',\n",
    "                 metrics=['accuracy'], loss_weights=None, sample_weight_mode=None, weighted_metrics=None, target_tensors=None)\n",
    "    \n",
    "print('\\n',modelC1.summary())\n",
    "history = modelC1.fit(X, y, batch_size=32, epochs=25, verbose=0)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def oned_cnn_model(sw_width, n_features, X, y, test_X, epoch_num, verbose_set):\n",
    "    model = Sequential()\n",
    "    \n",
    "    # 對於一維卷積來說，data_format='channels_last'是默認配置，該API的規則如下：\n",
    "    # 輸入形狀爲：(batch, steps, channels)；輸出形狀爲：(batch, new_steps, filters)，padding和strides的變化會導致new_steps變化\n",
    "    # 如果設置爲data_format = 'channels_first'，則要求輸入形狀爲： (batch, channels, steps).\n",
    "    model.add(Conv1D(filters=64, kernel_size=2, activation='relu',\n",
    "                     strides=1, padding='valid', data_format='channels_last',\n",
    "                     input_shape=(sw_width, n_features)))\n",
    "    \n",
    "    # 對於一維池化層來說，data_format='channels_last'是默認配置，該API的規則如下：\n",
    "    # 3D 張量的輸入形狀爲: (batch_size, steps, features)；輸出3D張量的形狀爲：(batch_size, downsampled_steps, features)\n",
    "    # 如果設置爲data_format = 'channels_first'，則要求輸入形狀爲：(batch_size, features, steps)\n",
    "    model.add(MaxPooling1D(pool_size=2, strides=None, padding='valid', \n",
    "                           data_format='channels_last')) \n",
    "    \n",
    "    # data_format參數的作用是在將模型從一種數據格式切換到另一種數據格式時保留權重順序。默認爲channels_last。\n",
    "    # 如果設置爲channels_last，那麼數據輸入形狀應爲：（batch，…，channels）；如果設置爲channels_first，那麼數據輸入形狀應該爲（batch，channels，…）\n",
    "    # 輸出爲（batch, 之後參數尺寸的乘積）\n",
    "    model.add(Flatten())\n",
    "    \n",
    "    # Dense執行以下操作：output=activation（dot（input，kernel）+bias），\n",
    "    # 其中,activation是激活函數，kernel是由層創建的權重矩陣，bias是由層創建的偏移向量（僅當use_bias爲True時適用）。\n",
    "    # 2D 輸入：(batch_size, input_dim)；對應 2D 輸出：(batch_size, units)\n",
    "    model.add(Dense(units=50, activation='relu',\n",
    "                use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros',))\n",
    "    \n",
    "    # 因爲要預測下一個時間步的值，因此units設置爲1\n",
    "    model.add(Dense(units=1))\n",
    "    \n",
    "    # 配置模型\n",
    "    model.compile(optimizer='adam', loss='mse',\n",
    "                 metrics=['accuracy'], loss_weights=None, sample_weight_mode=None, weighted_metrics=None, target_tensors=None)\n",
    "    \n",
    "    print('\\n',model.summary())\n",
    "    # X爲輸入數據，y爲數據標籤；batch_size：每次梯度更新的樣本數，默認爲32。\n",
    "    # verbose: 0,1,2. 0=訓練過程無輸出，1=顯示訓練過程進度條，2=每訓練一個epoch打印一次信息\n",
    "    \n",
    "    history = model.fit(X, y, batch_size=32, epochs=epoch_num, verbose=verbose_set)\n",
    "    \n",
    "    \n",
    "    yhat = model.predict(test_X, verbose=0)\n",
    "    print('\\nyhat:', yhat)\n",
    "    \n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "sw_width = 160\n",
    "n_features = 9\n",
    "epoch_num = 25\n",
    "verbose_set = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_18\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv1d_21 (Conv1D)          (None, 159, 64)           1216      \n",
      "                                                                 \n",
      " max_pooling1d_11 (MaxPoolin  (None, 79, 64)           0         \n",
      " g1D)                                                            \n",
      "                                                                 \n",
      " flatten_11 (Flatten)        (None, 5056)              0         \n",
      "                                                                 \n",
      " dense_22 (Dense)            (None, 50)                252850    \n",
      "                                                                 \n",
      " dense_23 (Dense)            (None, 1)                 51        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 254,117\n",
      "Trainable params: 254,117\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      " None\n",
      "\n",
      "yhat: [[0.05374524]\n",
      " [0.05374524]\n",
      " [0.05374524]\n",
      " ...\n",
      " [0.05374524]\n",
      " [0.05374524]\n",
      " [0.05374524]]\n"
     ]
    }
   ],
   "source": [
    "model, history = oned_cnn_model(sw_width, n_features, X_train, y_train, X_test, epoch_num, verbose_set)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "cdb0dce53ee342b4df2ec29da00f64db1b0b5ad6c6a758debba5208518f3513a"
  },
  "kernelspec": {
   "display_name": "Python 3.8.11 64-bit ('HighHeel': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
